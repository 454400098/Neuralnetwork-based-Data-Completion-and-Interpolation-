{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO MACHINE LEARNING LIBARY USED FOR TRAINING/REGRESSION/CLASSFICATION\n",
    "# sklean.feature_extraction ONLY USED for language data into Vector transformation\n",
    "# preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer#ONLY USED FOR TRANSFORM FROM LANGUAGE TO VECTOR\n",
    "from numpy import linalg as LA\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (17,55,59,61,65,68,69,70,83,90,91,92,93,120,121,122,123,126,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./Tab.delimited.Cleaned.dataset.WITH.variable.labels.csv', sep='\\t',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,11,12,19,20,129,132,169,230) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.read_csv('./ML3AllSites.csv', sep=',',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, n_h, n_iterate=10, learning_rate=1):\n",
    "        self.n_x = None  # size of the input layer\n",
    "        self.n_h = n_h  # size of the hidden layer\n",
    "        self.n_y = None # size of the output layer\n",
    "        self.W1 = None\n",
    "        self.W2 = None\n",
    "        self.b1 = None\n",
    "        self.b2 = None\n",
    "        self.A1 = None\n",
    "        self.A2 = None  # sigmoid output of the second activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterate = n_iterate\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.W1 = np.random.randn(self.n_h, self.n_x) * 0.01\n",
    "        self.b1 = np.zeros((self.n_h, 1))\n",
    "        self.W2 = np.random.randn(self.n_y, self.n_h) * 0.01\n",
    "        self.b2 = np.zeros((self.n_y, 1))\n",
    "    \n",
    "    \n",
    "    def relu(self, z):\n",
    "        return z * (z > 0)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def compute_cost(self, Y):\n",
    "        #cost = np.linalg.norm(self.A2 - Y)\n",
    "        cost = - (Y * np.log(self.A2) + (1-Y) * np.log(1-self.A2)).mean()\n",
    "        return np.squeeze(cost) \n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        self.A2 = self.sigmoid(self.W2 @ self.A1 + self.b2)\n",
    "\n",
    "    def backward_propagation(self, X, Y):\n",
    "        m = X.shape[1]\n",
    "\n",
    "        dZ2 = self.A2 - Y\n",
    "        dW2 = dZ2 @ self.A1.T / m\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "        dZ1 = self.W2.T @ dZ2 * (self.A1 > 0)\n",
    "        dW1 = dZ1 @ X.T / m\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        X, Y = X.T, Y.T\n",
    "        self.n_x = X.shape[0]\n",
    "        self.n_y = Y.shape[0]\n",
    "        self.initialize_parameters()\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(0, self.n_iterate):\n",
    "            self.forward_propagation(X)\n",
    "            self.backward_propagation(X, Y)\n",
    "            if i % 10 == 0:\n",
    "                cost = self.compute_cost(Y)\n",
    "                self.learning_rate = 5 * cost\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    def fit_continue(self,X,Y):\n",
    "        X, Y = X.T, Y.T\n",
    "        self.n_x = X.shape[0]\n",
    "        self.n_y = Y.shape[0]\n",
    "#         if self.W1 is None:\n",
    "#             self.initialize_parameters()\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(0, self.n_iterate):\n",
    "            self.forward_propagation(X)\n",
    "            self.backward_propagation(X, Y)\n",
    "            if i % 10 == 0:\n",
    "                cost = self.compute_cost(Y)\n",
    "                self.learning_rate = 5 * cost\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        X = X.T\n",
    "        A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        A2 = self.sigmoid(self.W2 @ A1 + self.b2)\n",
    "        return A2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=64):\n",
    "        self.n_clusters = n_clusters  # number of clusters\n",
    "        self.centers = None  # to record the centers\n",
    "        self.labels = None\n",
    "        self.Y = None\n",
    "\n",
    "    def random_center(self,Y): #only for 1d dataset\n",
    "    # randomly generate n_cluster clusters in the raange of X\n",
    "        self.centers = np.random.rand(self.n_clusters, len(Y))\n",
    "        for i in range(self.n_clusters):\n",
    "            self.centers[i] = Y[i]\n",
    "            \n",
    "            \n",
    "    def random_center2(self,Y):\n",
    "    # randomly generate n_cluster clusters in the raange of X\n",
    "        self.centers = np.random.rand(self.n_clusters, len(Y[0]))\n",
    "        for i in range(self.n_clusters):\n",
    "            self.centers[i] = Y[i]\n",
    "#         for j in range(0):\n",
    "#             Y_j_min = self.Y[:,j].min()\n",
    "#             Y_j_max = self.Y[:,j].max()\n",
    "#             self.centers[:,j] = Y_j_min + (Y_j_max - Y_j_min) * self.centers[:,j]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def dist(self, point1, point2): #old one\n",
    "        return 2*(point1[0]-point2[0])**2 + 4*(point1[1]-point2[1])**2 + 3*(point1[2]-point2[2])**2\n",
    "\n",
    "    def dist2(self,point1,point2):\n",
    "        return LA.norm(point1-point2)**2\n",
    "    \n",
    "    def fit(self, Y):\n",
    "        self.Y = Y\n",
    "        self.labels = np.zeros(Y.shape[0], dtype='uint8')  # record the current labels of each sample of X\n",
    "        self.random_center(Y)\n",
    "        diff = 1\n",
    "        \n",
    "        while diff > 1e-3:\n",
    "            old_center = self.centers.copy()\n",
    "\n",
    "            # go through all samples and label them using the nearest label\n",
    "            for i in range(Y.shape[0]):\n",
    "                distance = np.zeros(self.n_clusters)\n",
    "                for j in range(self.n_clusters):\n",
    "                    distance[j] = self.dist2(Y[i], self.centers[j])\n",
    "                self.labels[i] = np.argmin(distance)\n",
    "                \n",
    "                \n",
    "            # update the centers\n",
    "            for i in range(self.n_clusters):\n",
    "                self.centers[i] = Y[self.labels==i].mean(axis=0)\n",
    "                \n",
    "\n",
    "            # update the difference\n",
    "            diff = np.linalg.norm(self.centers - old_center)\n",
    "            print(diff)\n",
    "        return self\n",
    "    \n",
    "    def fit2(self, Y):\n",
    "        self.Y = Y\n",
    "        self.labels = np.zeros(Y.shape[0], dtype='uint8')  # record the current labels of each sample of X\n",
    "        self.random_center2(Y)\n",
    "        diff = 1\n",
    "        \n",
    "        while diff > 1e-3:\n",
    "            old_center = self.centers.copy()\n",
    "\n",
    "            # go through all samples and label them using the nearest label\n",
    "            for i in range(Y.shape[0]):\n",
    "                distance = np.zeros(self.n_clusters)\n",
    "                for j in range(self.n_clusters):\n",
    "                    distance[j] = self.dist2(Y[i], self.centers[j])\n",
    "                self.labels[i] = np.argmin(distance)\n",
    "                \n",
    "                \n",
    "            # update the centers\n",
    "            for i in range(self.n_clusters):\n",
    "                self.centers[i] = Y[self.labels==i].mean(axis=0)\n",
    "                \n",
    "\n",
    "            # update the difference\n",
    "            diff = np.linalg.norm(self.centers - old_center)\n",
    "            print(diff)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, Y):\n",
    "        out = np.zeros(Y.shape)\n",
    "        for i in range(self.n_clusters):\n",
    "            out[self.labels==i] = self.centers[i]\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oneHOTENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncoding(Y, kmeans):\n",
    "    out = np.zeros((len(Y), kmeans.n_clusters))\n",
    "    for i in range(len(Y)):\n",
    "        out[i, Y[i]] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test clustering numbered data:\n",
    "    step1 : get single column\n",
    "    step2 : sort in increasing order\n",
    "    step3 : based on distribution, cut data into 4 section with equally number of candidate\n",
    "    step4 : if data is null, set isnon to be true for that column\n",
    "    step5 : append new datacol to input_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isint(value):\n",
    "  try:\n",
    "    int(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "def isfloat(value):\n",
    "  try:\n",
    "    float(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "def getdistriarr(df,arr_str):\n",
    "    arr = df[arr_str].values\n",
    "    di = []\n",
    "    di_og = []  #RETURN OG SINCE di will be sort\n",
    "    for i in range(len(arr)):\n",
    "        if isint(arr[i]) == True:\n",
    "            di.append(int(arr[i]))\n",
    "            di_og.append(int(arr[i]))\n",
    "        else:\n",
    "            di.append(-1)\n",
    "            di_og.append(-1)\n",
    "    di.sort()\n",
    "    ans_arr = []\n",
    "    leng = len(di)\n",
    "    ans_arr.append(di[int(leng/3)])\n",
    "#     ans_arr.append(di[int(leng/2)])\n",
    "    ans_arr.append(di[int(leng*0.66)])\n",
    "    ans_arr.append(di[int(leng-1)])\n",
    "    return di_og,ans_arr\n",
    "\n",
    "def oneHotEncoding_return_arr(att_arr,class_arr): #one \n",
    "    #add one more column for each row since last col used as flag\n",
    "    choice = len(class_arr)\n",
    "    out = np.zeros((len(att_arr),len(class_arr)+1))\n",
    "    for i in range(len(att_arr)):\n",
    "        loc = 0\n",
    "        #first jude if data is null\n",
    "        if att_arr[i] == -1:\n",
    "            out[i][-1] = 0\n",
    "            continue\n",
    "        else:\n",
    "            out[i][-1] = 1\n",
    "            for j in range(len(class_arr)):\n",
    "                if att_arr[i] == class_arr[j]:\n",
    "                    out[i][j] = 1\n",
    "                    break\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "def GET_ONE_HOT_ARRAY_ONLY_DIGIT_COLUMN(df,attr_name,og_dic):\n",
    "    att_arr,class_arr = getdistriarr(df,attr_name)\n",
    "    dic_ans = create_dic_for_classification(class_arr)\n",
    "    append_arr_of_dic_to_overall(og_dic,dic_ans,attr_name)\n",
    "\n",
    "    return oneHotEncoding_return_arr(att_arr,class_arr)\n",
    "\n",
    "def create_dic_for_classification(class_arr):\n",
    "    cur_dic = {}\n",
    "    total_len = len(class_arr)\n",
    "    for i in range(len(class_arr)):\n",
    "        cur_dic[class_arr[i]] = i\n",
    "    return cur_dic\n",
    "\n",
    "def append_arr_of_dic_to_overall(og,dic,att_name):\n",
    "    og[att_name] = dic\n",
    "    \n",
    "    \n",
    "def append_arr(old,new):\n",
    "    #both old and new has the same num of row\n",
    "    #create a new nparray\n",
    "    \n",
    "    newdim = len(old[0])+len(new[0])\n",
    "    ans = np.zeros((len(old),newdim))\n",
    "    print(ans.shape)\n",
    "    for i in range(len(old)):\n",
    "        ans[i] = np.append(old[i],new[i])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP TWO CONVERT NLP DATASET INTO VECTOR:\n",
    "    STEP1: vectorize dataset\n",
    "    STEP2: apply kmeans clustering to language data\n",
    "    STEP3: ONEHOT ENCODING\n",
    "    STEP4: APPEND CLASSIFICATION DATASET INTO old CLASSFICAITION ARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLPDATAPRO(allsentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(allsentences)\n",
    "    ans = X.toarray()\n",
    "    return ans\n",
    "\n",
    "def createtextarr_ONEHOT_ENCODING(attri_col): #ONEHOT_ENCODING\n",
    "    df2 = df[attri_col]\n",
    "    arr = df2.values\n",
    "    for i in range(len(arr)):\n",
    "        di.append(arr[i])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP THREE CONVERT TEXT_DATASET WITH FIXED classification into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processnull(arr):\n",
    "    arrnew = []\n",
    "    for i in range(len(arr)):\n",
    "        if type(arr[i]) == float and math.isnan(arr[i]) :\n",
    "            arrnew.append('Nan')\n",
    "        else:\n",
    "            arrnew.append(arr[i])\n",
    "    return arrnew\n",
    "\n",
    "def returnuniqiearr(og):\n",
    "    ar = np.asarray(og)\n",
    "    return np.unique(ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: create a function to append text to old vecto(remember to append flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: TEST NN TONIGHT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onehoted_text_fixed_classification(attribu_name,df,km_num):\n",
    "    df_cur = df[attribu_name]\n",
    "    attribute_arr = df_cur.values\n",
    "    arr_no_null = processnull(attribute_arr) #create new arr with format fit by NLP(CountVectorizer)\n",
    "    NLP_ARR = NLPDATAPRO(arr_no_null)\n",
    "    #BEFORE ONEHOT_ENCODING\n",
    "    #APPLY KMEANS CLUSTERING SINCE TOO MANY DIFFERENT CLASSIFICATION\n",
    "    KM_TMP = KMeans(km_num)\n",
    "    KM_TMP.fit2(NLP_ARR)\n",
    "    labels_arr = KM_TMP.labels\n",
    "    return labels_arr\n",
    "    \n",
    "def onehot_enco(labelarr,df,att_name):\n",
    "    og_text_arr = df[att_name].values\n",
    "    #create dic to remeber loc for each value in the uniqe arr\n",
    "    uni_arr = np.unique(labelarr)\n",
    "    loc_dic = {}\n",
    "    for i in range(len(uni_arr)):\n",
    "        loc_dic[uni_arr[i]] = i\n",
    "    #based on the lenth of dic,create onehot_encod\n",
    "    choice = len(loc_dic)\n",
    "    out = np.zeros((len(labelarr),choice+1))\n",
    "    for i in range(len(labelarr)):\n",
    "        #first junde if data is null\n",
    "        if type(og_text_arr[i])!=str and math.isnan(og_text_arr[i]) == True:\n",
    "            out[i][-1] = 0\n",
    "            continue\n",
    "        else:\n",
    "            out[i][-1] = 1\n",
    "            cur_loc = loc_dic.get(labelarr[i])\n",
    "            out[i][cur_loc] = 1\n",
    "            \n",
    "    return out,loc_dic\n",
    "\n",
    "def GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,attr_name,og_dic,km_num):\n",
    "    labels_arr = create_onehoted_text_fixed_classification(attr_name,df,km_num)\n",
    "    out_arr,loc_dic = onehot_enco(labels_arr,df,attr_name)\n",
    "    #append dic to global array of dic\n",
    "    append_arr_of_dic_to_overall(og_dic,loc_dic,attr_name)\n",
    "    return out_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL WORK:\n",
    "    1. MAINTAIN AN ARRAY OF DICTIONARY S.T EVERY DIC\n",
    "       CONTAINS MAPPING FROM CLASSIFICAITON TO ACTUAL INDEX AFTER KMEANS\n",
    "    \n",
    "    2. AFTER TRAINING, WE WILL UTILIZED THE ARR_DIC TO RECOVER CANDIDATE ANSER\n",
    "    \n",
    "    3. TODO: REWRITE COST FUNCTION OF NERUAL NETWORK\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINDING:\n",
    "    AFTER combine several attributes column, num of candidate with dinct\n",
    "    ans grows extremly fast.\n",
    "    Need to apply Kmeans clustering again to the array of candidate before\n",
    "    threw into Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE A SCRIPT TO APPEND ASSIGNED ATTRIBUTE TO INPUT COLUMN\n",
    "\n",
    "(API)INSTRUCTION:<br>\n",
    "USER WHO WANTS TO CREATE PREPROCESSED DATASET ONLY NEED TO CREATE\n",
    "ARRAY[i]<br>\n",
    "       ARRAY[i][0] = 'name of attribute'<br>\n",
    "       ARRAY[i][1] = 1 : it is a digit column<br>\n",
    "       ARRAY[i][1] = 0 : it is a NLP column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_auto_append_attri(arr_of_attribute,df,og_dic):\n",
    "    for i in range(len(arr_of_attribute)):\n",
    "        name = arr_of_attribute[i][0]\n",
    "        isdigit = arr_of_attribute[i][1]\n",
    "        if isdigit == 1:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_DIGIT_COLUMN(df,name,og_dic)\n",
    "        else:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,name,og_dic,20)\n",
    "        #append old with new\n",
    "        if i == 0:\n",
    "            old = attr_arr\n",
    "        else:\n",
    "            old = append_arr(old,attr_arr)\n",
    "#             print(old.shape)\n",
    "    return old\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_semi_auto_append_attri(arr_of_attribute,df,og_dic):\n",
    "    for i in range(len(arr_of_attribute)):\n",
    "        name = arr_of_attribute[i][0]\n",
    "        isdigit = arr_of_attribute[i][1]\n",
    "        if isdigit == 1:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_DIGIT_COLUMN(df,name,og_dic)\n",
    "        elif isdigit == 0:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,name,og_dic,20)\n",
    "        elif isdigit == 2:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_FIXED_CHOICE_COLUMN(df,name,og_dic)\n",
    "        #append old with new\n",
    "        if i == 0:\n",
    "            old = attr_arr\n",
    "        else:\n",
    "            old = append_arr(old,attr_arr)\n",
    "#             print(old.shape)\n",
    "    return old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_choice_get_distinct_arr(df,arr_str):\n",
    "    arr = df[arr_str].values\n",
    "    uni_arr = np.unique(arr)\n",
    "    di = []\n",
    "#     di_og = [] #RETURN OG SINCE di will be sorted\n",
    "    div_arr = []\n",
    "    for i in range(len(uni_arr)):\n",
    "        if math.isnan(uni_arr[i])!=True:\n",
    "            div_arr.append(uni_arr[i])\n",
    "    for i in range(len(arr)):\n",
    "        if math.isnan(arr[i])!= True:\n",
    "            di.append(int(arr[i]))\n",
    "        else:\n",
    "            di.append(-1)\n",
    "    return di,div_arr\n",
    "\n",
    "def GET_ONE_HOT_ARRAY_ONLY_FIXED_CHOICE_COLUMN(df,attr_name,og_dic):\n",
    "    att_arr,class_arr = fix_choice_get_distinct_arr(df,attr_name)\n",
    "    print('class_arr',class_arr)\n",
    "    dic_ans = create_dic_for_classification(class_arr)\n",
    "    append_arr_of_dic_to_overall(og_dic,dic_ans,attr_name)\n",
    "    return oneHotEncoding_return_arr(att_arr,class_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Kmeans Clustering to the overal column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_whole_attribute_get_Y_for_Neural_NetWork(X,num_cluster):\n",
    "    km = KMeans(num_cluster)\n",
    "    km.fit2(X)\n",
    "    \n",
    "    #APPLY ONEHOT_ENCODING TO NEURAL_NETWORKAGAIN\n",
    "    \n",
    "    return oneHotEncoding(km.labels,km),km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLY NEURAL NETWORK TO SOLVE THIS QUESTION:\n",
    "\n",
    "REMEBER TO CONSIDER FLAG FOR EACH COLUMN\n",
    "\n",
    "# QUESTION: SHOULD I APPLY ONE HOT ENCODING TO EVERY ATTRIBUT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predicted_data(pred,kmeans):\n",
    "    ans = []\n",
    "    for i in range(len(pred)):\n",
    "        ans.append(kmeans.centers[np.argmax(pred[i])])\n",
    "    newarr = np.asarray(ans)\n",
    "    return newarr\n",
    "\n",
    "def find_classification_based_on_distance(pred,dic):\n",
    "    #create array with all dict keys based on insertion order\n",
    "    dicarr = list(dic.keys())\n",
    "    #isolate target awated_array used to classification\n",
    "    for i in range(len(pred)):\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "#             print('cur dic: ',cur_dict_key)\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_lengh = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_lengh  #update current cutting position\n",
    "            #restore dimension of onehotencoding for current dictionary\n",
    "            one_hot_cur_dic = np.zeros((cur_dict_lengh,cur_dict_lengh))\n",
    "#             print('before any processing cur loc is ', cur_loc, ' cur des is : ', cur_des)\n",
    "            for q in range(0,cur_dict_lengh-1):\n",
    "                one_hot_cur_dic[q][q] = 1\n",
    "                one_hot_cur_dic[q][-1] = 1\n",
    "            #isolate target awaited_classification array\n",
    "            cur_isolated_arr = pred[i][cur_loc:cur_des]\n",
    "            #compute classification with smallest distance\n",
    "            arr_store_distance= []\n",
    "            for h in range (len(one_hot_cur_dic)):\n",
    "                arr_store_distance.append(np.linalg.norm(cur_isolated_arr-one_hot_cur_dic[h]))\n",
    "            #transfer dic to arr s.t unilized argmim\n",
    "            dis_np_array = np.asarray(arr_store_distance)\n",
    "            index = np.argmin(dis_np_array)\n",
    "#             if cur_dict_key == 'big5_10':\n",
    "#                 print(index)\n",
    "#                 print('cur onehot_array is ', one_hot_cur_dic[3] )\n",
    "#                 print('cur loc is ', cur_loc, ' cur des is : ', cur_des)\n",
    "            # change value in output array\n",
    "            for k in range (cur_dict_lengh):\n",
    "                pred[i][cur_loc+k] = one_hot_cur_dic[index][k]\n",
    "                \n",
    "            cur_loc += cur_dict_lengh #update cur_loc not used in curretn loop\n",
    "\n",
    "def find_classification_based_on_prob(pred,dic):\n",
    "    dicarr = list(dic.keys())\n",
    "    for i in range(len(pred)):\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_lengh = len(cur_dict_arr)  + 1\n",
    "            cur_des += cur_dict_lengh\n",
    "            \n",
    "            cur_isolated_arr = pred[i][cur_loc:cur_des-1]\n",
    "            #find index of max value\n",
    "            index = np.argmax(cur_isolated_arr)\n",
    "            \n",
    "            for k in range(cur_dict_lengh):\n",
    "                pred[i][cur_loc+k] = 0\n",
    "            \n",
    "            pred[i][cur_loc + index] = 1\n",
    "            cur_loc +=cur_dict_lengh\n",
    "            \n",
    "            \n",
    "def create_reverse_dictionary(dic):\n",
    "    reverse = {}\n",
    "    dic_arr = list(dic.keys())\n",
    "    for i in range(len(dic_arr)):\n",
    "        cur_reverse_dict = {}\n",
    "        cur_dic_key = dic_arr[i]\n",
    "        cur_key_arr = list(dic.get(cur_dic_key).keys())\n",
    "        cur_index_arr = list(dic.get(cur_dic_key).values())\n",
    "        #reverse key_value pair in originaly dictionary\n",
    "        for j in range(len(cur_key_arr)):\n",
    "            cur_reverse_dict[cur_index_arr[j]] = cur_key_arr[j]\n",
    "        #append current dic to ans dic\n",
    "        reverse[cur_dic_key] = cur_reverse_dict\n",
    "    return reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create  function to knock out some attribute inorder to create more dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingset_generator(train,dic,arr_str):\n",
    "    #first find the location \n",
    "    #knock out\n",
    "    #return dataset\n",
    "    dicarr = list(dic.keys())\n",
    "    changed_arr = np.copy(train)\n",
    "    y_arr = np.copy(train)\n",
    "    #outermost loop : loop through those keys that need to be cleared\n",
    "    for d in range(len(arr_str)):\n",
    "        cur_key = arr_str[d]\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_length = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_length\n",
    "            #test if we find what we want\n",
    "            if cur_dict_key == cur_key:\n",
    "                for i in range(len(train)):\n",
    "                    for k in range(cur_dict_length):\n",
    "                        changed_arr[i][cur_loc + k] = 0\n",
    "                break\n",
    "            cur_loc +=cur_dict_length\n",
    "    return changed_arr,y_arr\n",
    "\n",
    "def append_row(old,new):\n",
    "    ans = np.vstack([old,new])\n",
    "    return ans\n",
    "\n",
    "\n",
    "def enlarge_dataset(train,y,dic,arr_of_arr_str):\n",
    "    list_of_new_x = []\n",
    "    list_of_new_y = []\n",
    "    new_x = np.copy(train)\n",
    "    new_y = np.copy(y)\n",
    "    for i in range(len(arr_of_arr_str)):\n",
    "        cur_str = arr_of_arr_str[i]\n",
    "        changed_arr,y_arr = trainingset_generator(train,dic,cur_str)\n",
    "        #append\n",
    "        new_x = append_row(new_x,changed_arr)\n",
    "        new_y = append_row(new_y,y_arr)\n",
    "        list_of_new_x.append(changed_arr)\n",
    "        list_of_new_y.append(y_arr)\n",
    "#     return new_x,new_y\n",
    "    return list_of_new_x,list_of_new_y\n",
    "\n",
    "def createlist_list_string(dic):\n",
    "    ans = []\n",
    "    key_arr = list(dic.keys())\n",
    "    for i in range(len(key_arr)):\n",
    "        tmp = []\n",
    "        tmp.append(key_arr[i])\n",
    "        ans.append(tmp)\n",
    "        \n",
    "    #degree 2\n",
    "#     for i in range(len(key_arr)):\n",
    "#         for j in range(i,len(key_arr)):\n",
    "#             tmp = []\n",
    "#             tmp.append(key_arr[i])\n",
    "#             tmp.append(key_arr[j])\n",
    "#             ans.append(tmp)\n",
    "    return ans\n",
    "\n",
    "def train_list_of_dataset(nn,x,y):\n",
    "    nn.fit(x[0],y[0])\n",
    "    for i in range(1,len(x)):\n",
    "        print('current iter', i)\n",
    "        nn.fit_continue(x[i],x[i])\n",
    "    \n",
    "    return nn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = createlist_list_string(og_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = enlarge_dataset(X_train_new,X_train_new,og_pred,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create testing attribut arra\n",
    "arr = []\n",
    "og_arr = []\n",
    "og_train = {} #init global dictionary\n",
    "og_pred = {}\n",
    "# arr.append(['age',1])\n",
    "# arr.append(['mood_01',2]) #Today I generally feel\n",
    "# arr.append(['major',0])\n",
    "# arr.append(['big5_01',2]) #I see myself as: Extraverted, enthusiastic.\n",
    "# arr.append(['big5_02',2]) #I see myself as: Critical, quarrelsome.\n",
    "# arr.append(['big5_03',2]) #I see myself as: Dependable, self-disciplined.\n",
    "# arr.append(['big5_04',2]) #I see myself as: Anxious, easily upset.\n",
    "# arr.append(['big5_05',2]) #I see myself as: Open to new experiences, complex.\n",
    "# arr.append(['big5_06',2]) #I see myself as: Reserved, quiet.\n",
    "# arr.append(['big5_07',2]) #I see myself as: Sympathetic, warm.\n",
    "# arr.append(['big5_08',2]) #I see myself as: Disorganized, careless.\n",
    "# arr.append(['big5_09',2]) #I see myself as: Calm, emotionally stable.\n",
    "# arr.append(['big5_10',2]) #I see myself as: Conventional, uncreative.\n",
    "arr.append(['intrinsic_01',2])\n",
    "arr.append(['intrinsic_02',2])\n",
    "arr.append(['intrinsic_03',2])\n",
    "arr.append(['intrinsic_04',2])\n",
    "arr.append(['intrinsic_05',2])\n",
    "arr.append(['intrinsic_06',2])\n",
    "arr.append(['intrinsic_07',2])\n",
    "arr.append(['intrinsic_08',2])\n",
    "arr.append(['intrinsic_09',2])\n",
    "arr.append(['intrinsic_10',2])\n",
    "arr.append(['intrinsic_11',2])\n",
    "arr.append(['intrinsic_12',2])\n",
    "arr.append(['intrinsic_13',2])\n",
    "arr.append(['intrinsic_14',2])\n",
    "arr.append(['intrinsic_15',2])\n",
    "arr.append(['mcfiller1',2])\n",
    "arr.append(['mcfiller2',2])\n",
    "arr.append(['mcfiller3',2])\n",
    "arr.append(['mcmost1',2])\n",
    "arr.append(['mcmost2',2])\n",
    "arr.append(['mcmost3',2])\n",
    "arr.append(['mcmost4',2])\n",
    "arr.append(['mcmost5',2])\n",
    "arr.append(['mcsome1',2])\n",
    "arr.append(['mcsome2',2])\n",
    "arr.append(['mcsome3',2])\n",
    "arr.append(['mcsome4',2])\n",
    "arr.append(['mcsome5',2])\n",
    "arr.append(['mood_01',2])\n",
    "arr.append(['mood_02',2])\n",
    "arr.append(['pate_01',2])\n",
    "arr.append(['pate_02',2])\n",
    "arr.append(['pate_03',2])\n",
    "arr.append(['pate_04',2])\n",
    "arr.append(['pate_05',2])\n",
    "arr.append(['stress_01',2])\n",
    "arr.append(['stress_02',2])\n",
    "arr.append(['stress_03',2])\n",
    "arr.append(['stress_04',2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_df = df3.iloc[:100]\n",
    "trainning_df = df3\n",
    "# trainning_df = trainning_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 10)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 54.0]\n",
      "(2998, 16)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 77.0]\n",
      "(2998, 22)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 28)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 33)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 39)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 45)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 50)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 55)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 61)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 66)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 71)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 77)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 82)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 87)\n",
      "class_arr [0.0, 1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 93)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 98)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 101)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 104)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 107)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 110)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 113)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 116)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 119)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 122)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 125)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 128)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 136)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 144)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 150)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 156)\n",
      "class_arr [1.0, 2.0, 3.0]\n",
      "(2998, 160)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "(2998, 167)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "(2998, 174)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 180)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 186)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 192)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 198)\n"
     ]
    }
   ],
   "source": [
    "# X_train = semi_auto_append_attri(arr,predict_df,og)\n",
    "X_train = new_semi_auto_append_attri(arr,trainning_df,og_pred)\n",
    "# print('start training dataset')\n",
    "# X_tra = semi_auto_append_attri(arr,trainning_df,og_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = X_train[:2900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice nparr\n",
    "x_pred = X_train[2900:]\n",
    "x_real_train = X_train[:2900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2900, 198)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(1600,200,0.01) #size of input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.692578\n",
      "Cost after iteration 10: 0.543332\n",
      "Cost after iteration 20: 0.481534\n",
      "Cost after iteration 30: 0.386758\n",
      "Cost after iteration 40: 0.474809\n",
      "Cost after iteration 50: 0.316313\n",
      "Cost after iteration 60: 0.325299\n",
      "Cost after iteration 70: 0.321666\n",
      "Cost after iteration 80: 0.318136\n",
      "Cost after iteration 90: 0.315552\n",
      "Cost after iteration 100: 1.177085\n",
      "Cost after iteration 110: 0.313433\n",
      "Cost after iteration 120: 0.312987\n",
      "Cost after iteration 130: 0.309059\n",
      "Cost after iteration 140: 0.306504\n",
      "Cost after iteration 150: 0.306141\n",
      "Cost after iteration 160: 0.305901\n",
      "Cost after iteration 170: 0.306555\n",
      "Cost after iteration 180: 0.314931\n",
      "Cost after iteration 190: 0.312783\n",
      "current iter 1\n",
      "Cost after iteration 0: 0.329220\n",
      "Cost after iteration 10: 0.299456\n",
      "Cost after iteration 20: 0.304661\n",
      "Cost after iteration 30: 0.305540\n",
      "Cost after iteration 40: 0.304266\n",
      "Cost after iteration 50: 0.305259\n",
      "Cost after iteration 60: 0.304007\n",
      "Cost after iteration 70: 0.303979\n",
      "Cost after iteration 80: 0.303876\n",
      "Cost after iteration 90: 0.304612\n",
      "Cost after iteration 100: 0.303811\n",
      "Cost after iteration 110: 0.305226\n",
      "Cost after iteration 120: 0.306274\n",
      "Cost after iteration 130: 0.303767\n",
      "Cost after iteration 140: 0.303732\n",
      "Cost after iteration 150: 0.303716\n",
      "Cost after iteration 160: 0.303704\n",
      "Cost after iteration 170: 0.303692\n",
      "Cost after iteration 180: 0.303682\n",
      "Cost after iteration 190: 0.303672\n",
      "current iter 2\n",
      "Cost after iteration 0: 0.362001\n",
      "Cost after iteration 10: 0.302574\n",
      "Cost after iteration 20: 0.304845\n",
      "Cost after iteration 30: 0.304155\n",
      "Cost after iteration 40: 0.303951\n",
      "Cost after iteration 50: 0.309428\n",
      "Cost after iteration 60: 0.310917\n",
      "Cost after iteration 70: 0.303842\n",
      "Cost after iteration 80: 0.303730\n",
      "Cost after iteration 90: 0.303796\n",
      "Cost after iteration 100: 0.303693\n",
      "Cost after iteration 110: 0.303659\n",
      "Cost after iteration 120: 0.303643\n",
      "Cost after iteration 130: 0.303628\n",
      "Cost after iteration 140: 0.303615\n",
      "Cost after iteration 150: 0.303603\n",
      "Cost after iteration 160: 0.303593\n",
      "Cost after iteration 170: 0.303583\n",
      "Cost after iteration 180: 0.303707\n",
      "Cost after iteration 190: 0.303565\n",
      "current iter 3\n",
      "Cost after iteration 0: 0.362435\n",
      "Cost after iteration 10: 0.306411\n",
      "Cost after iteration 20: 0.305018\n",
      "Cost after iteration 30: 0.304638\n",
      "Cost after iteration 40: 0.304468\n",
      "Cost after iteration 50: 0.304377\n",
      "Cost after iteration 60: 0.304321\n",
      "Cost after iteration 70: 0.304283\n",
      "Cost after iteration 80: 0.304255\n",
      "Cost after iteration 90: 0.304234\n",
      "Cost after iteration 100: 0.304218\n",
      "Cost after iteration 110: 0.304204\n",
      "Cost after iteration 120: 0.304193\n",
      "Cost after iteration 130: 0.304183\n",
      "Cost after iteration 140: 0.304175\n",
      "Cost after iteration 150: 0.304167\n",
      "Cost after iteration 160: 0.304161\n",
      "Cost after iteration 170: 0.304155\n",
      "Cost after iteration 180: 0.304150\n",
      "Cost after iteration 190: 0.304145\n",
      "current iter 4\n",
      "Cost after iteration 0: 0.362313\n",
      "Cost after iteration 10: 0.305777\n",
      "Cost after iteration 20: 0.304279\n",
      "Cost after iteration 30: 0.304069\n",
      "Cost after iteration 40: 0.303852\n",
      "Cost after iteration 50: 0.303774\n",
      "Cost after iteration 60: 0.303722\n",
      "Cost after iteration 70: 0.303685\n",
      "Cost after iteration 80: 0.303656\n",
      "Cost after iteration 90: 0.303785\n",
      "Cost after iteration 100: 0.303608\n",
      "Cost after iteration 110: 0.303632\n",
      "Cost after iteration 120: 0.303589\n",
      "Cost after iteration 130: 0.303578\n",
      "Cost after iteration 140: 0.302809\n",
      "Cost after iteration 150: 0.303574\n",
      "Cost after iteration 160: 0.303554\n",
      "Cost after iteration 170: 0.303547\n",
      "Cost after iteration 180: 0.303542\n",
      "Cost after iteration 190: 0.303536\n",
      "current iter 5\n",
      "Cost after iteration 0: 0.361902\n",
      "Cost after iteration 10: 0.306047\n",
      "Cost after iteration 20: 0.304380\n",
      "Cost after iteration 30: 0.304091\n",
      "Cost after iteration 40: 0.303939\n",
      "Cost after iteration 50: 0.303844\n",
      "Cost after iteration 60: 0.303782\n",
      "Cost after iteration 70: 0.303738\n",
      "Cost after iteration 80: 0.303706\n",
      "Cost after iteration 90: 0.303683\n",
      "Cost after iteration 100: 0.303665\n",
      "Cost after iteration 110: 0.303650\n",
      "Cost after iteration 120: 0.303638\n",
      "Cost after iteration 130: 0.303627\n",
      "Cost after iteration 140: 0.303619\n",
      "Cost after iteration 150: 0.303611\n",
      "Cost after iteration 160: 0.303604\n",
      "Cost after iteration 170: 0.303598\n",
      "Cost after iteration 180: 0.303593\n",
      "Cost after iteration 190: 0.303588\n",
      "current iter 6\n",
      "Cost after iteration 0: 0.362472\n",
      "Cost after iteration 10: 0.306098\n",
      "Cost after iteration 20: 0.305032\n",
      "Cost after iteration 30: 0.304068\n",
      "Cost after iteration 40: 0.303885\n",
      "Cost after iteration 50: 0.304021\n",
      "Cost after iteration 60: 0.303720\n",
      "Cost after iteration 70: 0.303678\n",
      "Cost after iteration 80: 0.303648\n",
      "Cost after iteration 90: 0.303626\n",
      "Cost after iteration 100: 0.303608\n",
      "Cost after iteration 110: 0.303593\n",
      "Cost after iteration 120: 0.303581\n",
      "Cost after iteration 130: 0.303570\n",
      "Cost after iteration 140: 0.303561\n",
      "Cost after iteration 150: 0.303554\n",
      "Cost after iteration 160: 0.303547\n",
      "Cost after iteration 170: 0.303541\n",
      "Cost after iteration 180: 0.303536\n",
      "Cost after iteration 190: 0.303531\n",
      "current iter 7\n",
      "Cost after iteration 0: 0.363031\n",
      "Cost after iteration 10: 0.306970\n",
      "Cost after iteration 20: 0.305274\n",
      "Cost after iteration 30: 0.304842\n",
      "Cost after iteration 40: 0.304680\n",
      "Cost after iteration 50: 0.304601\n",
      "Cost after iteration 60: 0.304554\n",
      "Cost after iteration 70: 0.304521\n",
      "Cost after iteration 80: 0.304497\n",
      "Cost after iteration 90: 0.304478\n",
      "Cost after iteration 100: 0.304463\n",
      "Cost after iteration 110: 0.304451\n",
      "Cost after iteration 120: 0.304441\n",
      "Cost after iteration 130: 0.304432\n",
      "Cost after iteration 140: 0.304424\n",
      "Cost after iteration 150: 0.304418\n",
      "Cost after iteration 160: 0.304412\n",
      "Cost after iteration 170: 0.304407\n",
      "Cost after iteration 180: 0.304402\n",
      "Cost after iteration 190: 0.304398\n",
      "current iter 8\n",
      "Cost after iteration 0: 0.363055\n",
      "Cost after iteration 10: 0.305878\n",
      "Cost after iteration 20: 0.304456\n",
      "Cost after iteration 30: 0.304128\n",
      "Cost after iteration 40: 0.303999\n",
      "Cost after iteration 50: 0.303925\n",
      "Cost after iteration 60: 0.303875\n",
      "Cost after iteration 70: 0.303839\n",
      "Cost after iteration 80: 0.303812\n",
      "Cost after iteration 90: 0.303791\n",
      "Cost after iteration 100: 0.303774\n",
      "Cost after iteration 110: 0.303760\n",
      "Cost after iteration 120: 0.303749\n",
      "Cost after iteration 130: 0.303739\n",
      "Cost after iteration 140: 0.303730\n",
      "Cost after iteration 150: 0.303723\n",
      "Cost after iteration 160: 0.303716\n",
      "Cost after iteration 170: 0.303710\n",
      "Cost after iteration 180: 0.303705\n",
      "Cost after iteration 190: 0.303701\n",
      "current iter 9\n",
      "Cost after iteration 0: 0.363639\n",
      "Cost after iteration 10: 0.307002\n",
      "Cost after iteration 20: 0.305413\n",
      "Cost after iteration 30: 0.305161\n",
      "Cost after iteration 40: 0.305033\n",
      "Cost after iteration 50: 0.304954\n",
      "Cost after iteration 60: 0.304900\n",
      "Cost after iteration 70: 0.304861\n",
      "Cost after iteration 80: 0.304833\n",
      "Cost after iteration 90: 0.304811\n",
      "Cost after iteration 100: 0.304794\n",
      "Cost after iteration 110: 0.304780\n",
      "Cost after iteration 120: 0.304769\n",
      "Cost after iteration 130: 0.304760\n",
      "Cost after iteration 140: 0.304752\n",
      "Cost after iteration 150: 0.304745\n",
      "Cost after iteration 160: 0.304739\n",
      "Cost after iteration 170: 0.304734\n",
      "Cost after iteration 180: 0.304729\n",
      "Cost after iteration 190: 0.304725\n",
      "current iter 10\n",
      "Cost after iteration 0: 0.363230\n",
      "Cost after iteration 10: 0.305777\n",
      "Cost after iteration 20: 0.304501\n",
      "Cost after iteration 30: 0.304135\n",
      "Cost after iteration 40: 0.303984\n",
      "Cost after iteration 50: 0.303905\n",
      "Cost after iteration 60: 0.303855\n",
      "Cost after iteration 70: 0.303819\n",
      "Cost after iteration 80: 0.303791\n",
      "Cost after iteration 90: 0.303770\n",
      "Cost after iteration 100: 0.303752\n",
      "Cost after iteration 110: 0.303738\n",
      "Cost after iteration 120: 0.303726\n",
      "Cost after iteration 130: 0.303716\n",
      "Cost after iteration 140: 0.303707\n",
      "Cost after iteration 150: 0.303700\n",
      "Cost after iteration 160: 0.303693\n",
      "Cost after iteration 170: 0.303687\n",
      "Cost after iteration 180: 0.303682\n",
      "Cost after iteration 190: 0.303677\n",
      "current iter 11\n",
      "Cost after iteration 0: 0.361542\n",
      "Cost after iteration 10: 0.307650\n",
      "Cost after iteration 20: 0.304057\n",
      "Cost after iteration 30: 0.303739\n",
      "Cost after iteration 40: 0.303586\n",
      "Cost after iteration 50: 0.303492\n",
      "Cost after iteration 60: 0.303428\n",
      "Cost after iteration 70: 0.303383\n",
      "Cost after iteration 80: 0.303350\n",
      "Cost after iteration 90: 0.303325\n",
      "Cost after iteration 100: 0.303305\n",
      "Cost after iteration 110: 0.303290\n",
      "Cost after iteration 120: 0.303277\n",
      "Cost after iteration 130: 0.303266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 140: 0.303257\n",
      "Cost after iteration 150: 0.303249\n",
      "Cost after iteration 160: 0.303243\n",
      "Cost after iteration 170: 0.303237\n",
      "Cost after iteration 180: 0.303231\n",
      "Cost after iteration 190: 0.303227\n",
      "current iter 12\n",
      "Cost after iteration 0: 0.362039\n",
      "Cost after iteration 10: 0.306754\n",
      "Cost after iteration 20: 0.304809\n",
      "Cost after iteration 30: 0.304388\n",
      "Cost after iteration 40: 0.304243\n",
      "Cost after iteration 50: 0.304169\n",
      "Cost after iteration 60: 0.304120\n",
      "Cost after iteration 70: 0.304086\n",
      "Cost after iteration 80: 0.304060\n",
      "Cost after iteration 90: 0.304040\n",
      "Cost after iteration 100: 0.304024\n",
      "Cost after iteration 110: 0.304010\n",
      "Cost after iteration 120: 0.303999\n",
      "Cost after iteration 130: 0.303990\n",
      "Cost after iteration 140: 0.303981\n",
      "Cost after iteration 150: 0.303974\n",
      "Cost after iteration 160: 0.303968\n",
      "Cost after iteration 170: 0.303962\n",
      "Cost after iteration 180: 0.303957\n",
      "Cost after iteration 190: 0.303953\n",
      "current iter 13\n",
      "Cost after iteration 0: 0.361392\n",
      "Cost after iteration 10: 0.305501\n",
      "Cost after iteration 20: 0.303851\n",
      "Cost after iteration 30: 0.303559\n",
      "Cost after iteration 40: 0.303421\n",
      "Cost after iteration 50: 0.303336\n",
      "Cost after iteration 60: 0.303279\n",
      "Cost after iteration 70: 0.303237\n",
      "Cost after iteration 80: 0.303206\n",
      "Cost after iteration 90: 0.303181\n",
      "Cost after iteration 100: 0.303161\n",
      "Cost after iteration 110: 0.303146\n",
      "Cost after iteration 120: 0.303132\n",
      "Cost after iteration 130: 0.303121\n",
      "Cost after iteration 140: 0.303112\n",
      "Cost after iteration 150: 0.303104\n",
      "Cost after iteration 160: 0.303097\n",
      "Cost after iteration 170: 0.303091\n",
      "Cost after iteration 180: 0.303086\n",
      "Cost after iteration 190: 0.303081\n",
      "current iter 14\n",
      "Cost after iteration 0: 0.363362\n",
      "Cost after iteration 10: 0.307476\n",
      "Cost after iteration 20: 0.305706\n",
      "Cost after iteration 30: 0.305373\n",
      "Cost after iteration 40: 0.305218\n",
      "Cost after iteration 50: 0.305139\n",
      "Cost after iteration 60: 0.305091\n",
      "Cost after iteration 70: 0.305059\n",
      "Cost after iteration 80: 0.305035\n",
      "Cost after iteration 90: 0.305017\n",
      "Cost after iteration 100: 0.305002\n",
      "Cost after iteration 110: 0.304990\n",
      "Cost after iteration 120: 0.304979\n",
      "Cost after iteration 130: 0.304971\n",
      "Cost after iteration 140: 0.304963\n",
      "Cost after iteration 150: 0.304956\n",
      "Cost after iteration 160: 0.304951\n",
      "Cost after iteration 170: 0.304946\n",
      "Cost after iteration 180: 0.304941\n",
      "Cost after iteration 190: 0.304937\n",
      "current iter 15\n",
      "Cost after iteration 0: 0.371697\n",
      "Cost after iteration 10: 0.307143\n",
      "Cost after iteration 20: 0.305980\n",
      "Cost after iteration 30: 0.311315\n",
      "Cost after iteration 40: 0.305618\n",
      "Cost after iteration 50: 0.305422\n",
      "Cost after iteration 60: 0.305353\n",
      "Cost after iteration 70: 0.305312\n",
      "Cost after iteration 80: 0.305283\n",
      "Cost after iteration 90: 0.305260\n",
      "Cost after iteration 100: 0.305236\n",
      "Cost after iteration 110: 0.305163\n",
      "Cost after iteration 120: 0.303010\n",
      "Cost after iteration 130: 0.299974\n",
      "Cost after iteration 140: 0.299624\n",
      "Cost after iteration 150: 0.299260\n",
      "Cost after iteration 160: 0.299134\n",
      "Cost after iteration 170: 0.299066\n",
      "Cost after iteration 180: 0.299020\n",
      "Cost after iteration 190: 0.298986\n",
      "current iter 16\n",
      "Cost after iteration 0: 0.366087\n",
      "Cost after iteration 10: 0.300837\n",
      "Cost after iteration 20: 0.299636\n",
      "Cost after iteration 30: 0.299257\n",
      "Cost after iteration 40: 0.299057\n",
      "Cost after iteration 50: 0.298939\n",
      "Cost after iteration 60: 0.298861\n",
      "Cost after iteration 70: 0.298698\n",
      "Cost after iteration 80: 0.298219\n",
      "Cost after iteration 90: 0.298258\n",
      "Cost after iteration 100: 0.298557\n",
      "Cost after iteration 110: 0.298076\n",
      "Cost after iteration 120: 0.298041\n",
      "Cost after iteration 130: 0.298019\n",
      "Cost after iteration 140: 0.298001\n",
      "Cost after iteration 150: 0.297986\n",
      "Cost after iteration 160: 0.299629\n",
      "Cost after iteration 170: 0.297984\n",
      "Cost after iteration 180: 0.297957\n",
      "Cost after iteration 190: 0.297943\n",
      "current iter 17\n",
      "Cost after iteration 0: 0.360600\n",
      "Cost after iteration 10: 0.301015\n",
      "Cost after iteration 20: 0.299444\n",
      "Cost after iteration 30: 0.299334\n",
      "Cost after iteration 40: 0.298997\n",
      "Cost after iteration 50: 0.299421\n",
      "Cost after iteration 60: 0.298896\n",
      "Cost after iteration 70: 0.298815\n",
      "Cost after iteration 80: 0.298775\n",
      "Cost after iteration 90: 0.298746\n",
      "Cost after iteration 100: 0.298723\n",
      "Cost after iteration 110: 0.298704\n",
      "Cost after iteration 120: 0.299516\n",
      "Cost after iteration 130: 0.298687\n",
      "Cost after iteration 140: 0.298660\n",
      "Cost after iteration 150: 0.298648\n",
      "Cost after iteration 160: 0.298636\n",
      "Cost after iteration 170: 0.301174\n",
      "Cost after iteration 180: 0.298671\n",
      "Cost after iteration 190: 0.298633\n",
      "current iter 18\n",
      "Cost after iteration 0: 0.343429\n",
      "Cost after iteration 10: 0.300361\n",
      "Cost after iteration 20: 0.298911\n",
      "Cost after iteration 30: 0.298734\n",
      "Cost after iteration 40: 0.298449\n",
      "Cost after iteration 50: 0.303085\n",
      "Cost after iteration 60: 0.298898\n",
      "Cost after iteration 70: 0.298327\n",
      "Cost after iteration 80: 0.298302\n",
      "Cost after iteration 90: 0.298285\n",
      "Cost after iteration 100: 0.298271\n",
      "Cost after iteration 110: 0.298260\n",
      "Cost after iteration 120: 0.298250\n",
      "Cost after iteration 130: 0.298242\n",
      "Cost after iteration 140: 0.298235\n",
      "Cost after iteration 150: 0.298229\n",
      "Cost after iteration 160: 0.298223\n",
      "Cost after iteration 170: 0.298218\n",
      "Cost after iteration 180: 0.298213\n",
      "Cost after iteration 190: 0.298209\n",
      "current iter 19\n",
      "Cost after iteration 0: 0.319857\n",
      "Cost after iteration 10: 0.299430\n",
      "Cost after iteration 20: 0.299462\n",
      "Cost after iteration 30: 0.298438\n",
      "Cost after iteration 40: 0.298183\n",
      "Cost after iteration 50: 0.298065\n",
      "Cost after iteration 60: 0.298014\n",
      "Cost after iteration 70: 0.297986\n",
      "Cost after iteration 80: 0.297966\n",
      "Cost after iteration 90: 0.297214\n",
      "Cost after iteration 100: 0.297946\n",
      "Cost after iteration 110: 0.304034\n",
      "Cost after iteration 120: 0.297982\n",
      "Cost after iteration 130: 0.298690\n",
      "Cost after iteration 140: 0.297925\n",
      "Cost after iteration 150: 0.293177\n",
      "Cost after iteration 160: 0.297907\n",
      "Cost after iteration 170: 0.297891\n",
      "Cost after iteration 180: 0.297886\n",
      "Cost after iteration 190: 0.297881\n",
      "current iter 20\n",
      "Cost after iteration 0: 0.320668\n",
      "Cost after iteration 10: 0.300108\n",
      "Cost after iteration 20: 0.298896\n",
      "Cost after iteration 30: 0.298631\n",
      "Cost after iteration 40: 0.300080\n",
      "Cost after iteration 50: 0.298439\n",
      "Cost after iteration 60: 0.297858\n",
      "Cost after iteration 70: 0.334240\n",
      "Cost after iteration 80: 0.298477\n",
      "Cost after iteration 90: 0.298454\n",
      "Cost after iteration 100: 0.298439\n",
      "Cost after iteration 110: 0.298427\n",
      "Cost after iteration 120: 0.298415\n",
      "Cost after iteration 130: 0.298404\n",
      "Cost after iteration 140: 0.298393\n",
      "Cost after iteration 150: 0.298949\n",
      "Cost after iteration 160: 0.298381\n",
      "Cost after iteration 170: 0.298366\n",
      "Cost after iteration 180: 0.298356\n",
      "Cost after iteration 190: 0.302385\n",
      "current iter 21\n",
      "Cost after iteration 0: 0.320376\n",
      "Cost after iteration 10: 0.299215\n",
      "Cost after iteration 20: 0.298420\n",
      "Cost after iteration 30: 0.298187\n",
      "Cost after iteration 40: 0.298063\n",
      "Cost after iteration 50: 0.297996\n",
      "Cost after iteration 60: 0.297957\n",
      "Cost after iteration 70: 0.297932\n",
      "Cost after iteration 80: 0.297913\n",
      "Cost after iteration 90: 0.299775\n",
      "Cost after iteration 100: 0.298244\n",
      "Cost after iteration 110: 0.297876\n",
      "Cost after iteration 120: 0.297866\n",
      "Cost after iteration 130: 0.297858\n",
      "Cost after iteration 140: 0.297851\n",
      "Cost after iteration 150: 0.297845\n",
      "Cost after iteration 160: 0.297839\n",
      "Cost after iteration 170: 0.297833\n",
      "Cost after iteration 180: 0.297828\n",
      "Cost after iteration 190: 0.297824\n",
      "current iter 22\n",
      "Cost after iteration 0: 0.321627\n",
      "Cost after iteration 10: 0.300699\n",
      "Cost after iteration 20: 0.299514\n",
      "Cost after iteration 30: 0.299281\n",
      "Cost after iteration 40: 0.299210\n",
      "Cost after iteration 50: 0.299173\n",
      "Cost after iteration 60: 0.299147\n",
      "Cost after iteration 70: 0.299127\n",
      "Cost after iteration 80: 0.299111\n",
      "Cost after iteration 90: 0.299099\n",
      "Cost after iteration 100: 0.299088\n",
      "Cost after iteration 110: 0.299079\n",
      "Cost after iteration 120: 0.299070\n",
      "Cost after iteration 130: 0.299063\n",
      "Cost after iteration 140: 0.299056\n",
      "Cost after iteration 150: 0.299049\n",
      "Cost after iteration 160: 0.299043\n",
      "Cost after iteration 170: 0.299037\n",
      "Cost after iteration 180: 0.299031\n",
      "Cost after iteration 190: 0.299026\n",
      "current iter 23\n",
      "Cost after iteration 0: 0.320724\n",
      "Cost after iteration 10: 0.286311\n",
      "Cost after iteration 20: 0.298165\n",
      "Cost after iteration 30: 0.297816\n",
      "Cost after iteration 40: 0.297702\n",
      "Cost after iteration 50: 0.297639\n",
      "Cost after iteration 60: 0.337054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 70: 0.297610\n",
      "Cost after iteration 80: 0.297251\n",
      "Cost after iteration 90: 0.297549\n",
      "Cost after iteration 100: 0.297513\n",
      "Cost after iteration 110: 0.297468\n",
      "Cost after iteration 120: 0.305608\n",
      "Cost after iteration 130: 0.297591\n",
      "Cost after iteration 140: 0.297532\n",
      "Cost after iteration 150: 0.297523\n",
      "Cost after iteration 160: 0.297516\n",
      "Cost after iteration 170: 0.297510\n",
      "Cost after iteration 180: 0.297646\n",
      "Cost after iteration 190: 0.297546\n",
      "current iter 24\n",
      "Cost after iteration 0: 0.318651\n",
      "Cost after iteration 10: 0.299869\n",
      "Cost after iteration 20: 0.298182\n",
      "Cost after iteration 30: 0.298011\n",
      "Cost after iteration 40: 0.297933\n",
      "Cost after iteration 50: 0.300680\n",
      "Cost after iteration 60: 0.300197\n",
      "Cost after iteration 70: 0.297837\n",
      "Cost after iteration 80: 0.297809\n",
      "Cost after iteration 90: 0.297793\n",
      "Cost after iteration 100: 0.297780\n",
      "Cost after iteration 110: 0.297769\n",
      "Cost after iteration 120: 0.297760\n",
      "Cost after iteration 130: 0.306687\n",
      "Cost after iteration 140: 0.299228\n",
      "Cost after iteration 150: 0.297770\n",
      "Cost after iteration 160: 0.297737\n",
      "Cost after iteration 170: 0.297916\n",
      "Cost after iteration 180: 0.297742\n",
      "Cost after iteration 190: 0.297728\n",
      "current iter 25\n",
      "Cost after iteration 0: 0.318533\n",
      "Cost after iteration 10: 0.299484\n",
      "Cost after iteration 20: 0.298121\n",
      "Cost after iteration 30: 0.297867\n",
      "Cost after iteration 40: 0.297781\n",
      "Cost after iteration 50: 0.297733\n",
      "Cost after iteration 60: 0.297700\n",
      "Cost after iteration 70: 0.295434\n",
      "Cost after iteration 80: 0.297773\n",
      "Cost after iteration 90: 0.297660\n",
      "Cost after iteration 100: 0.297634\n",
      "Cost after iteration 110: 0.297622\n",
      "Cost after iteration 120: 0.297612\n",
      "Cost after iteration 130: 0.297605\n",
      "Cost after iteration 140: 0.297598\n",
      "Cost after iteration 150: 0.297592\n",
      "Cost after iteration 160: 0.297586\n",
      "Cost after iteration 170: 0.297581\n",
      "Cost after iteration 180: 0.297577\n",
      "Cost after iteration 190: 0.297573\n",
      "current iter 26\n",
      "Cost after iteration 0: 0.517535\n",
      "Cost after iteration 10: 0.298357\n",
      "Cost after iteration 20: 0.297823\n",
      "Cost after iteration 30: 0.297722\n",
      "Cost after iteration 40: 0.297665\n",
      "Cost after iteration 50: 0.297627\n",
      "Cost after iteration 60: 0.297599\n",
      "Cost after iteration 70: 0.297578\n",
      "Cost after iteration 80: 0.297560\n",
      "Cost after iteration 90: 0.297546\n",
      "Cost after iteration 100: 0.297533\n",
      "Cost after iteration 110: 0.297521\n",
      "Cost after iteration 120: 0.297511\n",
      "Cost after iteration 130: 0.297500\n",
      "Cost after iteration 140: 0.297490\n",
      "Cost after iteration 150: 0.297480\n",
      "Cost after iteration 160: 0.297439\n",
      "Cost after iteration 170: 0.297392\n",
      "Cost after iteration 180: 0.297174\n",
      "Cost after iteration 190: 0.298598\n",
      "current iter 27\n",
      "Cost after iteration 0: 0.318573\n",
      "Cost after iteration 10: 0.299652\n",
      "Cost after iteration 20: 0.297958\n",
      "Cost after iteration 30: 0.297772\n",
      "Cost after iteration 40: 0.297695\n",
      "Cost after iteration 50: 0.297648\n",
      "Cost after iteration 60: 0.297616\n",
      "Cost after iteration 70: 0.297592\n",
      "Cost after iteration 80: 0.297574\n",
      "Cost after iteration 90: 0.297560\n",
      "Cost after iteration 100: 0.297548\n",
      "Cost after iteration 110: 0.297539\n",
      "Cost after iteration 120: 0.297531\n",
      "Cost after iteration 130: 0.297524\n",
      "Cost after iteration 140: 0.297518\n",
      "Cost after iteration 150: 0.297512\n",
      "Cost after iteration 160: 0.297508\n",
      "Cost after iteration 170: 0.297503\n",
      "Cost after iteration 180: 0.297499\n",
      "Cost after iteration 190: 0.297496\n",
      "current iter 28\n",
      "Cost after iteration 0: 0.339213\n",
      "Cost after iteration 10: 0.297624\n",
      "Cost after iteration 20: 0.295772\n",
      "Cost after iteration 30: 0.295475\n",
      "Cost after iteration 40: 0.295328\n",
      "Cost after iteration 50: 0.295234\n",
      "Cost after iteration 60: 0.295169\n",
      "Cost after iteration 70: 0.295120\n",
      "Cost after iteration 80: 0.295083\n",
      "Cost after iteration 90: 0.295053\n",
      "Cost after iteration 100: 0.295029\n",
      "Cost after iteration 110: 0.295009\n",
      "Cost after iteration 120: 0.294991\n",
      "Cost after iteration 130: 0.294977\n",
      "Cost after iteration 140: 0.294964\n",
      "Cost after iteration 150: 0.294952\n",
      "Cost after iteration 160: 0.294942\n",
      "Cost after iteration 170: 0.294933\n",
      "Cost after iteration 180: 0.294925\n",
      "Cost after iteration 190: 0.294918\n",
      "current iter 29\n",
      "Cost after iteration 0: 0.358960\n",
      "Cost after iteration 10: 0.298850\n",
      "Cost after iteration 20: 0.296387\n",
      "Cost after iteration 30: 0.295829\n",
      "Cost after iteration 40: 0.295555\n",
      "Cost after iteration 50: 0.295405\n",
      "Cost after iteration 60: 0.295312\n",
      "Cost after iteration 70: 0.295249\n",
      "Cost after iteration 80: 0.295203\n",
      "Cost after iteration 90: 0.295168\n",
      "Cost after iteration 100: 0.295140\n",
      "Cost after iteration 110: 0.295117\n",
      "Cost after iteration 120: 0.295098\n",
      "Cost after iteration 130: 0.295082\n",
      "Cost after iteration 140: 0.295068\n",
      "Cost after iteration 150: 0.295056\n",
      "Cost after iteration 160: 0.295045\n",
      "Cost after iteration 170: 0.295035\n",
      "Cost after iteration 180: 0.295027\n",
      "Cost after iteration 190: 0.295019\n",
      "current iter 30\n",
      "Cost after iteration 0: 0.359698\n",
      "Cost after iteration 10: 0.300564\n",
      "Cost after iteration 20: 0.298085\n",
      "Cost after iteration 30: 0.297533\n",
      "Cost after iteration 40: 0.297310\n",
      "Cost after iteration 50: 0.297202\n",
      "Cost after iteration 60: 0.297137\n",
      "Cost after iteration 70: 0.297090\n",
      "Cost after iteration 80: 0.297055\n",
      "Cost after iteration 90: 0.297027\n",
      "Cost after iteration 100: 0.297004\n",
      "Cost after iteration 110: 0.296986\n",
      "Cost after iteration 120: 0.296970\n",
      "Cost after iteration 130: 0.296957\n",
      "Cost after iteration 140: 0.296946\n",
      "Cost after iteration 150: 0.296936\n",
      "Cost after iteration 160: 0.296927\n",
      "Cost after iteration 170: 0.296919\n",
      "Cost after iteration 180: 0.296913\n",
      "Cost after iteration 190: 0.296907\n",
      "current iter 31\n",
      "Cost after iteration 0: 0.362656\n",
      "Cost after iteration 10: 0.299629\n",
      "Cost after iteration 20: 0.298296\n",
      "Cost after iteration 30: 0.297699\n",
      "Cost after iteration 40: 0.297529\n",
      "Cost after iteration 50: 0.297427\n",
      "Cost after iteration 60: 0.297358\n",
      "Cost after iteration 70: 0.297311\n",
      "Cost after iteration 80: 0.297276\n",
      "Cost after iteration 90: 0.297249\n",
      "Cost after iteration 100: 0.297229\n",
      "Cost after iteration 110: 0.297212\n",
      "Cost after iteration 120: 0.297198\n",
      "Cost after iteration 130: 0.297187\n",
      "Cost after iteration 140: 0.297177\n",
      "Cost after iteration 150: 0.297168\n",
      "Cost after iteration 160: 0.297160\n",
      "Cost after iteration 170: 0.297153\n",
      "Cost after iteration 180: 0.297147\n",
      "Cost after iteration 190: 0.297142\n",
      "current iter 32\n",
      "Cost after iteration 0: 0.367962\n",
      "Cost after iteration 10: 0.302756\n",
      "Cost after iteration 20: 0.301108\n",
      "Cost after iteration 30: 0.300858\n",
      "Cost after iteration 40: 0.300739\n",
      "Cost after iteration 50: 0.300666\n",
      "Cost after iteration 60: 0.300616\n",
      "Cost after iteration 70: 0.300580\n",
      "Cost after iteration 80: 0.300553\n",
      "Cost after iteration 90: 0.300531\n",
      "Cost after iteration 100: 0.300514\n",
      "Cost after iteration 110: 0.300500\n",
      "Cost after iteration 120: 0.300488\n",
      "Cost after iteration 130: 0.300479\n",
      "Cost after iteration 140: 0.300470\n",
      "Cost after iteration 150: 0.300463\n",
      "Cost after iteration 160: 0.300457\n",
      "Cost after iteration 170: 0.300451\n",
      "Cost after iteration 180: 0.300447\n",
      "Cost after iteration 190: 0.300442\n",
      "current iter 33\n",
      "Cost after iteration 0: 0.370381\n",
      "Cost after iteration 10: 0.301983\n",
      "Cost after iteration 20: 0.300729\n",
      "Cost after iteration 30: 0.300461\n",
      "Cost after iteration 40: 0.300331\n",
      "Cost after iteration 50: 0.300249\n",
      "Cost after iteration 60: 0.300192\n",
      "Cost after iteration 70: 0.300149\n",
      "Cost after iteration 80: 0.300116\n",
      "Cost after iteration 90: 0.300090\n",
      "Cost after iteration 100: 0.300068\n",
      "Cost after iteration 110: 0.300050\n",
      "Cost after iteration 120: 0.300034\n",
      "Cost after iteration 130: 0.300021\n",
      "Cost after iteration 140: 0.300009\n",
      "Cost after iteration 150: 0.299999\n",
      "Cost after iteration 160: 0.299990\n",
      "Cost after iteration 170: 0.299982\n",
      "Cost after iteration 180: 0.299975\n",
      "Cost after iteration 190: 0.299969\n",
      "current iter 34\n",
      "Cost after iteration 0: 0.360456\n",
      "Cost after iteration 10: 0.296834\n",
      "Cost after iteration 20: 0.295536\n",
      "Cost after iteration 30: 0.295014\n",
      "Cost after iteration 40: 0.294738\n",
      "Cost after iteration 50: 0.294576\n",
      "Cost after iteration 60: 0.294471\n",
      "Cost after iteration 70: 0.294397\n",
      "Cost after iteration 80: 0.294343\n",
      "Cost after iteration 90: 0.294302\n",
      "Cost after iteration 100: 0.294271\n",
      "Cost after iteration 110: 0.294245\n",
      "Cost after iteration 120: 0.294224\n",
      "Cost after iteration 130: 0.294207\n",
      "Cost after iteration 140: 0.294192\n",
      "Cost after iteration 150: 0.294179\n",
      "Cost after iteration 160: 0.294168\n",
      "Cost after iteration 170: 0.294158\n",
      "Cost after iteration 180: 0.294150\n",
      "Cost after iteration 190: 0.294142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current iter 35\n",
      "Cost after iteration 0: 0.356194\n",
      "Cost after iteration 10: 0.300158\n",
      "Cost after iteration 20: 0.296685\n",
      "Cost after iteration 30: 0.296090\n",
      "Cost after iteration 40: 0.295879\n",
      "Cost after iteration 50: 0.295757\n",
      "Cost after iteration 60: 0.295680\n",
      "Cost after iteration 70: 0.295627\n",
      "Cost after iteration 80: 0.295588\n",
      "Cost after iteration 90: 0.295558\n",
      "Cost after iteration 100: 0.295534\n",
      "Cost after iteration 110: 0.295514\n",
      "Cost after iteration 120: 0.295497\n",
      "Cost after iteration 130: 0.295482\n",
      "Cost after iteration 140: 0.295470\n",
      "Cost after iteration 150: 0.295459\n",
      "Cost after iteration 160: 0.295449\n",
      "Cost after iteration 170: 0.295440\n",
      "Cost after iteration 180: 0.295433\n",
      "Cost after iteration 190: 0.295425\n",
      "current iter 36\n",
      "Cost after iteration 0: 0.358684\n",
      "Cost after iteration 10: 0.299382\n",
      "Cost after iteration 20: 0.297095\n",
      "Cost after iteration 30: 0.296581\n",
      "Cost after iteration 40: 0.296337\n",
      "Cost after iteration 50: 0.296212\n",
      "Cost after iteration 60: 0.296141\n",
      "Cost after iteration 70: 0.296093\n",
      "Cost after iteration 80: 0.296057\n",
      "Cost after iteration 90: 0.296030\n",
      "Cost after iteration 100: 0.296008\n",
      "Cost after iteration 110: 0.295989\n",
      "Cost after iteration 120: 0.295973\n",
      "Cost after iteration 130: 0.295960\n",
      "Cost after iteration 140: 0.295949\n",
      "Cost after iteration 150: 0.295938\n",
      "Cost after iteration 160: 0.295929\n",
      "Cost after iteration 170: 0.295921\n",
      "Cost after iteration 180: 0.295914\n",
      "Cost after iteration 190: 0.295908\n",
      "current iter 37\n",
      "Cost after iteration 0: 0.358867\n",
      "Cost after iteration 10: 0.299034\n",
      "Cost after iteration 20: 0.296984\n",
      "Cost after iteration 30: 0.296497\n",
      "Cost after iteration 40: 0.296310\n",
      "Cost after iteration 50: 0.296209\n",
      "Cost after iteration 60: 0.296142\n",
      "Cost after iteration 70: 0.296093\n",
      "Cost after iteration 80: 0.296055\n",
      "Cost after iteration 90: 0.296026\n",
      "Cost after iteration 100: 0.296002\n",
      "Cost after iteration 110: 0.295982\n",
      "Cost after iteration 120: 0.295966\n",
      "Cost after iteration 130: 0.295951\n",
      "Cost after iteration 140: 0.295939\n",
      "Cost after iteration 150: 0.295929\n",
      "Cost after iteration 160: 0.295919\n",
      "Cost after iteration 170: 0.295911\n",
      "Cost after iteration 180: 0.295904\n",
      "Cost after iteration 190: 0.295897\n",
      "current iter 38\n",
      "Cost after iteration 0: 0.358131\n",
      "Cost after iteration 10: 0.298101\n",
      "Cost after iteration 20: 0.296214\n",
      "Cost after iteration 30: 0.295709\n",
      "Cost after iteration 40: 0.295476\n",
      "Cost after iteration 50: 0.295343\n",
      "Cost after iteration 60: 0.295259\n",
      "Cost after iteration 70: 0.295201\n",
      "Cost after iteration 80: 0.295159\n",
      "Cost after iteration 90: 0.295127\n",
      "Cost after iteration 100: 0.295102\n",
      "Cost after iteration 110: 0.295081\n",
      "Cost after iteration 120: 0.295065\n",
      "Cost after iteration 130: 0.295051\n",
      "Cost after iteration 140: 0.295039\n",
      "Cost after iteration 150: 0.295028\n",
      "Cost after iteration 160: 0.295019\n",
      "Cost after iteration 170: 0.295011\n",
      "Cost after iteration 180: 0.295004\n",
      "Cost after iteration 190: 0.294997\n"
     ]
    }
   ],
   "source": [
    "nn = train_list_of_dataset(nn,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique ans 25\n",
      "unique input 2393\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ans = nn.predict(x_real_train)\n",
    "find_classification_based_on_distance(ans,og_pred)\n",
    "print('unique ans', len(np.unique(ans,axis = 0)))\n",
    "print('unique input',len(np.unique(x_real_train,axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.366206896551724\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "p = []\n",
    "for i in range(len(x_real_train)):\n",
    "    for j in range(len(x_real_train[0])):\n",
    "        if x_real_train[i][j]!=ans[i][j]:\n",
    "            count+=1\n",
    "           \n",
    "print(count/len(x_real_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "p = []\n",
    "for i in range(len(x_real_train)):\n",
    "    if np.array_equal(x_real_train[i],ans[i]):\n",
    "        count+=1\n",
    "        if np.count_nonzero(ans[i]) > 0:\n",
    "            p.append(i)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[408]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[223, 272, 408, 983, 1004, 1015, 1338, 1450, 1452, 2087, 2348, 2489, 2493]\n"
     ]
    }
   ],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_classification_based_on_prob(ans,og_pred)\n",
    "print('unique ans', len(np.unique(ans,axis = 0)))\n",
    "print('unique input',len(np.unique(x,axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = nn.predict(x)\n",
    "# find_classification_based_on_distance(ans,og_pred)\n",
    "find_classification_based_on_prob(ans,og_pred)\n",
    "print('unique ans', len(np.unique(ans,axis = 0)))\n",
    "print('unique input',len(np.unique(x,axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THOUGHTS: Might need another algorithm to autocomple those column that is nan, based on knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO : CREATE FUNCTION TO RECOVER CLASSIFICATION DATASET INTO HUMAN-Fridendly DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_answer(predict_arr,dic):\n",
    "    reverse_dic = create_reverse_dictionary(dic)\n",
    "    #init an dic as final anser array\n",
    "    ans_arr = []\n",
    "    #create array with all dict keys based on insertion order\n",
    "    dic_arr = list(dic.keys())\n",
    "    #recover predicted candidate sequentilaay\n",
    "    for i in range(len(predict_arr)):\n",
    "        tmp_ans_array = {} # cur dict to store anser for this candidate\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        #loop through dictionary array to recover each attribute column\n",
    "        for j in range(len(dic_arr)):\n",
    "            cur_dict_key = dic_arr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_reverse_dic_arr = reverse_dic.get(cur_dict_key)\n",
    "            cur_dict_length = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_length #update current cutting position\n",
    "            #restore\n",
    "            tmp_cutting_arr = predict_arr[i][cur_loc:cur_des]\n",
    "            tag = tmp_cutting_arr[-1]\n",
    "            if tag == 0:\n",
    "                tmp_ans_array[cur_dict_key] = 'NA'\n",
    "            else:\n",
    "                #find the position where tag is 1\n",
    "                loc = 0\n",
    "                for pos in range(len(tmp_cutting_arr)):\n",
    "                    if tmp_cutting_arr[pos] == 1:\n",
    "                        loc = pos\n",
    "                        break\n",
    "                #now based on location, create cover anser\n",
    "                #case one loc is 0\n",
    "                if loc == 0:\n",
    "                    ans =\"<\" + str(cur_reverse_dic_arr.get(loc))\n",
    "                    tmp_ans_array[cur_dict_key] = ans\n",
    "                else:\n",
    "                    last_loc = loc-1\n",
    "                    ans=\"\"\n",
    "                    ans+=str(cur_reverse_dic_arr.get(last_loc))+\" < \"\n",
    "                    ans+=str(cur_reverse_dic_arr.get(loc))\n",
    "                    tmp_ans_array[cur_dict_key] = ans\n",
    "#                 ans =cur_reverse_dic_arr.get(loc)\n",
    "#                 tmp_ans_array[cur_dict_key] = ans\n",
    "        \n",
    "            cur_loc+=cur_dict_length\n",
    "        #append current tmp candidate array to finaly answer\n",
    "        ans_arr.append(tmp_ans_array)\n",
    "    \n",
    "    return ans_arr\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE FUNCTION FOR CROSS VALIDATION\n",
    "Assess the accuracy by count how many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(arr_all_candidate,num_fold,og_pred,size_hidden_layer,iter_num,learning_rate):\n",
    "    cost = 0\n",
    "    #create num_fold equal size fold from input nparray\n",
    "    list_of_fold = np.array_split(arr_all_candidate,num_fold)\n",
    "    \n",
    "    #need to append training dataset together, while leave testing set alone\n",
    "    for i in range(num_fold):\n",
    "        x_pred = list_of_fold[i]\n",
    "        \n",
    "        tmp = 0\n",
    "        for j in range(0,num_fold):\n",
    "            if j == i:\n",
    "                continue\n",
    "            else:\n",
    "                \n",
    "                if tmp == 0:\n",
    "                    ans = np.asarray(list_of_fold[j])\n",
    "                    tmp+=1\n",
    "                \n",
    "                elif tmp > 0:\n",
    "                    \n",
    "                    ans = cro_new_append(ans,list_of_fold[j])\n",
    "\n",
    "            \n",
    "        #finished rebuilding training dataset\n",
    "        #start training neural network\n",
    "        nn = NeuralNetwork(size_hidden_layer,iter_num,learning_rate) #size of input layer\n",
    "        \n",
    "        nn.fit(ans,ans)\n",
    "        predict_ans = nn.predict(x_pred)\n",
    "        find_classification_based_on_distance(predict_ans,og_pred)\n",
    "        #coun\n",
    "        tmpcost = cost_function(predict_ans,x_pred)\n",
    "        cost+=tmpcost\n",
    "        print('#### current cutting position is : ', i, 'current cost is: ',tmpcost)\n",
    "    print('final avg cost is : ', cost/num_fold)\n",
    "    return cost/num_fold\n",
    "\n",
    "\n",
    "\n",
    "def cost_function(x,y):\n",
    "    #compute the average num of wrongly predicted column as cost function\n",
    "    total_num = len(x)\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        tmpcount = 0\n",
    "        for j in range(len(x[i])):\n",
    "            if x[i][j]!=y[i][j]:\n",
    "                tmpcount+=1\n",
    "        count+=tmpcount\n",
    "    avg_count = count/total_num\n",
    "    return avg_count\n",
    "def cross_valid_append_arr(old,new):\n",
    "    ans = np.append([old],[new],axis = 0)\n",
    "    return ans\n",
    "\n",
    "def cro_new_append(old,new):\n",
    "    #num_col\n",
    "    num_col = len(old[0])\n",
    "    new_ans = []\n",
    "    for i in range(len(old)):\n",
    "        new_ans.append(old[i])\n",
    "    for i in range(len(new)):\n",
    "        new_ans.append(new[i])\n",
    "    return np.asarray(new_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(X_train_new,10,og_pred,60,4000,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
