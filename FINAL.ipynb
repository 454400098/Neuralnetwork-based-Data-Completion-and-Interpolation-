{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Add one more utility function to answer bonus 2 ( WHY THE FUCK SO MANY Q)\n",
    "# Relu as activation function in hidden layer works better than sigmoid\n",
    "# The most powerful NN setting:\n",
    "1. only 1 hidden layer\n",
    "2. 2000 neurons\n",
    "3. learning rate: 0.1\n",
    "\n",
    "# NO MACHINE LEARNING LIBARY USED FOR TRAINING/REGRESSION/CLASSFICATION\n",
    "# sklean.feature_extraction ONLY USED for language data into Vector transformation\n",
    "# preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer#ONLY USED FOR TRANSFORM FROM LANGUAGE TO VECTOR\n",
    "from numpy import linalg as LA\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (17,55,59,61,65,68,69,70,83,90,91,92,93,120,121,122,123,126,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./Tab.delimited.Cleaned.dataset.WITH.variable.labels.csv', sep='\\t',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,11,12,19,20,129,132,169,230) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.read_csv('./ML3AllSites.csv', sep=',',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, n_h, n_iterate=10, learning_rate=1, dic = []):\n",
    "        self.n_x = None  # size of the input layer\n",
    "        self.n_h = n_h  # size of the hidden layer\n",
    "        self.n_y = None # size of the output layer\n",
    "        self.W1 = None\n",
    "        self.W2 = None\n",
    "        self.b1 = None\n",
    "        self.b2 = None\n",
    "        self.A1 = None\n",
    "        self.A2 = None  # sigmoid output of the second activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterate = n_iterate\n",
    "        self.lamda = 0.01\n",
    "#         self.dic = dic\n",
    "        self.dic = dic\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        self.W1 = np.random.randn(self.n_h, self.n_x) * 0.01\n",
    "        self.b1 = np.zeros((self.n_h, 1))\n",
    "        self.W2 = np.random.randn(self.n_y, self.n_h) * 0.01\n",
    "        self.b2 = np.zeros((self.n_y, 1))\n",
    "       \n",
    "    def MSE(self,Y):\n",
    "        cost = 0\n",
    "        for i in range(len(Y)):\n",
    "            for j in range(len(Y[0])):\n",
    "                cost+=(Y[i][j]-self.A2[i][j])**2\n",
    "        cost = cost/len(Y)\n",
    "        return cost\n",
    "        \n",
    "    def relu(self, z):\n",
    "        return z * (z > 0)\n",
    "    \n",
    "#     def softmax(self,z):\n",
    "#         exps = np.exp(z-np.max(z,axis= 1,keepdims = True))\n",
    "#         return exps/np.sum(exps,axis = 1, keepdims = True)\n",
    "    \n",
    "    \n",
    "\n",
    "#     def softmax(self,A):  \n",
    "#         expA = np.exp(A)\n",
    "#         return expA / expA.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    \n",
    "    def cro_entro(self,Y):\n",
    "        cost = - (Y * np.log(self.A2) + (1-Y) * np.log(1-self.A2)).mean()\n",
    "        return cost\n",
    "    \n",
    "    #changed for mult layer\n",
    "    #add lasso regularization \n",
    "    def compute_cost(self, Y):# cross entrophy\n",
    "        #cost = np.linalg.norm(self.A2 - Y)\n",
    "        cost = - (Y * np.log(self.A2) + (1-Y) * np.log(1-self.A2)).mean() \n",
    "#         print('shape cost is: ' ,cost.shape)\n",
    "\n",
    "#         <----lasso regularization ---->\n",
    "#         tmpcost = self.A2.T\n",
    "#         tmpcost = np.sum(tmpcost,axis = 1)/len(Y)\n",
    "#         <----End of lasso regularization implementation ----- >\n",
    "        return np.squeeze(cost)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        #dropout function for output in hidden layer \n",
    "        u1 = np.random.binomial(1,0.5,size = self.A1.shape)\n",
    "        self.A1 *= u1\n",
    "    \n",
    "        self.A2 = self.sigmoid(self.W2 @ self.A1 + self.b2)\n",
    "\n",
    "#         u2 = np.random.binomial(1,0.5,size = self.A2.shape)\n",
    "#         self.A2 *= u2\n",
    "#         self.A2 = self.soft_max_For_multi_attribute(self.A2,self.dic)\n",
    "\n",
    "        \n",
    "    \n",
    "    def backward_propagation(self, X, Y):\n",
    "        m = X.shape[1]\n",
    "#       <------dz2 for lasso regression ----- > \n",
    "#         dZ2 = (self.A2 - Y)/m  +self.lamda/m\n",
    "#       <------dz2 for lasso regression ----- > \n",
    "\n",
    "        dZ2 = (self.A2 - Y)/m\n",
    "        dW2 = dZ2 @ self.A1.T \n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True) \n",
    "        dZ1 = self.W2.T @ dZ2 * (self.A1 > 0)\n",
    "        dW1 = dZ1 @ X.T \n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True) \n",
    "\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        X, Y = X.T, Y.T\n",
    "        self.n_x = X.shape[0]\n",
    "        self.n_y = Y.shape[0]\n",
    "        print('n_x is',self.n_x ,'n_y is ',self.n_y )\n",
    "        self.initialize_parameters()\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(0, self.n_iterate):\n",
    "            self.forward_propagation(X)\n",
    "            self.backward_propagation(X, Y)\n",
    "            if i % 10 == 0:\n",
    "                cost = self.compute_cost(Y)\n",
    "#                  self.learning_rate = 5 * cost\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    def fit_continue(self,X,Y):\n",
    "        X, Y = X.T, Y.T\n",
    "        self.n_x = X.shape[0]\n",
    "        self.n_y = Y.shape[0]\n",
    "#         if self.W1 is None:\n",
    "#             self.initialize_parameters()\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(0, self.n_iterate):\n",
    "            self.forward_propagation(X)\n",
    "            self.backward_propagation(X, Y)\n",
    "            if i % 10 == 0:\n",
    "                cost = self.compute_cost(Y)\n",
    "#                 self.learning_rate = 5 * cost\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        X = X.T\n",
    "        A1 = self.relu(self.W1 @ X + self.b1)\n",
    "\n",
    "        \n",
    "        A2 = self.sigmoid(self.W2 @ A1 + self.b2)\n",
    "        tmp = np.dot(self.W2,self.A1)\n",
    "    \n",
    "        return A2.T\n",
    "    \n",
    "    def predict_one_layer(self,X):\n",
    "        X = X.T\n",
    "        A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        return A1.T\n",
    "    \n",
    "    def soft_max_For_multi_attribute(self,arr,dic):\n",
    "        #create array with all dict keys based on insertion order\n",
    "        tmp_arr = np.copy(arr)\n",
    "        \n",
    "        dicarr = list(dic.keys())\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_length = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_length    \n",
    "            #find the targeted array need to perform softmax\n",
    "            #create a dic to contain the location of those onehotencoding attribute\n",
    "            loc_dic = []\n",
    "            for i in range(cur_dict_length-1):\n",
    "                loc_dic.append(cur_loc+i)\n",
    "            #find targed array\n",
    "            target = tmp_arr[:,loc_dic]\n",
    "            #get softmaxed arr\n",
    "            arr_soft_max = self.api_softmax(target)\n",
    "            #reassig value back to orignal arr\n",
    "            tmp_arr[:,loc_dic] = arr_soft_max\n",
    "            #update cur_loc\n",
    "            cur_loc +=cur_dict_length\n",
    "            \n",
    "        return tmp_arr\n",
    "\n",
    "    def api_softmax(self,z):\n",
    "        exps = np.exp(z)\n",
    "        return exps/np.sum(exps,axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=64):\n",
    "        self.n_clusters = n_clusters  # number of clusters\n",
    "        self.centers = None  # to record the centers\n",
    "        self.labels = None\n",
    "        self.Y = None\n",
    "\n",
    "    def random_center(self,Y): #only for 1d dataset\n",
    "    # randomly generate n_cluster clusters in the raange of X\n",
    "        self.centers = np.random.rand(self.n_clusters, len(Y))\n",
    "        for i in range(self.n_clusters):\n",
    "            self.centers[i] = Y[i]\n",
    "            \n",
    "            \n",
    "    def random_center2(self,Y):\n",
    "    # randomly generate n_cluster clusters in the raange of X\n",
    "        self.centers = np.random.rand(self.n_clusters, len(Y[0]))\n",
    "        for i in range(self.n_clusters):\n",
    "            self.centers[i] = Y[i]\n",
    "#         for j in range(0):\n",
    "#             Y_j_min = self.Y[:,j].min()\n",
    "#             Y_j_max = self.Y[:,j].max()\n",
    "#             self.centers[:,j] = Y_j_min + (Y_j_max - Y_j_min) * self.centers[:,j]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def dist(self, point1, point2): #old one\n",
    "        return 2*(point1[0]-point2[0])**2 + 4*(point1[1]-point2[1])**2 + 3*(point1[2]-point2[2])**2\n",
    "\n",
    "    def dist2(self,point1,point2):\n",
    "        return LA.norm(point1-point2)**2\n",
    "    \n",
    "    def fit(self, Y):\n",
    "        self.Y = Y\n",
    "        self.labels = np.zeros(Y.shape[0], dtype='uint8')  # record the current labels of each sample of X\n",
    "        self.random_center(Y)\n",
    "        diff = 1\n",
    "        \n",
    "        while diff > 1e-3:\n",
    "            old_center = self.centers.copy()\n",
    "\n",
    "            # go through all samples and label them using the nearest label\n",
    "            for i in range(Y.shape[0]):\n",
    "                distance = np.zeros(self.n_clusters)\n",
    "                for j in range(self.n_clusters):\n",
    "                    distance[j] = self.dist2(Y[i], self.centers[j])\n",
    "                self.labels[i] = np.argmin(distance)\n",
    "                \n",
    "                \n",
    "            # update the centers\n",
    "            for i in range(self.n_clusters):\n",
    "                self.centers[i] = Y[self.labels==i].mean(axis=0)\n",
    "                \n",
    "\n",
    "            # update the difference\n",
    "            diff = np.linalg.norm(self.centers - old_center)\n",
    "            print(diff)\n",
    "        return self\n",
    "    \n",
    "    def fit2(self, Y):\n",
    "        self.Y = Y\n",
    "        self.labels = np.zeros(Y.shape[0], dtype='uint8')  # record the current labels of each sample of X\n",
    "        self.random_center2(Y)\n",
    "        diff = 1\n",
    "        \n",
    "        while diff > 1e-3:\n",
    "            old_center = self.centers.copy()\n",
    "\n",
    "            # go through all samples and label them using the nearest label\n",
    "            for i in range(Y.shape[0]):\n",
    "                distance = np.zeros(self.n_clusters)\n",
    "                for j in range(self.n_clusters):\n",
    "                    distance[j] = self.dist2(Y[i], self.centers[j])\n",
    "                self.labels[i] = np.argmin(distance)\n",
    "                \n",
    "                \n",
    "            # update the centers\n",
    "            for i in range(self.n_clusters):\n",
    "                self.centers[i] = Y[self.labels==i].mean(axis=0)\n",
    "                \n",
    "\n",
    "            # update the difference\n",
    "            diff = np.linalg.norm(self.centers - old_center)\n",
    "            print(diff)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, Y):\n",
    "        out = np.zeros(Y.shape)\n",
    "        for i in range(self.n_clusters):\n",
    "            out[self.labels==i] = self.centers[i]\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oneHOTENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncoding(Y, kmeans):\n",
    "    out = np.zeros((len(Y), kmeans.n_clusters))\n",
    "    for i in range(len(Y)):\n",
    "        out[i, Y[i]] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test clustering numbered data:\n",
    "    step1 : get single column\n",
    "    step2 : sort in increasing order\n",
    "    step3 : based on distribution, cut data into 4 section with equally number of candidate\n",
    "    step4 : if data is null, set isnon to be true for that column\n",
    "    step5 : append new datacol to input_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isint(value):\n",
    "  try:\n",
    "    int(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "def isfloat(value):\n",
    "  try:\n",
    "    float(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "def getdistriarr(df,arr_str):\n",
    "    arr = df[arr_str].values\n",
    "    di = []\n",
    "    di_og = []  #RETURN OG SINCE di will be sort\n",
    "    for i in range(len(arr)):\n",
    "        if isint(arr[i]) == True:\n",
    "            di.append(int(arr[i]))\n",
    "            di_og.append(int(arr[i]))\n",
    "        else:\n",
    "            di.append(-1)\n",
    "            di_og.append(-1)\n",
    "    di.sort()\n",
    "    ans_arr = []\n",
    "    leng = len(di)\n",
    "    ans_arr.append(di[int(leng/3)])\n",
    "#     ans_arr.append(di[int(leng/2)])\n",
    "    ans_arr.append(di[int(leng*0.66)])\n",
    "    ans_arr.append(di[int(leng-1)])\n",
    "    return di_og,ans_arr\n",
    "\n",
    "def oneHotEncoding_return_arr(att_arr,class_arr): #one \n",
    "    #add one more column for each row since last col used as flag\n",
    "    choice = len(class_arr)\n",
    "    out = np.zeros((len(att_arr),len(class_arr)+1))\n",
    "    for i in range(len(att_arr)):\n",
    "        loc = 0\n",
    "        #first jude if data is null\n",
    "        if att_arr[i] == -1:\n",
    "            out[i][-1] = 0\n",
    "            continue\n",
    "        else:\n",
    "            out[i][-1] = 1\n",
    "            for j in range(len(class_arr)):\n",
    "                if att_arr[i] == class_arr[j]:\n",
    "                    out[i][j] = 1\n",
    "                    break\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "def GET_ONE_HOT_ARRAY_ONLY_DIGIT_COLUMN(df,attr_name,og_dic):\n",
    "    att_arr,class_arr = getdistriarr(df,attr_name)\n",
    "    dic_ans = create_dic_for_classification(class_arr)\n",
    "    append_arr_of_dic_to_overall(og_dic,dic_ans,attr_name)\n",
    "\n",
    "    return oneHotEncoding_return_arr(att_arr,class_arr)\n",
    "\n",
    "def create_dic_for_classification(class_arr):\n",
    "    cur_dic = {}\n",
    "    total_len = len(class_arr)\n",
    "    for i in range(len(class_arr)):\n",
    "        cur_dic[class_arr[i]] = i\n",
    "    return cur_dic\n",
    "\n",
    "def append_arr_of_dic_to_overall(og,dic,att_name):\n",
    "    og[att_name] = dic\n",
    "    \n",
    "    \n",
    "def append_arr(old,new):\n",
    "    #both old and new has the same num of row\n",
    "    #create a new nparray\n",
    "    \n",
    "    newdim = len(old[0])+len(new[0])\n",
    "    ans = np.zeros((len(old),newdim))\n",
    "    print(ans.shape)\n",
    "    for i in range(len(old)):\n",
    "        ans[i] = np.append(old[i],new[i])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP TWO CONVERT NLP DATASET INTO VECTOR:\n",
    "    STEP1: vectorize dataset\n",
    "    STEP2: apply kmeans clustering to language data\n",
    "    STEP3: ONEHOT ENCODING\n",
    "    STEP4: APPEND CLASSIFICATION DATASET INTO old CLASSFICAITION ARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLPDATAPRO(allsentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(allsentences)\n",
    "    ans = X.toarray()\n",
    "    return ans\n",
    "\n",
    "def createtextarr_ONEHOT_ENCODING(attri_col): #ONEHOT_ENCODING\n",
    "    df2 = df[attri_col]\n",
    "    arr = df2.values\n",
    "    for i in range(len(arr)):\n",
    "        di.append(arr[i])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP THREE CONVERT TEXT_DATASET WITH FIXED classification into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processnull(arr):\n",
    "    arrnew = []\n",
    "    for i in range(len(arr)):\n",
    "        if type(arr[i]) == float and math.isnan(arr[i]) :\n",
    "            arrnew.append('Nan')\n",
    "        else:\n",
    "            arrnew.append(arr[i])\n",
    "    return arrnew\n",
    "\n",
    "def returnuniqiearr(og):\n",
    "    ar = np.asarray(og)\n",
    "    return np.unique(ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: create a function to append text to old vecto(remember to append flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: TEST NN TONIGHT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onehoted_text_fixed_classification(attribu_name,df,km_num):\n",
    "    df_cur = df[attribu_name]\n",
    "    attribute_arr = df_cur.values\n",
    "    \n",
    "    arr_no_null = processnull(attribute_arr) #create new arr with format fit by NLP(CountVectorizer)\n",
    "   \n",
    "    NLP_ARR = NLPDATAPRO(arr_no_null)\n",
    "    #BEFORE ONEHOT_ENCODING\n",
    "    #APPLY KMEANS CLUSTERING SINCE TOO MANY DIFFERENT CLASSIFICATION\n",
    "    KM_TMP = KMeans(km_num)\n",
    "    KM_TMP.fit2(NLP_ARR)\n",
    "    labels_arr = KM_TMP.labels\n",
    "    \n",
    "    return labels_arr\n",
    "\n",
    "\n",
    "\n",
    "def onehot_enco(labelarr,df,att_name):\n",
    "    og_text_arr = df[att_name].values\n",
    "    #create dic to remeber loc for each value in the uniqe arr\n",
    "    uni_arr = np.unique(labelarr)\n",
    "    loc_dic = {}\n",
    "    for i in range(len(uni_arr)):\n",
    "        loc_dic[uni_arr[i]] = i\n",
    "    #based on the lenth of dic,create onehot_encod\n",
    "    choice = len(loc_dic)\n",
    "    out = np.zeros((len(labelarr),choice+1))\n",
    "    for i in range(len(labelarr)):\n",
    "        #first junde if data is null\n",
    "        if type(og_text_arr[i])!=str and math.isnan(og_text_arr[i]) == True:\n",
    "            out[i][-1] = 0\n",
    "            continue\n",
    "        else:\n",
    "            out[i][-1] = 1\n",
    "            cur_loc = loc_dic.get(labelarr[i])\n",
    "            out[i][cur_loc] = 1\n",
    "            \n",
    "    return out,loc_dic\n",
    "\n",
    "def GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,attr_name,og_dic,km_num):\n",
    "    labels_arr = create_onehoted_text_fixed_classification(attr_name,df,km_num)\n",
    "    out_arr,loc_dic = onehot_enco(labels_arr,df,attr_name)\n",
    "    #append dic to global array of dic\n",
    "    #append reverse dic to overall\n",
    "#     dicnew = reversedict(loc_dic)\n",
    "    append_arr_of_dic_to_overall(og_dic,loc_dic,attr_name)\n",
    "    p = list(np.unique(labels_arr))\n",
    "    print('class_arr of nlp', p)\n",
    "    return out_arr\n",
    "#     return out_arr   \n",
    "\n",
    "\n",
    "def Bonus_GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,attr_name,og_dic,km_num):\n",
    "    labels_arr = create_onehoted_text_fixed_classification(attr_name,df,km_num)\n",
    "    out_arr,loc_dic = onehot_enco(labels_arr,df,attr_name)\n",
    "    #append dic to global array of dic\n",
    "    #append reverse dic to overall\n",
    "#     dicnew = reversedict(loc_dic)\n",
    "    append_arr_of_dic_to_overall(og_dic,loc_dic,attr_name)\n",
    "    p = list(np.unique(labels_arr))\n",
    "    print('class_arr of nlp', p)\n",
    "    return out_arr,labels_arr\n",
    "\n",
    "# def reversedict(dic):\n",
    "#     dicnew = {}\n",
    "#     keyarr = list(dic.keys())\n",
    "#     valarr = list(dic.values())\n",
    "#     for i in range(len(keyarr)):\n",
    "#         dicnew[valarr[i]] = keyarr[i]\n",
    "#     return dicnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL WORK:\n",
    "    1. MAINTAIN AN ARRAY OF DICTIONARY S.T EVERY DIC\n",
    "       CONTAINS MAPPING FROM CLASSIFICAITON TO ACTUAL INDEX AFTER KMEANS\n",
    "    \n",
    "    2. AFTER TRAINING, WE WILL UTILIZED THE ARR_DIC TO RECOVER CANDIDATE ANSER\n",
    "    \n",
    "    3. TODO: REWRITE COST FUNCTION OF NERUAL NETWORK\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINDING:\n",
    "    AFTER combine several attributes column, num of candidate with dinct\n",
    "    ans grows extremly fast.\n",
    "    Need to apply Kmeans clustering again to the array of candidate before\n",
    "    threw into Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE A SCRIPT TO APPEND ASSIGNED ATTRIBUTE TO INPUT COLUMN\n",
    "\n",
    "(API)INSTRUCTION:<br>\n",
    "USER WHO WANTS TO CREATE PREPROCESSED DATASET ONLY NEED TO CREATE\n",
    "ARRAY[i]<br>\n",
    "       ARRAY[i][0] = 'name of attribute'<br>\n",
    "       ARRAY[i][1] = 1 : it is a digit column<br>\n",
    "       ARRAY[i][1] = 0 : it is a NLP column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_auto_append_attri(arr_of_attribute,df,og_dic):\n",
    "    for i in range(len(arr_of_attribute)):\n",
    "        name = arr_of_attribute[i][0]\n",
    "        isdigit = arr_of_attribute[i][1]\n",
    "        if isdigit == 1:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_DIGIT_COLUMN(df,name,og_dic)\n",
    "        else:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,name,og_dic,80)\n",
    "        #append old with new\n",
    "        if i == 0:\n",
    "            old = attr_arr\n",
    "        else:\n",
    "            old = append_arr(old,attr_arr)\n",
    "#             print(old.shape)\n",
    "    return old\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_semi_auto_append_attri(arr_of_attribute,df,og_dic):\n",
    "    for i in range(len(arr_of_attribute)):\n",
    "        name = arr_of_attribute[i][0]\n",
    "        print('cur key is', name)\n",
    "        isdigit = arr_of_attribute[i][1]\n",
    "        if isdigit == 1:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_DIGIT_COLUMN(df,name,og_dic)\n",
    "        elif isdigit == 0:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,name,og_dic,30)\n",
    "        elif isdigit == 2:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_FIXED_CHOICE_COLUMN(df,name,og_dic)\n",
    "        #append old with new\n",
    "        if i == 0:\n",
    "            old = attr_arr\n",
    "        else:\n",
    "            old = append_arr(old,attr_arr)\n",
    "#             print(old.shape)\n",
    "    return old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_choice_get_distinct_arr(df,arr_str):\n",
    "    arr = df[arr_str].values\n",
    "    uni_arr = np.unique(arr)\n",
    "    di = []\n",
    "#     di_og = [] #RETURN OG SINCE di will be sorted\n",
    "    div_arr = []\n",
    "    for i in range(len(uni_arr)):\n",
    "        if math.isnan(uni_arr[i])!=True:\n",
    "            div_arr.append(uni_arr[i])\n",
    "    for i in range(len(arr)):\n",
    "        if math.isnan(arr[i])!= True:\n",
    "            di.append(int(arr[i]))\n",
    "        else:\n",
    "            di.append(-1)\n",
    "    return di,div_arr\n",
    "\n",
    "def GET_ONE_HOT_ARRAY_ONLY_FIXED_CHOICE_COLUMN(df,attr_name,og_dic):\n",
    "    att_arr,class_arr = fix_choice_get_distinct_arr(df,attr_name)\n",
    "    print('class_arr',class_arr)\n",
    "    dic_ans = create_dic_for_classification(class_arr)\n",
    "    append_arr_of_dic_to_overall(og_dic,dic_ans,attr_name)\n",
    "    return oneHotEncoding_return_arr(att_arr,class_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Kmeans Clustering to the overal column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_whole_attribute_get_Y_for_Neural_NetWork(X,num_cluster):\n",
    "    km = KMeans(num_cluster)\n",
    "    km.fit2(X)\n",
    "    \n",
    "    #APPLY ONEHOT_ENCODING TO NEURAL_NETWORKAGAIN\n",
    "    \n",
    "    return oneHotEncoding(km.labels,km),km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLY NEURAL NETWORK TO SOLVE THIS QUESTION:\n",
    "\n",
    "REMEBER TO CONSIDER FLAG FOR EACH COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predicted_data(pred,kmeans):\n",
    "    ans = []\n",
    "    for i in range(len(pred)):\n",
    "        ans.append(kmeans.centers[np.argmax(pred[i])])\n",
    "    newarr = np.asarray(ans)\n",
    "    return newarr\n",
    "\n",
    "def find_classification_based_on_distance(pred,dic):\n",
    "    #create array with all dict keys based on insertion order\n",
    "    dicarr = list(dic.keys())\n",
    "    #isolate target awated_array used to classification\n",
    "    for i in range(len(pred)):\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "#             print('cur dic: ',cur_dict_key)\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_lengh = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_lengh  #update current cutting position\n",
    "            #restore dimension of onehotencoding for current dictionary\n",
    "            one_hot_cur_dic = np.zeros((cur_dict_lengh,cur_dict_lengh))\n",
    "#             print('before any processing cur loc is ', cur_loc, ' cur des is : ', cur_des)\n",
    "            for q in range(0,cur_dict_lengh-1):\n",
    "                one_hot_cur_dic[q][q] = 1\n",
    "                one_hot_cur_dic[q][-1] = 1\n",
    "            #isolate target awaited_classification array\n",
    "            cur_isolated_arr = pred[i][cur_loc:cur_des]\n",
    "            #compute classification with smallest distance\n",
    "            arr_store_distance= []\n",
    "            for h in range (len(one_hot_cur_dic)):\n",
    "                arr_store_distance.append(np.linalg.norm(cur_isolated_arr-one_hot_cur_dic[h]))\n",
    "            #transfer dic to arr s.t unilized argmim\n",
    "            dis_np_array = np.asarray(arr_store_distance)\n",
    "            index = np.argmin(dis_np_array)\n",
    "#             if cur_dict_key == 'big5_10':\n",
    "#                 print(index)\n",
    "#                 print('cur onehot_array is ', one_hot_cur_dic[3] )\n",
    "#                 print('cur loc is ', cur_loc, ' cur des is : ', cur_des)\n",
    "            # change value in output array\n",
    "            for k in range (cur_dict_lengh):\n",
    "                pred[i][cur_loc+k] = one_hot_cur_dic[index][k]\n",
    "                \n",
    "            cur_loc += cur_dict_lengh #update cur_loc not used in curretn loop\n",
    "\n",
    "def find_classification_based_on_prob(pred,dic):\n",
    "    dicarr = list(dic.keys())\n",
    "    for i in range(len(pred)):\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_lengh = len(cur_dict_arr)  + 1\n",
    "            cur_des += cur_dict_lengh\n",
    "            \n",
    "            cur_isolated_arr = pred[i][cur_loc:cur_des-1]\n",
    "            #find index of max value\n",
    "            index = np.argmax(cur_isolated_arr)\n",
    "            \n",
    "            for k in range(cur_dict_lengh):\n",
    "                pred[i][cur_loc+k] = 0\n",
    "            \n",
    "            pred[i][cur_loc + index] = 1\n",
    "            cur_loc +=cur_dict_lengh\n",
    "    \n",
    "            \n",
    "def create_reverse_dictionary(dic):\n",
    "    reverse = {}\n",
    "    dic_arr = list(dic.keys())\n",
    "    for i in range(len(dic_arr)):\n",
    "        cur_reverse_dict = {}\n",
    "        cur_dic_key = dic_arr[i]\n",
    "        cur_key_arr = list(dic.get(cur_dic_key).keys())\n",
    "        cur_index_arr = list(dic.get(cur_dic_key).values())\n",
    "        #reverse key_value pair in originaly dictionary\n",
    "        for j in range(len(cur_key_arr)):\n",
    "            cur_reverse_dict[cur_index_arr[j]] = cur_key_arr[j]\n",
    "        #append current dic to ans dic\n",
    "        reverse[cur_dic_key] = cur_reverse_dict\n",
    "    return reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create  function to knock out some attribute inorder to create more dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingset_generator(train,dic,arr_str):\n",
    "    #first find the location \n",
    "    #knock out\n",
    "    #return dataset\n",
    "    dicarr = list(dic.keys())\n",
    "    changed_arr = np.copy(train)\n",
    "    y_arr = np.copy(train)\n",
    "    #outermost loop : loop through those keys that need to be cleared\n",
    "    for d in range(len(arr_str)):\n",
    "        cur_key = arr_str[d]\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_length = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_length\n",
    "            #test if we find what we want\n",
    "            if cur_dict_key == cur_key:\n",
    "                for i in range(len(train)):\n",
    "                    for k in range(cur_dict_length):\n",
    "                        changed_arr[i][cur_loc + k] = 0\n",
    "                        \n",
    "                break\n",
    "            cur_loc +=cur_dict_length\n",
    "    return changed_arr,y_arr\n",
    "\n",
    "def append_row(old,new):\n",
    "    ans = np.vstack([old,new])\n",
    "    return ans\n",
    "\n",
    "\n",
    "def enlarge_dataset(train,y,dic,arr_of_arr_str):\n",
    "    list_of_new_x = []\n",
    "    list_of_new_y = []\n",
    "    new_x = np.copy(train)\n",
    "    new_y = np.copy(y)\n",
    "    for i in range(len(arr_of_arr_str)):\n",
    "        cur_str = arr_of_arr_str[i]\n",
    "        changed_arr,y_arr = trainingset_generator(train,dic,cur_str)\n",
    "        #append\n",
    "        new_x = append_row(new_x,changed_arr)\n",
    "        new_y = append_row(new_y,y_arr)\n",
    "        list_of_new_x.append(changed_arr)\n",
    "        list_of_new_y.append(y_arr)\n",
    "#     return new_x,new_y\n",
    "    return list_of_new_x,list_of_new_y\n",
    "\n",
    "def createlist_list_string(dic):\n",
    "    ans = []\n",
    "    key_arr = list(dic.keys())\n",
    "    for i in range(len(key_arr)):\n",
    "        tmp = []\n",
    "        tmp.append(key_arr[i])\n",
    "        ans.append(tmp)\n",
    "        \n",
    "    #degree 2\n",
    "#     for i in range(len(key_arr)):\n",
    "#         for j in range(i,len(key_arr)):\n",
    "#             tmp = []\n",
    "#             tmp.append(key_arr[i])\n",
    "#             tmp.append(key_arr[j])\n",
    "#             ans.append(tmp)\n",
    "    return ans\n",
    "\n",
    "def train_list_of_dataset(nn,x,y):\n",
    "    nn.fit(y,y)\n",
    "    for i in range(0,len(x)):\n",
    "        print('current iter', i)\n",
    "        nn.fit_continue(x[i],y)\n",
    "        #after training compute num of correctly predicted choice for candidate\n",
    "        ans = nn.predict(x[i])\n",
    "        find_classification_based_on_prob(ans,nn.dic)\n",
    "        count_num_of_accurately_predicted_avg(ans,X_train_new,nn.dic)\n",
    "    \n",
    "    return nn\n",
    "    \n",
    "def count_num_of_accurately_predicted_avg(x,y,dic):\n",
    "    dicarr = list(dic.keys())\n",
    "    \n",
    "    right_predicted_count = 0\n",
    "    for i in range(len(x)):\n",
    "        #iterate thourhgt each choice\n",
    "        tmp_right_count = 0\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dic_length = len(cur_dict_arr) + 1\n",
    "            cur_des += cur_dic_length\n",
    "            issame = True\n",
    "            #compare x and y in range cur_loc - > cur_des\n",
    "            for pointer in range(cur_loc,cur_des-1):\n",
    "                if x[i][pointer] != y[i][pointer]:\n",
    "                    issame = False\n",
    "                    break\n",
    "            \n",
    "            #now check whether x[i] and y[i] are same\n",
    "            if issame == True:\n",
    "                tmp_right_count+=1\n",
    "            cur_loc += cur_dic_length\n",
    "        #finished iterate throught current candidate\n",
    "        right_predicted_count+=tmp_right_count\n",
    "    \n",
    "    print('fianl avg right predicted count is ', right_predicted_count/len(x))\n",
    "    \n",
    "    \n",
    "    #function used to find how well the model interpolate\n",
    "#need a array contains all attribute name\n",
    "#already has the function to mask every attribute\n",
    "#what is output? key value pair, key: attribute name, value: avg accuracy\n",
    "def evaluate(X_train_arr,Y_train_arr,og_pred,att_name_arr,hidden_neuron,iter_num):\n",
    "    evaluate_dic = {}\n",
    "    #to do for every attribute array utlize cross validation\n",
    "    #temporary: we split first 2500 as train\n",
    "    for i in range(len(X_train_arr)):\n",
    "        x_cur_all = X_train_arr[i]\n",
    "        y_cur_all = Y_train_arr[i]\n",
    "        cur_mask_key = att_name_arr[i][0]\n",
    "        #split dataset into training\n",
    "        x_cur_train = x_cur_all[:2400]\n",
    "        y_cur_train = y_cur_all[:2400]\n",
    "        x_cur_pred = x_cur_all[2400:]\n",
    "        y_cur_pred = y_cur_all[2400:]\n",
    "        #create nn object\n",
    "        mm = NeuralNetwork(hidden_neuron,iter_num,0.1,og_pred)\n",
    "        mm.fit(x_cur_train,y_cur_train)\n",
    "        #predict the answer\n",
    "        predict_ans = mm.predict(x_cur_pred)\n",
    "        tmp_eval_ans = count_right_predicted_num(predict_ans,y_cur_pred,og_pred,cur_mask_key)\n",
    "        evaluate_dic[str(cur_mask_key)] = tmp_eval_ans\n",
    "        print('cur masked name is ', cur_mask_key, ' eval value is: ', tmp_eval_ans)\n",
    "        \n",
    "def count_right_predicted_num(ans,y_pred,dic,att_name):\n",
    "    #first find the location of masked attribute name\n",
    "    #loop through dictionary list\n",
    "    anscount = 0\n",
    "    totallen = len(ans)\n",
    "    dicarr = list(dic.keys())\n",
    "    cur_loc = 0\n",
    "    cur_des = 0\n",
    "\n",
    "    for j in range(len(dicarr)):\n",
    "        cur_dict_key = dicarr[j]\n",
    "#         print('curloop key', cur_dict_key)\n",
    "        cur_dict_arr = dic.get(cur_dict_key)\n",
    "        cur_dict_length = len(cur_dict_arr) + 1\n",
    "        cur_des += cur_dict_length\n",
    "        #find if cur key == att_name\n",
    "        if cur_dict_key == att_name:\n",
    "            break\n",
    "        else:\n",
    "            cur_loc +=cur_dict_length\n",
    "#     print('!!!!curloc ', cur_loc, ' des ', cur_des)\n",
    "    #now we have location of masked attribute column, we just need to examin the answer\n",
    "    #first --softmax\n",
    "    find_classification_based_on_prob(ans,og_pred)\n",
    "    #second -> exame\n",
    "    for i in range(totallen):\n",
    "        issame = True\n",
    "        xcur = ans[i][cur_loc:cur_des-1]\n",
    "        ycur = y_pred[i][cur_loc:cur_des-1]\n",
    "        if np.array_equal(xcur,ycur):\n",
    "            anscount+=1\n",
    "    print('current loc and des is', cur_loc, 'and ', cur_des, ' total cound is',  anscount,'percent is', anscount/totallen)\n",
    "    return anscount/totallen\n",
    "    \n",
    "    \n",
    "    \n",
    "def create_dictionary_for_every_location_of_feature(dic):\n",
    "    dic_look_up = {}\n",
    "    dicarr = list(dic.keys())\n",
    "    cur_loc = 0\n",
    "    cur_des = 0 \n",
    "    for i in range(len(dicarr)):\n",
    "        locarr = []\n",
    "        cur_dict_key = dicarr[i]\n",
    "        cur_dict_arr = dic.get(cur_dict_key)\n",
    "        cur_dict_length = len(cur_dict_arr)+1\n",
    "        cur_des +=cur_dict_length\n",
    "        #append loccation\n",
    "        locarr.append(cur_loc)\n",
    "        locarr.append(cur_des)\n",
    "        dic_look_up[cur_dict_key] = locarr\n",
    "        cur_loc  += cur_dict_length\n",
    "    return dic_look_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder - Double Hidden Layer Neural Network  Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MULTI_NeuralNetwork:\n",
    "    def __init__(self, n_h,n_h2, n_iterate=10, learning_rate=1, dic = []):\n",
    "        self.n_x = None  # size of the input layer\n",
    "        self.n_h = n_h  # size of the hidden layer\n",
    "        self.n_h2= n_h2\n",
    "        self.n_y = None # size of the output layer\n",
    "        self.W1 = None\n",
    "        self.W2 = None\n",
    "        self.W3 = None\n",
    "        self.b1 = None\n",
    "        self.b2 = None\n",
    "        self.b3 = None\n",
    "        self.A1 = None\n",
    "        self.A2 = None  # sigmoid output of the second activation\n",
    "        self.A3 = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterate = n_iterate\n",
    "#         self.dic = dic\n",
    "        self.dic = dic\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        self.W1 = np.random.randn(self.n_h, self.n_x) * 0.01\n",
    "        self.b1 = np.zeros((self.n_h, 1))\n",
    "        \n",
    "        self.W2 = np.random.randn(self.n_h2, self.n_h) * 0.01\n",
    "        self.b2 = np.zeros((self.n_h2, 1))\n",
    "        \n",
    "        self.W3 = np.random.randn(self.n_y, self.n_h2) * 0.01\n",
    "        self.b3 = np.zeros((self.n_y, 1))\n",
    "    \n",
    "    \n",
    "    def MSE(self,Y):\n",
    "        cost = 0\n",
    "        for i in range(len(Y)):\n",
    "            for j in range(len(Y[0])):\n",
    "                cost+=(Y[i][j]-self.A2[i][j])**2\n",
    "        cost = cost/len(Y)\n",
    "        return cost\n",
    "        \n",
    "    def relu(self, z):\n",
    "        return z * (z > 0)\n",
    "    \n",
    "#     def softmax(self,z):\n",
    "#         exps = np.exp(z-np.max(z,axis= 1,keepdims = True))\n",
    "#         return exps/np.sum(exps,axis = 1, keepdims = True)\n",
    "    \n",
    "    \n",
    "\n",
    "#     def softmax(self,A):  \n",
    "#         expA = np.exp(A)\n",
    "#         return expA / expA.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derv(self,x):\n",
    "        return self.sigmoid(x)*(1-self.sigmoid(x))\n",
    "\n",
    "    \n",
    "    \n",
    "    def cro_entro(self,Y):\n",
    "        cost = - (Y * np.log(self.A2) + (1-Y) * np.log(1-self.A2)).mean()\n",
    "        return cost\n",
    "    \n",
    "    def compute_cost(self, Y):# cross entrophy\n",
    "        #cost = np.linalg.norm(self.A2 - Y)\n",
    "        cost = - (Y * np.log(self.A3) + (1-Y) * np.log(1-self.A3)).mean()\n",
    "#         print('shape cost is: ' ,cost.shape)\n",
    "        return np.squeeze(cost) \n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        #dropout function for output in hidden layer\n",
    "        u1 = np.random.binomial(1,0.3,size = self.A1.shape)\n",
    "        self.A1 *= u1\n",
    "\n",
    "        self.A2 = self.relu(self.W2 @ self.A1 + self.b2)\n",
    "        #dropout function for hidden layer two\n",
    "        u2 = np.random.binomial(1,0.3,size = self.A2.shape)\n",
    "        self.A2 *= u2\n",
    "        \n",
    "        self.A3 = self.sigmoid(self.W3 @self.A2 + self.b3)\n",
    "        \n",
    "\n",
    "        \n",
    "#         u2 = np.random.binomial(1,0.5,size = self.A2.shape)\n",
    "#         self.A2 *= u2\n",
    "#         self.A2 = self.soft_max_For_multi_attribute(self.A2,self.dic)\n",
    "\n",
    "        \n",
    "    \n",
    "    def backward_propagation(self, X, Y):\n",
    "        m = X.shape[1]  #num of input candidate\n",
    "\n",
    "#         dZ2 = self.A2 - Y\n",
    "# #         print('cur shape of dz2 :',dZ2.shape)\n",
    "# #         print('cur cost of cros entro', c)\n",
    "#         dW2 = dZ2 @ self.A1.T / m\n",
    "#         db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "#         dZ1 = self.W2.T @ dZ2 * (self.A1 > 0)\n",
    "#         dW1 = dZ1 @ X.T / m\n",
    "#         db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "#         self.W1 -= self.learning_rate * dW1\n",
    "#         self.b1 -= self.learning_rate * db1\n",
    "#         self.W2 -= self.learning_rate * dW2\n",
    "#         self.b2 -= self.learning_rate * db2\n",
    "\n",
    "# <------------------------------------------------------>\n",
    "#           used for two sigmoid hidden layer\n",
    "#         dZ3 = (self.A3 - Y)/m\n",
    "#         dW3 = np.dot(dZ3 , self.A2.T) \n",
    "#         db3 = np.sum(dZ3,axis = 1, keepdims = True)\n",
    "#         dZ2 = np.dot(self.W3.T,  dZ3) * self.sigmoid_derv(self.A2)\n",
    "#         dW2 = dZ2 @self.A1.T\n",
    "#         db2 = np.sum(dZ2,axis = 1, keepdims = True)\n",
    "#         dZ1 = np.dot(self.W2.T , dZ2) * self.sigmoid_derv(self.A1)\n",
    "#         dW1 = dZ1@ X.T\n",
    "#         db1 = np.sum(dZ1,axis = 1, keepdims = True)\n",
    "# <------------------------------------------------------->\n",
    "\n",
    "\n",
    "        dZ3 = (self.A3 - Y)/m\n",
    "        dW3 = np.dot(dZ3 , self.A2.T) \n",
    "        db3 = np.sum(dZ3,axis = 1, keepdims = True)\n",
    "        dZ2 = np.dot(self.W3.T,  dZ3) * (self.A2 > 0)\n",
    "        dW2 = dZ2 @self.A1.T\n",
    "        db2 = np.sum(dZ2,axis = 1, keepdims = True)\n",
    "        dZ1 = np.dot(self.W2.T , dZ2) * (self.A1 > 0)\n",
    "        dW1 = dZ1@ X.T\n",
    "        db1 = np.sum(dZ1,axis = 1, keepdims = True)\n",
    "        \n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.W3 -= self.learning_rate * dW3\n",
    "        self.b3 -= self.learning_rate * db3\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        X, Y = X.T, Y.T\n",
    "    \n",
    "        self.n_x = X.shape[0]\n",
    "        self.n_y = Y.shape[0]\n",
    "        self.initialize_parameters()\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(0, self.n_iterate):\n",
    "            self.forward_propagation(X)\n",
    "            self.backward_propagation(X, Y)\n",
    "            if i % 10 == 0:\n",
    "                cost = self.compute_cost(Y)\n",
    "#                  self.learning_rate = 5 * cost\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    def fit_continue(self,X,Y):\n",
    "        X, Y = X.T, Y.T\n",
    "        self.n_x = X.shape[0]\n",
    "        self.n_y = Y.shape[0]\n",
    "#         if self.W1 is None:\n",
    "#             self.initialize_parameters()\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(0, self.n_iterate):\n",
    "            self.forward_propagation(X)\n",
    "            self.backward_propagation(X, Y)\n",
    "            if i % 10 == 0:\n",
    "                cost = self.compute_cost(Y)\n",
    "#                 self.learning_rate = 5 * cost\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        X = X.T\n",
    "        A1 = self.sigmoid(self.W1 @ X + self.b1)\n",
    "        A2 = self.sigmoid(self.W2 @ A1 + self.b2)\n",
    "        A3 = self.sigmoid(self.W3 @ A2 + self.b3)\n",
    "        \n",
    "        return A3.T\n",
    "    \n",
    "    def predict_one_layer(self,X):\n",
    "        X = X.T\n",
    "        A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        return A1.T\n",
    "    \n",
    "    def soft_max_For_multi_attribute(self,arr,dic):\n",
    "        #create array with all dict keys based on insertion order\n",
    "        tmp_arr = np.copy(arr)\n",
    "        dicarr = list(dic.keys())\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_length = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_length    \n",
    "            #find the targeted array need to perform softmax\n",
    "            #create a dic to contain the location of those onehotencoding attribute\n",
    "            loc_dic = []\n",
    "            for i in range(cur_dict_length-1):\n",
    "                loc_dic.append(cur_loc+i)\n",
    "            #find targed array\n",
    "            target = tmp_arr[:,loc_dic]\n",
    "            #get softmaxed arr\n",
    "            arr_soft_max = self.api_softmax(target)\n",
    "            #reassig value back to orignal arr\n",
    "            tmp_arr[:,loc_dic] = arr_soft_max\n",
    "            #update cur_loc\n",
    "            cur_loc +=cur_dict_length\n",
    "            \n",
    "        return tmp_arr\n",
    "\n",
    "    def api_softmax(self,z):\n",
    "        exps = np.exp(z)\n",
    "        return exps/np.sum(exps,axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write function to complete quesiton: Analyze the Data\n",
    "#step1 : use the answer from evalue function - > those feature that has high dependencies\n",
    "#first find those masked vector based on the name of features with high depen\n",
    "#step2 : try also mask every other feature to which additional maskede feature lead to smallest accurayc\n",
    "#of interpolating\n",
    "def Analyzethedata(x,y,dic,arr_of_feature_high_mutual_dependencies,masked_arr,dic_location,hidden_neuron_num,iter_num):\n",
    "    finalans = {}\n",
    "    for i in range(len(arr_of_feature_high_mutual_dependencies)):\n",
    "        for j in range(len(masked_arr)):\n",
    "            if arr_of_feature_high_mutual_dependencies[i] == masked_arr[j][0]:\n",
    "                cur_key_ans_arr = []\n",
    "                #now current index j could be used to find the masked array\n",
    "                x_feature_masked_high_muldependencies = x[j]\n",
    "                y_feature_masked_high_muldependencies = y[j]\n",
    "                cur_masked_key = arr_of_feature_high_mutual_dependencies[i]\n",
    "                #step 2 loop throught every other key\n",
    "                #first aplit dataset into training and preding\n",
    "                for k in range(len(masked_arr)):\n",
    "                    if masked_arr[k][0]!= cur_masked_key:\n",
    "                        print('current masked key is', cur_masked_key, ' and other key is', masked_arr[k][0])\n",
    "                        #masked other feature  -> first find the location of other feaure in one hot encoded array\n",
    "                        x_cur_tmp_feature_masked_high_muldenpendencies = np.copy(x_feature_masked_high_muldependencies)\n",
    "                        y_cur_tmp_feature_masked_high_muldenpendencies = np.copy(y_feature_masked_high_muldependencies)\n",
    "                        cur_key = masked_arr[k][0]\n",
    "                        #find location\n",
    "                        cur_loc = dic_location.get(cur_key)[0]\n",
    "                        cur_des = dic_location.get(cur_key)[1]\n",
    "                        print('cur masked loc', cur_loc, 'cur des ', cur_des)\n",
    "                        #mask\n",
    "                        for pointer in range(len(x_cur_tmp_feature_masked_high_muldenpendencies)):\n",
    "                            for pointer2 in range(cur_loc,cur_des):\n",
    "                                x_cur_tmp_feature_masked_high_muldenpendencies[pointer][pointer2] = 0\n",
    "                        #mask process complete\n",
    "                        \n",
    "                        #split dataset into training and predicting\n",
    "                        x_cur_train = x_cur_tmp_feature_masked_high_muldenpendencies[:2000]\n",
    "                        y_cur_train = y_cur_tmp_feature_masked_high_muldenpendencies[:2000]\n",
    "                        x_cur_pred = x_cur_tmp_feature_masked_high_muldenpendencies[2000:]\n",
    "                        y_cur_pred = y_cur_tmp_feature_masked_high_muldenpendencies[2000:]\n",
    "                        nn = NeuralNetwork(hidden_neuron_num,iter_num,0.1,dic)\n",
    "                        nn.fit(x_cur_train,y_cur_train)\n",
    "                        #predict the asnwer\n",
    "                        predict_ans = nn.predict(x_cur_pred)\n",
    "                        totallen = len(predict_ans)\n",
    "                        tmp_eval_ans = count_right_predicted_num(predict_ans,y_cur_pred,dic,cur_masked_key)\n",
    "                        print('curmasked key is', cur_masked_key,'cur_iter_key is', cur_key,'percent is', tmp_eval_ans)\n",
    "                        cur_dic = {}\n",
    "                        cur_dic[cur_key] = tmp_eval_ans\n",
    "                        cur_key_ans_arr.append(cur_dic)\n",
    "                finalans[cur_masked_key] = cur_key_ans_arr\n",
    "    return finalans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create testing attribut arra\n",
    "arr = []\n",
    "og_arr = []\n",
    "og_train = {} #init global dictionary\n",
    "og_pred = {}\n",
    "arr.append(['age',1])\n",
    "arr.append(['mood_01',2]) #Today I generally feel\n",
    "arr.append(['mood_02',2])\n",
    "arr.append(['major',0])\n",
    "arr.append(['big5_01',2]) #I see myself as: Extraverted, enthusiastic.\n",
    "arr.append(['big5_02',2]) #I see myself as: Critical, quarrelsome.\n",
    "arr.append(['big5_03',2]) #I see myself as: Dependable, self-disciplined.\n",
    "arr.append(['big5_04',2]) #I see myself as: Anxious, easily upset.\n",
    "arr.append(['big5_05',2]) #I see myself as: Open to new experiences, complex.\n",
    "arr.append(['big5_06',2]) #I see myself as: Reserved, quiet.\n",
    "arr.append(['big5_07',2]) #I see myself as: Sympathetic, warm.\n",
    "arr.append(['big5_08',2]) #I see myself as: Disorganized, careless.\n",
    "arr.append(['big5_09',2]) #I see myself as: Calm, emotionally stable.\n",
    "arr.append(['big5_10',2]) #I see myself as: Conventional, uncreative.\n",
    "arr.append(['highpower',0])\n",
    "arr.append(['intrinsic_01',2])\n",
    "arr.append(['intrinsic_02',2])\n",
    "arr.append(['intrinsic_03',2])\n",
    "arr.append(['intrinsic_04',2])\n",
    "arr.append(['intrinsic_05',2])\n",
    "arr.append(['intrinsic_06',2])\n",
    "arr.append(['intrinsic_07',2])\n",
    "arr.append(['intrinsic_08',2])\n",
    "arr.append(['intrinsic_09',2])\n",
    "arr.append(['intrinsic_10',2])\n",
    "arr.append(['intrinsic_11',2])\n",
    "arr.append(['intrinsic_12',2])\n",
    "arr.append(['intrinsic_13',2])\n",
    "arr.append(['intrinsic_14',2])\n",
    "arr.append(['intrinsic_15',2])\n",
    "arr.append(['lowpower',0])\n",
    "arr.append(['mcfiller1',2])\n",
    "arr.append(['mcfiller2',2])\n",
    "arr.append(['mcfiller3',2])\n",
    "arr.append(['mcmost1',2])\n",
    "arr.append(['mcmost2',2])\n",
    "arr.append(['mcmost3',2])\n",
    "arr.append(['mcmost4',2])\n",
    "arr.append(['mcmost5',2])\n",
    "arr.append(['mcsome1',2])\n",
    "arr.append(['mcsome2',2])\n",
    "arr.append(['mcsome3',2])\n",
    "arr.append(['mcsome4',2])\n",
    "arr.append(['mcsome5',2])\n",
    "arr.append(['mcdv1',2])\n",
    "arr.append(['mcdv2',2])\n",
    "arr.append(['pate_01',2])\n",
    "arr.append(['pate_02',2])\n",
    "arr.append(['pate_03',2])\n",
    "arr.append(['pate_04',2])\n",
    "arr.append(['pate_05',2])\n",
    "arr.append(['stress_01',2])\n",
    "arr.append(['stress_02',2])\n",
    "arr.append(['stress_03',2])\n",
    "arr.append(['stress_04',2])\n",
    "arr.append(['nfc_01',2])\n",
    "arr.append(['nfc_02',2])\n",
    "arr.append(['nfc_03',2])\n",
    "arr.append(['nfc_04',2])\n",
    "arr.append(['nfc_05',2])\n",
    "arr.append(['nfc_06',2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur key is age\n",
      "cur key is mood_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 12)\n",
      "cur key is mood_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 20)\n",
      "cur key is major\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n",
      "/usr/lib/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "class_arr of nlp [0, 2, 4, 6, 7, 9, 12, 13, 17, 20, 23, 27, 29]\n",
      "(2998, 34)\n",
      "cur key is big5_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 42)\n",
      "cur key is big5_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 50)\n",
      "cur key is big5_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 58)\n",
      "cur key is big5_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 66)\n",
      "cur key is big5_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 74)\n",
      "cur key is big5_06\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 82)\n",
      "cur key is big5_07\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 90)\n",
      "cur key is big5_08\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 98)\n",
      "cur key is big5_09\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 106)\n",
      "cur key is big5_10\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 114)\n",
      "cur key is highpower\n",
      "nan\n",
      "class_arr of nlp [0, 2, 3, 4, 5, 7, 10, 13, 14, 17, 18, 20, 23, 25, 26, 29]\n",
      "(2998, 131)\n",
      "cur key is intrinsic_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 136)\n",
      "cur key is intrinsic_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 141)\n",
      "cur key is intrinsic_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 54.0]\n",
      "(2998, 147)\n",
      "cur key is intrinsic_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 77.0]\n",
      "(2998, 153)\n",
      "cur key is intrinsic_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 159)\n",
      "cur key is intrinsic_06\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 164)\n",
      "cur key is intrinsic_07\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 170)\n",
      "cur key is intrinsic_08\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 176)\n",
      "cur key is intrinsic_09\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 181)\n",
      "cur key is intrinsic_10\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 186)\n",
      "cur key is intrinsic_11\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 192)\n",
      "cur key is intrinsic_12\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 197)\n",
      "cur key is intrinsic_13\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 202)\n",
      "cur key is intrinsic_14\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 208)\n",
      "cur key is intrinsic_15\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 213)\n",
      "cur key is lowpower\n",
      "nan\n",
      "class_arr of nlp [0, 6, 9, 11, 19, 24, 27, 28]\n",
      "(2998, 222)\n",
      "cur key is mcfiller1\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 227)\n",
      "cur key is mcfiller2\n",
      "class_arr [0.0, 1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 233)\n",
      "cur key is mcfiller3\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 238)\n",
      "cur key is mcmost1\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 241)\n",
      "cur key is mcmost2\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 244)\n",
      "cur key is mcmost3\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 247)\n",
      "cur key is mcmost4\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 250)\n",
      "cur key is mcmost5\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 253)\n",
      "cur key is mcsome1\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 256)\n",
      "cur key is mcsome2\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 259)\n",
      "cur key is mcsome3\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 262)\n",
      "cur key is mcsome4\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 265)\n",
      "cur key is mcsome5\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 268)\n",
      "cur key is mcdv1\n",
      "class_arr [-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0]\n",
      "(2998, 276)\n",
      "cur key is mcdv2\n",
      "class_arr [-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0]\n",
      "(2998, 284)\n",
      "cur key is pate_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 290)\n",
      "cur key is pate_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 296)\n",
      "cur key is pate_03\n",
      "class_arr [1.0, 2.0, 3.0]\n",
      "(2998, 300)\n",
      "cur key is pate_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "(2998, 307)\n",
      "cur key is pate_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "(2998, 314)\n",
      "cur key is stress_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 320)\n",
      "cur key is stress_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 326)\n",
      "cur key is stress_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 332)\n",
      "cur key is stress_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 338)\n",
      "cur key is nfc_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 344)\n",
      "cur key is nfc_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 350)\n",
      "cur key is nfc_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 356)\n",
      "cur key is nfc_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 362)\n",
      "cur key is nfc_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 368)\n",
      "cur key is nfc_06\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 374)\n"
     ]
    }
   ],
   "source": [
    "# predict_df = df3.iloc[:100]\n",
    "trainning_df = df3\n",
    "# trainning_df = trainning_df.reset_index(drop = True)\n",
    "\n",
    "# X_train = semi_auto_append_attri(arr,predict_df,og)\n",
    "X_train = new_semi_auto_append_attri(arr,trainning_df,og_pred)\n",
    "# print('start training dataset')\n",
    "# X_tra = semi_auto_append_attri(arr,trainning_df,og_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = X_train[:2900]\n",
    "#slice nparr\n",
    "x_real_train = X_train[:2900]\n",
    "x_pred = X_train[2900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = createlist_list_string(og_pred)\n",
    "x,y = enlarge_dataset(X_train_new,X_train_new,og_pred,test)\n",
    "dic_look = create_dictionary_for_every_location_of_feature(og_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693137\n",
      "Cost after iteration 10: 0.608048\n",
      "Cost after iteration 20: 0.366226\n",
      "Cost after iteration 30: 0.321158\n",
      "Cost after iteration 40: 0.288153\n",
      "Cost after iteration 50: 0.275451\n",
      "Cost after iteration 60: 0.262345\n",
      "Cost after iteration 70: 0.256449\n",
      "Cost after iteration 80: 0.247787\n",
      "Cost after iteration 90: 0.243578\n",
      "Cost after iteration 100: 0.237524\n",
      "Cost after iteration 110: 0.232126\n",
      "Cost after iteration 120: 0.227163\n",
      "Cost after iteration 130: 0.223249\n",
      "Cost after iteration 140: 0.218627\n",
      "Cost after iteration 150: 0.212329\n",
      "Cost after iteration 160: 0.210667\n",
      "Cost after iteration 170: 0.204694\n",
      "Cost after iteration 180: 0.199388\n",
      "Cost after iteration 190: 0.193867\n",
      "current loc and des is 0 and  4  total cound is 185 percent is 0.37\n",
      "cur masked name is  age  eval value is:  0.37\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693338\n",
      "Cost after iteration 10: 0.608776\n",
      "Cost after iteration 20: 0.366608\n",
      "Cost after iteration 30: 0.321556\n",
      "Cost after iteration 40: 0.288409\n",
      "Cost after iteration 50: 0.278974\n",
      "Cost after iteration 60: 0.260370\n",
      "Cost after iteration 70: 0.253940\n",
      "Cost after iteration 80: 0.245077\n",
      "Cost after iteration 90: 0.240019\n",
      "Cost after iteration 100: 0.235181\n",
      "Cost after iteration 110: 0.229961\n",
      "Cost after iteration 120: 0.237340\n",
      "Cost after iteration 130: 0.227266\n",
      "Cost after iteration 140: 0.220758\n",
      "Cost after iteration 150: 0.214959\n",
      "Cost after iteration 160: 0.214390\n",
      "Cost after iteration 170: 0.201497\n",
      "Cost after iteration 180: 0.201086\n",
      "Cost after iteration 190: 0.198097\n",
      "current loc and des is 4 and  12  total cound is 174 percent is 0.348\n",
      "cur masked name is  mood_01  eval value is:  0.348\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693127\n",
      "Cost after iteration 10: 0.601749\n",
      "Cost after iteration 20: 0.368599\n",
      "Cost after iteration 30: 0.321109\n",
      "Cost after iteration 40: 0.287855\n",
      "Cost after iteration 50: 0.272725\n",
      "Cost after iteration 60: 0.261257\n",
      "Cost after iteration 70: 0.253761\n",
      "Cost after iteration 80: 0.249699\n",
      "Cost after iteration 90: 0.239791\n",
      "Cost after iteration 100: 0.235143\n",
      "Cost after iteration 110: 0.232598\n",
      "Cost after iteration 120: 0.226722\n",
      "Cost after iteration 130: 0.225945\n",
      "Cost after iteration 140: 0.222844\n",
      "Cost after iteration 150: 0.214591\n",
      "Cost after iteration 160: 0.206070\n",
      "Cost after iteration 170: 0.203863\n",
      "Cost after iteration 180: 0.199767\n",
      "Cost after iteration 190: 0.199108\n",
      "current loc and des is 12 and  20  total cound is 208 percent is 0.416\n",
      "cur masked name is  mood_02  eval value is:  0.416\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693394\n",
      "Cost after iteration 10: 0.605453\n",
      "Cost after iteration 20: 0.364920\n",
      "Cost after iteration 30: 0.321329\n",
      "Cost after iteration 40: 0.288261\n",
      "Cost after iteration 50: 0.272299\n",
      "Cost after iteration 60: 0.261667\n",
      "Cost after iteration 70: 0.253885\n",
      "Cost after iteration 80: 0.246508\n",
      "Cost after iteration 90: 0.251642\n",
      "Cost after iteration 100: 0.236738\n",
      "Cost after iteration 110: 0.237065\n",
      "Cost after iteration 120: 0.226338\n",
      "Cost after iteration 130: 0.225116\n",
      "Cost after iteration 140: 0.220155\n",
      "Cost after iteration 150: 0.213682\n",
      "Cost after iteration 160: 0.206078\n",
      "Cost after iteration 170: 0.201091\n",
      "Cost after iteration 180: 0.202911\n",
      "Cost after iteration 190: 0.192737\n",
      "current loc and des is 20 and  28  total cound is 132 percent is 0.264\n",
      "cur masked name is  big5_01  eval value is:  0.264\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693216\n",
      "Cost after iteration 10: 0.609908\n",
      "Cost after iteration 20: 0.364662\n",
      "Cost after iteration 30: 0.320563\n",
      "Cost after iteration 40: 0.287429\n",
      "Cost after iteration 50: 0.276524\n",
      "Cost after iteration 60: 0.262968\n",
      "Cost after iteration 70: 0.256003\n",
      "Cost after iteration 80: 0.246851\n",
      "Cost after iteration 90: 0.244377\n",
      "Cost after iteration 100: 0.236570\n",
      "Cost after iteration 110: 0.233083\n",
      "Cost after iteration 120: 0.227120\n",
      "Cost after iteration 130: 0.220464\n",
      "Cost after iteration 140: 0.220741\n",
      "Cost after iteration 150: 0.212499\n",
      "Cost after iteration 160: 0.206139\n",
      "Cost after iteration 170: 0.207104\n",
      "Cost after iteration 180: 0.197438\n",
      "Cost after iteration 190: 0.191740\n",
      "current loc and des is 28 and  36  total cound is 116 percent is 0.232\n",
      "cur masked name is  big5_02  eval value is:  0.232\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693520\n",
      "Cost after iteration 10: 0.598199\n",
      "Cost after iteration 20: 0.363223\n",
      "Cost after iteration 30: 0.320610\n",
      "Cost after iteration 40: 0.287777\n",
      "Cost after iteration 50: 0.274835\n",
      "Cost after iteration 60: 0.259578\n",
      "Cost after iteration 70: 0.251546\n",
      "Cost after iteration 80: 0.246105\n",
      "Cost after iteration 90: 0.242561\n",
      "Cost after iteration 100: 0.236229\n",
      "Cost after iteration 110: 0.230524\n",
      "Cost after iteration 120: 0.224704\n",
      "Cost after iteration 130: 0.222552\n",
      "Cost after iteration 140: 0.216702\n",
      "Cost after iteration 150: 0.210708\n",
      "Cost after iteration 160: 0.207259\n",
      "Cost after iteration 170: 0.201415\n",
      "Cost after iteration 180: 0.197888\n",
      "Cost after iteration 190: 0.193108\n",
      "current loc and des is 36 and  44  total cound is 171 percent is 0.342\n",
      "cur masked name is  big5_03  eval value is:  0.342\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693381\n",
      "Cost after iteration 10: 0.605445\n",
      "Cost after iteration 20: 0.367098\n",
      "Cost after iteration 30: 0.322237\n",
      "Cost after iteration 40: 0.289059\n",
      "Cost after iteration 50: 0.281173\n",
      "Cost after iteration 60: 0.261770\n",
      "Cost after iteration 70: 0.254606\n",
      "Cost after iteration 80: 0.246086\n",
      "Cost after iteration 90: 0.241666\n",
      "Cost after iteration 100: 0.237268\n",
      "Cost after iteration 110: 0.235144\n",
      "Cost after iteration 120: 0.225687\n",
      "Cost after iteration 130: 0.221561\n",
      "Cost after iteration 140: 0.220060\n",
      "Cost after iteration 150: 0.211686\n",
      "Cost after iteration 160: 0.213193\n",
      "Cost after iteration 170: 0.202465\n",
      "Cost after iteration 180: 0.195847\n",
      "Cost after iteration 190: 0.192621\n",
      "current loc and des is 44 and  52  total cound is 127 percent is 0.254\n",
      "cur masked name is  big5_04  eval value is:  0.254\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.692996\n",
      "Cost after iteration 10: 0.606016\n",
      "Cost after iteration 20: 0.369135\n",
      "Cost after iteration 30: 0.323916\n",
      "Cost after iteration 40: 0.289393\n",
      "Cost after iteration 50: 0.283367\n",
      "Cost after iteration 60: 0.262920\n",
      "Cost after iteration 70: 0.256721\n",
      "Cost after iteration 80: 0.247087\n",
      "Cost after iteration 90: 0.246804\n",
      "Cost after iteration 100: 0.239006\n",
      "Cost after iteration 110: 0.229495\n",
      "Cost after iteration 120: 0.228480\n",
      "Cost after iteration 130: 0.223378\n",
      "Cost after iteration 140: 0.215922\n",
      "Cost after iteration 150: 0.211377\n",
      "Cost after iteration 160: 0.212223\n",
      "Cost after iteration 170: 0.207805\n",
      "Cost after iteration 180: 0.200179\n",
      "Cost after iteration 190: 0.192070\n",
      "current loc and des is 52 and  60  total cound is 187 percent is 0.374\n",
      "cur masked name is  big5_05  eval value is:  0.374\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693439\n",
      "Cost after iteration 10: 0.611789\n",
      "Cost after iteration 20: 0.366909\n",
      "Cost after iteration 30: 0.321490\n",
      "Cost after iteration 40: 0.288312\n",
      "Cost after iteration 50: 0.282747\n",
      "Cost after iteration 60: 0.261194\n",
      "Cost after iteration 70: 0.253508\n",
      "Cost after iteration 80: 0.245993\n",
      "Cost after iteration 90: 0.251778\n",
      "Cost after iteration 100: 0.238000\n",
      "Cost after iteration 110: 0.240563\n",
      "Cost after iteration 120: 0.227100\n",
      "Cost after iteration 130: 0.224513\n",
      "Cost after iteration 140: 0.222396\n",
      "Cost after iteration 150: 0.218661\n",
      "Cost after iteration 160: 0.209091\n",
      "Cost after iteration 170: 0.206160\n",
      "Cost after iteration 180: 0.210810\n",
      "Cost after iteration 190: 0.196633\n",
      "current loc and des is 60 and  68  total cound is 126 percent is 0.252\n",
      "cur masked name is  big5_06  eval value is:  0.252\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693421\n",
      "Cost after iteration 10: 0.607490\n",
      "Cost after iteration 20: 0.366651\n",
      "Cost after iteration 30: 0.321881\n",
      "Cost after iteration 40: 0.287935\n",
      "Cost after iteration 50: 0.271248\n",
      "Cost after iteration 60: 0.258359\n",
      "Cost after iteration 70: 0.251913\n",
      "Cost after iteration 80: 0.245852\n",
      "Cost after iteration 90: 0.251469\n",
      "Cost after iteration 100: 0.237894\n",
      "Cost after iteration 110: 0.231917\n",
      "Cost after iteration 120: 0.226166\n",
      "Cost after iteration 130: 0.226991\n",
      "Cost after iteration 140: 0.221559\n",
      "Cost after iteration 150: 0.213671\n",
      "Cost after iteration 160: 0.209851\n",
      "Cost after iteration 170: 0.205368\n",
      "Cost after iteration 180: 0.199259\n",
      "Cost after iteration 190: 0.194880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loc and des is 68 and  76  total cound is 167 percent is 0.334\n",
      "cur masked name is  big5_07  eval value is:  0.334\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693349\n",
      "Cost after iteration 10: 0.602155\n",
      "Cost after iteration 20: 0.368933\n",
      "Cost after iteration 30: 0.321973\n",
      "Cost after iteration 40: 0.288587\n",
      "Cost after iteration 50: 0.277427\n",
      "Cost after iteration 60: 0.260672\n",
      "Cost after iteration 70: 0.254386\n",
      "Cost after iteration 80: 0.246495\n",
      "Cost after iteration 90: 0.242232\n",
      "Cost after iteration 100: 0.237617\n",
      "Cost after iteration 110: 0.233832\n",
      "Cost after iteration 120: 0.227371\n",
      "Cost after iteration 130: 0.220197\n",
      "Cost after iteration 140: 0.213409\n",
      "Cost after iteration 150: 0.215503\n",
      "Cost after iteration 160: 0.205877\n",
      "Cost after iteration 170: 0.205322\n",
      "Cost after iteration 180: 0.200915\n",
      "Cost after iteration 190: 0.189226\n",
      "current loc and des is 76 and  84  total cound is 130 percent is 0.26\n",
      "cur masked name is  big5_08  eval value is:  0.26\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.692814\n",
      "Cost after iteration 10: 0.606311\n",
      "Cost after iteration 20: 0.366165\n",
      "Cost after iteration 30: 0.320734\n",
      "Cost after iteration 40: 0.287525\n",
      "Cost after iteration 50: 0.276257\n",
      "Cost after iteration 60: 0.261955\n",
      "Cost after iteration 70: 0.255540\n",
      "Cost after iteration 80: 0.246853\n",
      "Cost after iteration 90: 0.244842\n",
      "Cost after iteration 100: 0.236803\n",
      "Cost after iteration 110: 0.233471\n",
      "Cost after iteration 120: 0.229260\n",
      "Cost after iteration 130: 0.223348\n",
      "Cost after iteration 140: 0.219063\n",
      "Cost after iteration 150: 0.211152\n",
      "Cost after iteration 160: 0.210215\n",
      "Cost after iteration 170: 0.206592\n",
      "Cost after iteration 180: 0.199310\n",
      "Cost after iteration 190: 0.193377\n",
      "current loc and des is 84 and  92  total cound is 141 percent is 0.282\n",
      "cur masked name is  big5_09  eval value is:  0.282\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693128\n",
      "Cost after iteration 10: 0.600065\n",
      "Cost after iteration 20: 0.368151\n",
      "Cost after iteration 30: 0.322740\n",
      "Cost after iteration 40: 0.288429\n",
      "Cost after iteration 50: 0.279227\n",
      "Cost after iteration 60: 0.262703\n",
      "Cost after iteration 70: 0.255274\n",
      "Cost after iteration 80: 0.247001\n",
      "Cost after iteration 90: 0.249687\n",
      "Cost after iteration 100: 0.239899\n",
      "Cost after iteration 110: 0.229366\n",
      "Cost after iteration 120: 0.229272\n",
      "Cost after iteration 130: 0.220557\n",
      "Cost after iteration 140: 0.218636\n",
      "Cost after iteration 150: 0.211025\n",
      "Cost after iteration 160: 0.208273\n",
      "Cost after iteration 170: 0.206354\n",
      "Cost after iteration 180: 0.204244\n",
      "Cost after iteration 190: 0.197439\n",
      "current loc and des is 92 and  100  total cound is 112 percent is 0.224\n",
      "cur masked name is  big5_10  eval value is:  0.224\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693498\n",
      "Cost after iteration 10: 0.605383\n",
      "Cost after iteration 20: 0.362619\n",
      "Cost after iteration 30: 0.320220\n",
      "Cost after iteration 40: 0.288049\n",
      "Cost after iteration 50: 0.271817\n",
      "Cost after iteration 60: 0.262107\n",
      "Cost after iteration 70: 0.254625\n",
      "Cost after iteration 80: 0.251075\n",
      "Cost after iteration 90: 0.240529\n",
      "Cost after iteration 100: 0.237591\n",
      "Cost after iteration 110: 0.232834\n",
      "Cost after iteration 120: 0.226331\n",
      "Cost after iteration 130: 0.227153\n",
      "Cost after iteration 140: 0.217545\n",
      "Cost after iteration 150: 0.211957\n",
      "Cost after iteration 160: 0.207458\n",
      "Cost after iteration 170: 0.203004\n",
      "Cost after iteration 180: 0.199095\n",
      "Cost after iteration 190: 0.193833\n",
      "current loc and des is 100 and  105  total cound is 254 percent is 0.508\n",
      "cur masked name is  intrinsic_01  eval value is:  0.508\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693230\n",
      "Cost after iteration 10: 0.607314\n",
      "Cost after iteration 20: 0.366107\n",
      "Cost after iteration 30: 0.321718\n",
      "Cost after iteration 40: 0.287923\n",
      "Cost after iteration 50: 0.282317\n",
      "Cost after iteration 60: 0.262711\n",
      "Cost after iteration 70: 0.255902\n",
      "Cost after iteration 80: 0.247586\n",
      "Cost after iteration 90: 0.251268\n",
      "Cost after iteration 100: 0.237345\n",
      "Cost after iteration 110: 0.236905\n",
      "Cost after iteration 120: 0.226856\n",
      "Cost after iteration 130: 0.225693\n",
      "Cost after iteration 140: 0.219222\n",
      "Cost after iteration 150: 0.215281\n",
      "Cost after iteration 160: 0.215552\n",
      "Cost after iteration 170: 0.208871\n",
      "Cost after iteration 180: 0.200957\n",
      "Cost after iteration 190: 0.205608\n",
      "current loc and des is 105 and  110  total cound is 264 percent is 0.528\n",
      "cur masked name is  intrinsic_02  eval value is:  0.528\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693407\n",
      "Cost after iteration 10: 0.609755\n",
      "Cost after iteration 20: 0.366881\n",
      "Cost after iteration 30: 0.322053\n",
      "Cost after iteration 40: 0.288361\n",
      "Cost after iteration 50: 0.271870\n",
      "Cost after iteration 60: 0.259886\n",
      "Cost after iteration 70: 0.253242\n",
      "Cost after iteration 80: 0.246787\n",
      "Cost after iteration 90: 0.249769\n",
      "Cost after iteration 100: 0.237313\n",
      "Cost after iteration 110: 0.232375\n",
      "Cost after iteration 120: 0.226998\n",
      "Cost after iteration 130: 0.226054\n",
      "Cost after iteration 140: 0.221302\n",
      "Cost after iteration 150: 0.213677\n",
      "Cost after iteration 160: 0.207490\n",
      "Cost after iteration 170: 0.202421\n",
      "Cost after iteration 180: 0.210060\n",
      "Cost after iteration 190: 0.193142\n",
      "current loc and des is 110 and  116  total cound is 228 percent is 0.456\n",
      "cur masked name is  intrinsic_03  eval value is:  0.456\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693101\n",
      "Cost after iteration 10: 0.600738\n",
      "Cost after iteration 20: 0.367489\n",
      "Cost after iteration 30: 0.322715\n",
      "Cost after iteration 40: 0.288959\n",
      "Cost after iteration 50: 0.275899\n",
      "Cost after iteration 60: 0.260399\n",
      "Cost after iteration 70: 0.255029\n",
      "Cost after iteration 80: 0.247253\n",
      "Cost after iteration 90: 0.243639\n",
      "Cost after iteration 100: 0.236369\n",
      "Cost after iteration 110: 0.232569\n",
      "Cost after iteration 120: 0.224968\n",
      "Cost after iteration 130: 0.220550\n",
      "Cost after iteration 140: 0.216301\n",
      "Cost after iteration 150: 0.212466\n",
      "Cost after iteration 160: 0.211863\n",
      "Cost after iteration 170: 0.209394\n",
      "Cost after iteration 180: 0.202797\n",
      "Cost after iteration 190: 0.190819\n",
      "current loc and des is 116 and  122  total cound is 261 percent is 0.522\n",
      "cur masked name is  intrinsic_04  eval value is:  0.522\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693376\n",
      "Cost after iteration 10: 0.603594\n",
      "Cost after iteration 20: 0.365117\n",
      "Cost after iteration 30: 0.320282\n",
      "Cost after iteration 40: 0.287612\n",
      "Cost after iteration 50: 0.273106\n",
      "Cost after iteration 60: 0.262746\n",
      "Cost after iteration 70: 0.252438\n",
      "Cost after iteration 80: 0.251324\n",
      "Cost after iteration 90: 0.242397\n",
      "Cost after iteration 100: 0.241379\n",
      "Cost after iteration 110: 0.230768\n",
      "Cost after iteration 120: 0.225651\n",
      "Cost after iteration 130: 0.223086\n",
      "Cost after iteration 140: 0.220802\n",
      "Cost after iteration 150: 0.211050\n",
      "Cost after iteration 160: 0.207626\n",
      "Cost after iteration 170: 0.202597\n",
      "Cost after iteration 180: 0.200698\n",
      "Cost after iteration 190: 0.195657\n",
      "current loc and des is 122 and  128  total cound is 210 percent is 0.42\n",
      "cur masked name is  intrinsic_05  eval value is:  0.42\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693350\n",
      "Cost after iteration 10: 0.607806\n",
      "Cost after iteration 20: 0.366991\n",
      "Cost after iteration 30: 0.322473\n",
      "Cost after iteration 40: 0.288745\n",
      "Cost after iteration 50: 0.272494\n",
      "Cost after iteration 60: 0.260081\n",
      "Cost after iteration 70: 0.252984\n",
      "Cost after iteration 80: 0.246091\n",
      "Cost after iteration 90: 0.248100\n",
      "Cost after iteration 100: 0.236504\n",
      "Cost after iteration 110: 0.234472\n",
      "Cost after iteration 120: 0.225502\n",
      "Cost after iteration 130: 0.224796\n",
      "Cost after iteration 140: 0.221324\n",
      "Cost after iteration 150: 0.216304\n",
      "Cost after iteration 160: 0.211642\n",
      "Cost after iteration 170: 0.202789\n",
      "Cost after iteration 180: 0.195900\n",
      "Cost after iteration 190: 0.192026\n",
      "current loc and des is 128 and  133  total cound is 197 percent is 0.394\n",
      "cur masked name is  intrinsic_06  eval value is:  0.394\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693078\n",
      "Cost after iteration 10: 0.610453\n",
      "Cost after iteration 20: 0.365840\n",
      "Cost after iteration 30: 0.321288\n",
      "Cost after iteration 40: 0.287897\n",
      "Cost after iteration 50: 0.282282\n",
      "Cost after iteration 60: 0.264209\n",
      "Cost after iteration 70: 0.255034\n",
      "Cost after iteration 80: 0.246416\n",
      "Cost after iteration 90: 0.248195\n",
      "Cost after iteration 100: 0.236478\n",
      "Cost after iteration 110: 0.234156\n",
      "Cost after iteration 120: 0.227579\n",
      "Cost after iteration 130: 0.221787\n",
      "Cost after iteration 140: 0.221031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 150: 0.211830\n",
      "Cost after iteration 160: 0.207758\n",
      "Cost after iteration 170: 0.206549\n",
      "Cost after iteration 180: 0.199966\n",
      "Cost after iteration 190: 0.201031\n",
      "current loc and des is 133 and  139  total cound is 222 percent is 0.444\n",
      "cur masked name is  intrinsic_07  eval value is:  0.444\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.692844\n",
      "Cost after iteration 10: 0.608390\n",
      "Cost after iteration 20: 0.365803\n",
      "Cost after iteration 30: 0.323466\n",
      "Cost after iteration 40: 0.290505\n",
      "Cost after iteration 50: 0.283459\n",
      "Cost after iteration 60: 0.264228\n",
      "Cost after iteration 70: 0.257194\n",
      "Cost after iteration 80: 0.246894\n",
      "Cost after iteration 90: 0.240349\n",
      "Cost after iteration 100: 0.236990\n",
      "Cost after iteration 110: 0.230340\n",
      "Cost after iteration 120: 0.228695\n",
      "Cost after iteration 130: 0.224564\n",
      "Cost after iteration 140: 0.218090\n",
      "Cost after iteration 150: 0.214751\n",
      "Cost after iteration 160: 0.210779\n",
      "Cost after iteration 170: 0.202022\n",
      "Cost after iteration 180: 0.199342\n",
      "Cost after iteration 190: 0.197666\n",
      "current loc and des is 139 and  145  total cound is 239 percent is 0.478\n",
      "cur masked name is  intrinsic_08  eval value is:  0.478\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693481\n",
      "Cost after iteration 10: 0.607129\n",
      "Cost after iteration 20: 0.364416\n",
      "Cost after iteration 30: 0.320580\n",
      "Cost after iteration 40: 0.287765\n",
      "Cost after iteration 50: 0.272869\n",
      "Cost after iteration 60: 0.257736\n",
      "Cost after iteration 70: 0.252171\n",
      "Cost after iteration 80: 0.245610\n",
      "Cost after iteration 90: 0.247106\n",
      "Cost after iteration 100: 0.236744\n",
      "Cost after iteration 110: 0.234163\n",
      "Cost after iteration 120: 0.226932\n",
      "Cost after iteration 130: 0.222917\n",
      "Cost after iteration 140: 0.220195\n",
      "Cost after iteration 150: 0.211607\n",
      "Cost after iteration 160: 0.208132\n",
      "Cost after iteration 170: 0.204177\n",
      "Cost after iteration 180: 0.202262\n",
      "Cost after iteration 190: 0.200511\n",
      "current loc and des is 145 and  150  total cound is 217 percent is 0.434\n",
      "cur masked name is  intrinsic_09  eval value is:  0.434\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.692839\n",
      "Cost after iteration 10: 0.607989\n",
      "Cost after iteration 20: 0.369883\n",
      "Cost after iteration 30: 0.324486\n",
      "Cost after iteration 40: 0.290694\n",
      "Cost after iteration 50: 0.273847\n",
      "Cost after iteration 60: 0.260928\n",
      "Cost after iteration 70: 0.258469\n",
      "Cost after iteration 80: 0.247958\n",
      "Cost after iteration 90: 0.244996\n",
      "Cost after iteration 100: 0.236568\n",
      "Cost after iteration 110: 0.234714\n",
      "Cost after iteration 120: 0.229923\n",
      "Cost after iteration 130: 0.225357\n",
      "Cost after iteration 140: 0.218812\n",
      "Cost after iteration 150: 0.218327\n",
      "Cost after iteration 160: 0.208903\n",
      "Cost after iteration 170: 0.205313\n",
      "Cost after iteration 180: 0.198419\n",
      "Cost after iteration 190: 0.196022\n",
      "current loc and des is 150 and  155  total cound is 284 percent is 0.568\n",
      "cur masked name is  intrinsic_10  eval value is:  0.568\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693663\n",
      "Cost after iteration 10: 0.595637\n",
      "Cost after iteration 20: 0.363643\n",
      "Cost after iteration 30: 0.321290\n",
      "Cost after iteration 40: 0.288150\n",
      "Cost after iteration 50: 0.276235\n",
      "Cost after iteration 60: 0.259999\n",
      "Cost after iteration 70: 0.253226\n",
      "Cost after iteration 80: 0.245685\n",
      "Cost after iteration 90: 0.247314\n",
      "Cost after iteration 100: 0.236118\n",
      "Cost after iteration 110: 0.236767\n",
      "Cost after iteration 120: 0.225082\n",
      "Cost after iteration 130: 0.221694\n",
      "Cost after iteration 140: 0.220247\n",
      "Cost after iteration 150: 0.210816\n",
      "Cost after iteration 160: 0.205081\n",
      "Cost after iteration 170: 0.201808\n",
      "Cost after iteration 180: 0.194284\n",
      "Cost after iteration 190: 0.191724\n",
      "current loc and des is 155 and  161  total cound is 217 percent is 0.434\n",
      "cur masked name is  intrinsic_11  eval value is:  0.434\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693272\n",
      "Cost after iteration 10: 0.598776\n",
      "Cost after iteration 20: 0.363956\n",
      "Cost after iteration 30: 0.321065\n",
      "Cost after iteration 40: 0.287718\n",
      "Cost after iteration 50: 0.278514\n",
      "Cost after iteration 60: 0.260830\n",
      "Cost after iteration 70: 0.253970\n",
      "Cost after iteration 80: 0.246387\n",
      "Cost after iteration 90: 0.255263\n",
      "Cost after iteration 100: 0.237256\n",
      "Cost after iteration 110: 0.228972\n",
      "Cost after iteration 120: 0.228248\n",
      "Cost after iteration 130: 0.222812\n",
      "Cost after iteration 140: 0.222895\n",
      "Cost after iteration 150: 0.212278\n",
      "Cost after iteration 160: 0.206012\n",
      "Cost after iteration 170: 0.198589\n",
      "Cost after iteration 180: 0.198935\n",
      "Cost after iteration 190: 0.194709\n",
      "current loc and des is 161 and  166  total cound is 217 percent is 0.434\n",
      "cur masked name is  intrinsic_12  eval value is:  0.434\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693202\n",
      "Cost after iteration 10: 0.603883\n",
      "Cost after iteration 20: 0.365704\n",
      "Cost after iteration 30: 0.320873\n",
      "Cost after iteration 40: 0.287959\n",
      "Cost after iteration 50: 0.277887\n",
      "Cost after iteration 60: 0.261819\n",
      "Cost after iteration 70: 0.253725\n",
      "Cost after iteration 80: 0.251999\n",
      "Cost after iteration 90: 0.240916\n",
      "Cost after iteration 100: 0.237227\n",
      "Cost after iteration 110: 0.231672\n",
      "Cost after iteration 120: 0.225637\n",
      "Cost after iteration 130: 0.225116\n",
      "Cost after iteration 140: 0.222658\n",
      "Cost after iteration 150: 0.212639\n",
      "Cost after iteration 160: 0.207962\n",
      "Cost after iteration 170: 0.202347\n",
      "Cost after iteration 180: 0.199851\n",
      "Cost after iteration 190: 0.195205\n",
      "current loc and des is 166 and  171  total cound is 245 percent is 0.49\n",
      "cur masked name is  intrinsic_13  eval value is:  0.49\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693178\n",
      "Cost after iteration 10: 0.603859\n",
      "Cost after iteration 20: 0.367595\n",
      "Cost after iteration 30: 0.321976\n",
      "Cost after iteration 40: 0.288858\n",
      "Cost after iteration 50: 0.273013\n",
      "Cost after iteration 60: 0.259799\n",
      "Cost after iteration 70: 0.253221\n",
      "Cost after iteration 80: 0.246336\n",
      "Cost after iteration 90: 0.248667\n",
      "Cost after iteration 100: 0.237549\n",
      "Cost after iteration 110: 0.233965\n",
      "Cost after iteration 120: 0.228749\n",
      "Cost after iteration 130: 0.221399\n",
      "Cost after iteration 140: 0.216123\n",
      "Cost after iteration 150: 0.217282\n",
      "Cost after iteration 160: 0.212769\n",
      "Cost after iteration 170: 0.213913\n",
      "Cost after iteration 180: 0.199488\n",
      "Cost after iteration 190: 0.194965\n",
      "current loc and des is 171 and  177  total cound is 184 percent is 0.368\n",
      "cur masked name is  intrinsic_14  eval value is:  0.368\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693247\n",
      "Cost after iteration 10: 0.608827\n",
      "Cost after iteration 20: 0.364607\n",
      "Cost after iteration 30: 0.320814\n",
      "Cost after iteration 40: 0.287953\n",
      "Cost after iteration 50: 0.284612\n",
      "Cost after iteration 60: 0.262937\n",
      "Cost after iteration 70: 0.256862\n",
      "Cost after iteration 80: 0.246622\n",
      "Cost after iteration 90: 0.240514\n",
      "Cost after iteration 100: 0.236161\n",
      "Cost after iteration 110: 0.228935\n",
      "Cost after iteration 120: 0.228397\n",
      "Cost after iteration 130: 0.221812\n",
      "Cost after iteration 140: 0.217526\n",
      "Cost after iteration 150: 0.213805\n",
      "Cost after iteration 160: 0.208189\n",
      "Cost after iteration 170: 0.205936\n",
      "Cost after iteration 180: 0.201857\n",
      "Cost after iteration 190: 0.193351\n",
      "current loc and des is 177 and  182  total cound is 280 percent is 0.56\n",
      "cur masked name is  intrinsic_15  eval value is:  0.56\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.692677\n",
      "Cost after iteration 10: 0.601994\n",
      "Cost after iteration 20: 0.369554\n",
      "Cost after iteration 30: 0.322324\n",
      "Cost after iteration 40: 0.288737\n",
      "Cost after iteration 50: 0.283345\n",
      "Cost after iteration 60: 0.262288\n",
      "Cost after iteration 70: 0.253847\n",
      "Cost after iteration 80: 0.246776\n",
      "Cost after iteration 90: 0.239781\n",
      "Cost after iteration 100: 0.234732\n",
      "Cost after iteration 110: 0.229497\n",
      "Cost after iteration 120: 0.226878\n",
      "Cost after iteration 130: 0.227875\n",
      "Cost after iteration 140: 0.218250\n",
      "Cost after iteration 150: 0.216198\n",
      "Cost after iteration 160: 0.208823\n",
      "Cost after iteration 170: 0.201219\n",
      "Cost after iteration 180: 0.196020\n",
      "Cost after iteration 190: 0.189768\n",
      "current loc and des is 182 and  187  total cound is 269 percent is 0.538\n",
      "cur masked name is  mcfiller1  eval value is:  0.538\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693486\n",
      "Cost after iteration 10: 0.599339\n",
      "Cost after iteration 20: 0.361372\n",
      "Cost after iteration 30: 0.319253\n",
      "Cost after iteration 40: 0.286663\n",
      "Cost after iteration 50: 0.279126\n",
      "Cost after iteration 60: 0.260260\n",
      "Cost after iteration 70: 0.252365\n",
      "Cost after iteration 80: 0.246145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 90: 0.253943\n",
      "Cost after iteration 100: 0.235575\n",
      "Cost after iteration 110: 0.232020\n",
      "Cost after iteration 120: 0.227651\n",
      "Cost after iteration 130: 0.222736\n",
      "Cost after iteration 140: 0.216552\n",
      "Cost after iteration 150: 0.211640\n",
      "Cost after iteration 160: 0.204418\n",
      "Cost after iteration 170: 0.203054\n",
      "Cost after iteration 180: 0.197679\n",
      "Cost after iteration 190: 0.191775\n",
      "current loc and des is 187 and  193  total cound is 265 percent is 0.53\n",
      "cur masked name is  mcfiller2  eval value is:  0.53\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693167\n",
      "Cost after iteration 10: 0.604700\n",
      "Cost after iteration 20: 0.363842\n",
      "Cost after iteration 30: 0.320261\n",
      "Cost after iteration 40: 0.287123\n",
      "Cost after iteration 50: 0.280987\n",
      "Cost after iteration 60: 0.261228\n",
      "Cost after iteration 70: 0.251362\n",
      "Cost after iteration 80: 0.244916\n",
      "Cost after iteration 90: 0.242042\n",
      "Cost after iteration 100: 0.234255\n",
      "Cost after iteration 110: 0.232791\n",
      "Cost after iteration 120: 0.225783\n",
      "Cost after iteration 130: 0.223110\n",
      "Cost after iteration 140: 0.222757\n",
      "Cost after iteration 150: 0.213048\n",
      "Cost after iteration 160: 0.206698\n",
      "Cost after iteration 170: 0.205516\n",
      "Cost after iteration 180: 0.198400\n",
      "Cost after iteration 190: 0.189972\n",
      "current loc and des is 193 and  198  total cound is 316 percent is 0.632\n",
      "cur masked name is  mcfiller3  eval value is:  0.632\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693176\n",
      "Cost after iteration 10: 0.612768\n",
      "Cost after iteration 20: 0.369885\n",
      "Cost after iteration 30: 0.330211\n",
      "Cost after iteration 40: 0.289244\n",
      "Cost after iteration 50: 0.268514\n",
      "Cost after iteration 60: 0.257440\n",
      "Cost after iteration 70: 0.253211\n",
      "Cost after iteration 80: 0.245831\n",
      "Cost after iteration 90: 0.249538\n",
      "Cost after iteration 100: 0.238961\n",
      "Cost after iteration 110: 0.233931\n",
      "Cost after iteration 120: 0.227558\n",
      "Cost after iteration 130: 0.226527\n",
      "Cost after iteration 140: 0.222141\n",
      "Cost after iteration 150: 0.213886\n",
      "Cost after iteration 160: 0.208380\n",
      "Cost after iteration 170: 0.208697\n",
      "Cost after iteration 180: 0.201261\n",
      "Cost after iteration 190: 0.194685\n",
      "current loc and des is 198 and  201  total cound is 188 percent is 0.376\n",
      "cur masked name is  mcmost1  eval value is:  0.376\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693460\n",
      "Cost after iteration 10: 0.604970\n",
      "Cost after iteration 20: 0.370740\n",
      "Cost after iteration 30: 0.325771\n",
      "Cost after iteration 40: 0.287331\n",
      "Cost after iteration 50: 0.283537\n",
      "Cost after iteration 60: 0.262991\n",
      "Cost after iteration 70: 0.254546\n",
      "Cost after iteration 80: 0.245097\n",
      "Cost after iteration 90: 0.243220\n",
      "Cost after iteration 100: 0.238854\n",
      "Cost after iteration 110: 0.238934\n",
      "Cost after iteration 120: 0.229169\n",
      "Cost after iteration 130: 0.223354\n",
      "Cost after iteration 140: 0.217915\n",
      "Cost after iteration 150: 0.211805\n",
      "Cost after iteration 160: 0.209709\n",
      "Cost after iteration 170: 0.205957\n",
      "Cost after iteration 180: 0.202284\n",
      "Cost after iteration 190: 0.203331\n",
      "current loc and des is 201 and  204  total cound is 168 percent is 0.336\n",
      "cur masked name is  mcmost2  eval value is:  0.336\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693352\n",
      "Cost after iteration 10: 0.613435\n",
      "Cost after iteration 20: 0.368689\n",
      "Cost after iteration 30: 0.325324\n",
      "Cost after iteration 40: 0.288202\n",
      "Cost after iteration 50: 0.267609\n",
      "Cost after iteration 60: 0.258237\n",
      "Cost after iteration 70: 0.252144\n",
      "Cost after iteration 80: 0.246842\n",
      "Cost after iteration 90: 0.245998\n",
      "Cost after iteration 100: 0.235990\n",
      "Cost after iteration 110: 0.237069\n",
      "Cost after iteration 120: 0.226775\n",
      "Cost after iteration 130: 0.222698\n",
      "Cost after iteration 140: 0.223349\n",
      "Cost after iteration 150: 0.218273\n",
      "Cost after iteration 160: 0.208924\n",
      "Cost after iteration 170: 0.206031\n",
      "Cost after iteration 180: 0.207149\n",
      "Cost after iteration 190: 0.194373\n",
      "current loc and des is 204 and  207  total cound is 209 percent is 0.418\n",
      "cur masked name is  mcmost3  eval value is:  0.418\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693379\n",
      "Cost after iteration 10: 0.610186\n",
      "Cost after iteration 20: 0.368060\n",
      "Cost after iteration 30: 0.324966\n",
      "Cost after iteration 40: 0.287094\n",
      "Cost after iteration 50: 0.266951\n",
      "Cost after iteration 60: 0.278043\n",
      "Cost after iteration 70: 0.258539\n",
      "Cost after iteration 80: 0.249112\n",
      "Cost after iteration 90: 0.247676\n",
      "Cost after iteration 100: 0.238523\n",
      "Cost after iteration 110: 0.237264\n",
      "Cost after iteration 120: 0.227529\n",
      "Cost after iteration 130: 0.224256\n",
      "Cost after iteration 140: 0.222197\n",
      "Cost after iteration 150: 0.213323\n",
      "Cost after iteration 160: 0.211240\n",
      "Cost after iteration 170: 0.207818\n",
      "Cost after iteration 180: 0.202835\n",
      "Cost after iteration 190: 0.196998\n",
      "current loc and des is 207 and  210  total cound is 161 percent is 0.322\n",
      "cur masked name is  mcmost4  eval value is:  0.322\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693314\n",
      "Cost after iteration 10: 0.615050\n",
      "Cost after iteration 20: 0.374565\n",
      "Cost after iteration 30: 0.333826\n",
      "Cost after iteration 40: 0.291789\n",
      "Cost after iteration 50: 0.274310\n",
      "Cost after iteration 60: 0.261199\n",
      "Cost after iteration 70: 0.254780\n",
      "Cost after iteration 80: 0.246944\n",
      "Cost after iteration 90: 0.248307\n",
      "Cost after iteration 100: 0.236042\n",
      "Cost after iteration 110: 0.233530\n",
      "Cost after iteration 120: 0.225807\n",
      "Cost after iteration 130: 0.225119\n",
      "Cost after iteration 140: 0.217691\n",
      "Cost after iteration 150: 0.213926\n",
      "Cost after iteration 160: 0.211308\n",
      "Cost after iteration 170: 0.205705\n",
      "Cost after iteration 180: 0.201806\n",
      "Cost after iteration 190: 0.194505\n",
      "current loc and des is 210 and  213  total cound is 227 percent is 0.454\n",
      "cur masked name is  mcmost5  eval value is:  0.454\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.692932\n",
      "Cost after iteration 10: 0.614064\n",
      "Cost after iteration 20: 0.371984\n",
      "Cost after iteration 30: 0.328275\n",
      "Cost after iteration 40: 0.290916\n",
      "Cost after iteration 50: 0.276281\n",
      "Cost after iteration 60: 0.259668\n",
      "Cost after iteration 70: 0.252296\n",
      "Cost after iteration 80: 0.249616\n",
      "Cost after iteration 90: 0.242255\n",
      "Cost after iteration 100: 0.239220\n",
      "Cost after iteration 110: 0.231576\n",
      "Cost after iteration 120: 0.227206\n",
      "Cost after iteration 130: 0.226375\n",
      "Cost after iteration 140: 0.225219\n",
      "Cost after iteration 150: 0.212826\n",
      "Cost after iteration 160: 0.210372\n",
      "Cost after iteration 170: 0.201844\n",
      "Cost after iteration 180: 0.202593\n",
      "Cost after iteration 190: 0.199439\n",
      "current loc and des is 213 and  216  total cound is 141 percent is 0.282\n",
      "cur masked name is  mcsome1  eval value is:  0.282\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693044\n",
      "Cost after iteration 10: 0.605242\n",
      "Cost after iteration 20: 0.371614\n",
      "Cost after iteration 30: 0.328366\n",
      "Cost after iteration 40: 0.289824\n",
      "Cost after iteration 50: 0.282747\n",
      "Cost after iteration 60: 0.262664\n",
      "Cost after iteration 70: 0.252159\n",
      "Cost after iteration 80: 0.252710\n",
      "Cost after iteration 90: 0.242303\n",
      "Cost after iteration 100: 0.240117\n",
      "Cost after iteration 110: 0.230891\n",
      "Cost after iteration 120: 0.226845\n",
      "Cost after iteration 130: 0.222831\n",
      "Cost after iteration 140: 0.223618\n",
      "Cost after iteration 150: 0.212760\n",
      "Cost after iteration 160: 0.209835\n",
      "Cost after iteration 170: 0.207152\n",
      "Cost after iteration 180: 0.203618\n",
      "Cost after iteration 190: 0.194703\n",
      "current loc and des is 216 and  219  total cound is 143 percent is 0.286\n",
      "cur masked name is  mcsome2  eval value is:  0.286\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.692982\n",
      "Cost after iteration 10: 0.616224\n",
      "Cost after iteration 20: 0.371370\n",
      "Cost after iteration 30: 0.330171\n",
      "Cost after iteration 40: 0.290811\n",
      "Cost after iteration 50: 0.270009\n",
      "Cost after iteration 60: 0.259488\n",
      "Cost after iteration 70: 0.253471\n",
      "Cost after iteration 80: 0.246427\n",
      "Cost after iteration 90: 0.247677\n",
      "Cost after iteration 100: 0.238269\n",
      "Cost after iteration 110: 0.236384\n",
      "Cost after iteration 120: 0.228529\n",
      "Cost after iteration 130: 0.224906\n",
      "Cost after iteration 140: 0.220795\n",
      "Cost after iteration 150: 0.218676\n",
      "Cost after iteration 160: 0.211928\n",
      "Cost after iteration 170: 0.202404\n",
      "Cost after iteration 180: 0.198787\n",
      "Cost after iteration 190: 0.192837\n",
      "current loc and des is 219 and  222  total cound is 143 percent is 0.286\n",
      "cur masked name is  mcsome3  eval value is:  0.286\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693459\n",
      "Cost after iteration 10: 0.613226\n",
      "Cost after iteration 20: 0.371928\n",
      "Cost after iteration 30: 0.329409\n",
      "Cost after iteration 40: 0.290089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 50: 0.279033\n",
      "Cost after iteration 60: 0.261217\n",
      "Cost after iteration 70: 0.253278\n",
      "Cost after iteration 80: 0.247042\n",
      "Cost after iteration 90: 0.243837\n",
      "Cost after iteration 100: 0.236794\n",
      "Cost after iteration 110: 0.237149\n",
      "Cost after iteration 120: 0.227932\n",
      "Cost after iteration 130: 0.222578\n",
      "Cost after iteration 140: 0.217393\n",
      "Cost after iteration 150: 0.218502\n",
      "Cost after iteration 160: 0.209406\n",
      "Cost after iteration 170: 0.207284\n",
      "Cost after iteration 180: 0.203524\n",
      "Cost after iteration 190: 0.196796\n",
      "current loc and des is 222 and  225  total cound is 140 percent is 0.28\n",
      "cur masked name is  mcsome4  eval value is:  0.28\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.692947\n",
      "Cost after iteration 10: 0.613454\n",
      "Cost after iteration 20: 0.370950\n",
      "Cost after iteration 30: 0.330205\n",
      "Cost after iteration 40: 0.289781\n",
      "Cost after iteration 50: 0.268393\n",
      "Cost after iteration 60: 0.259171\n",
      "Cost after iteration 70: 0.253818\n",
      "Cost after iteration 80: 0.247480\n",
      "Cost after iteration 90: 0.246727\n",
      "Cost after iteration 100: 0.237160\n",
      "Cost after iteration 110: 0.237311\n",
      "Cost after iteration 120: 0.227653\n",
      "Cost after iteration 130: 0.223806\n",
      "Cost after iteration 140: 0.217084\n",
      "Cost after iteration 150: 0.216506\n",
      "Cost after iteration 160: 0.210351\n",
      "Cost after iteration 170: 0.209440\n",
      "Cost after iteration 180: 0.200363\n",
      "Cost after iteration 190: 0.194785\n",
      "current loc and des is 225 and  228  total cound is 152 percent is 0.304\n",
      "cur masked name is  mcsome5  eval value is:  0.304\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693368\n",
      "Cost after iteration 10: 0.606227\n",
      "Cost after iteration 20: 0.365554\n",
      "Cost after iteration 30: 0.321173\n",
      "Cost after iteration 40: 0.288086\n",
      "Cost after iteration 50: 0.286036\n",
      "Cost after iteration 60: 0.261517\n",
      "Cost after iteration 70: 0.252660\n",
      "Cost after iteration 80: 0.244716\n",
      "Cost after iteration 90: 0.242447\n",
      "Cost after iteration 100: 0.234838\n",
      "Cost after iteration 110: 0.235543\n",
      "Cost after iteration 120: 0.227339\n",
      "Cost after iteration 130: 0.221826\n",
      "Cost after iteration 140: 0.222187\n",
      "Cost after iteration 150: 0.213117\n",
      "Cost after iteration 160: 0.205195\n",
      "Cost after iteration 170: 0.204251\n",
      "Cost after iteration 180: 0.200534\n",
      "Cost after iteration 190: 0.198201\n",
      "current loc and des is 228 and  236  total cound is 271 percent is 0.542\n",
      "cur masked name is  mcdv1  eval value is:  0.542\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693018\n",
      "Cost after iteration 10: 0.606885\n",
      "Cost after iteration 20: 0.369134\n",
      "Cost after iteration 30: 0.324251\n",
      "Cost after iteration 40: 0.290063\n",
      "Cost after iteration 50: 0.282106\n",
      "Cost after iteration 60: 0.262756\n",
      "Cost after iteration 70: 0.256963\n",
      "Cost after iteration 80: 0.247542\n",
      "Cost after iteration 90: 0.243447\n",
      "Cost after iteration 100: 0.237252\n",
      "Cost after iteration 110: 0.229016\n",
      "Cost after iteration 120: 0.227738\n",
      "Cost after iteration 130: 0.226978\n",
      "Cost after iteration 140: 0.222517\n",
      "Cost after iteration 150: 0.214373\n",
      "Cost after iteration 160: 0.209845\n",
      "Cost after iteration 170: 0.203311\n",
      "Cost after iteration 180: 0.198793\n",
      "Cost after iteration 190: 0.205634\n",
      "current loc and des is 236 and  244  total cound is 203 percent is 0.406\n",
      "cur masked name is  mcdv2  eval value is:  0.406\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693034\n",
      "Cost after iteration 10: 0.600721\n",
      "Cost after iteration 20: 0.368130\n",
      "Cost after iteration 30: 0.320567\n",
      "Cost after iteration 40: 0.287042\n",
      "Cost after iteration 50: 0.279657\n",
      "Cost after iteration 60: 0.263382\n",
      "Cost after iteration 70: 0.253324\n",
      "Cost after iteration 80: 0.250421\n",
      "Cost after iteration 90: 0.240160\n",
      "Cost after iteration 100: 0.235748\n",
      "Cost after iteration 110: 0.232442\n",
      "Cost after iteration 120: 0.225636\n",
      "Cost after iteration 130: 0.224217\n",
      "Cost after iteration 140: 0.223852\n",
      "Cost after iteration 150: 0.212863\n",
      "Cost after iteration 160: 0.206844\n",
      "Cost after iteration 170: 0.200526\n",
      "Cost after iteration 180: 0.198999\n",
      "Cost after iteration 190: 0.193664\n",
      "current loc and des is 244 and  250  total cound is 162 percent is 0.324\n",
      "cur masked name is  pate_01  eval value is:  0.324\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693171\n",
      "Cost after iteration 10: 0.607923\n",
      "Cost after iteration 20: 0.366058\n",
      "Cost after iteration 30: 0.322243\n",
      "Cost after iteration 40: 0.288701\n",
      "Cost after iteration 50: 0.285239\n",
      "Cost after iteration 60: 0.263584\n",
      "Cost after iteration 70: 0.255970\n",
      "Cost after iteration 80: 0.246305\n",
      "Cost after iteration 90: 0.250156\n",
      "Cost after iteration 100: 0.237359\n",
      "Cost after iteration 110: 0.234095\n",
      "Cost after iteration 120: 0.228547\n",
      "Cost after iteration 130: 0.220722\n",
      "Cost after iteration 140: 0.218318\n",
      "Cost after iteration 150: 0.220281\n",
      "Cost after iteration 160: 0.207906\n",
      "Cost after iteration 170: 0.205659\n",
      "Cost after iteration 180: 0.204168\n",
      "Cost after iteration 190: 0.192787\n",
      "current loc and des is 250 and  256  total cound is 200 percent is 0.4\n",
      "cur masked name is  pate_02  eval value is:  0.4\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.692720\n",
      "Cost after iteration 10: 0.601989\n",
      "Cost after iteration 20: 0.364200\n",
      "Cost after iteration 30: 0.320806\n",
      "Cost after iteration 40: 0.287574\n",
      "Cost after iteration 50: 0.278731\n",
      "Cost after iteration 60: 0.260797\n",
      "Cost after iteration 70: 0.254509\n",
      "Cost after iteration 80: 0.246233\n",
      "Cost after iteration 90: 0.242426\n",
      "Cost after iteration 100: 0.237024\n",
      "Cost after iteration 110: 0.232132\n",
      "Cost after iteration 120: 0.227978\n",
      "Cost after iteration 130: 0.222713\n",
      "Cost after iteration 140: 0.219514\n",
      "Cost after iteration 150: 0.211349\n",
      "Cost after iteration 160: 0.206933\n",
      "Cost after iteration 170: 0.205391\n",
      "Cost after iteration 180: 0.200637\n",
      "Cost after iteration 190: 0.195446\n",
      "current loc and des is 256 and  260  total cound is 350 percent is 0.7\n",
      "cur masked name is  pate_03  eval value is:  0.7\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693038\n",
      "Cost after iteration 10: 0.604523\n",
      "Cost after iteration 20: 0.364205\n",
      "Cost after iteration 30: 0.320792\n",
      "Cost after iteration 40: 0.287892\n",
      "Cost after iteration 50: 0.276022\n",
      "Cost after iteration 60: 0.260473\n",
      "Cost after iteration 70: 0.251845\n",
      "Cost after iteration 80: 0.245113\n",
      "Cost after iteration 90: 0.241068\n",
      "Cost after iteration 100: 0.234499\n",
      "Cost after iteration 110: 0.233254\n",
      "Cost after iteration 120: 0.225962\n",
      "Cost after iteration 130: 0.222185\n",
      "Cost after iteration 140: 0.223282\n",
      "Cost after iteration 150: 0.221724\n",
      "Cost after iteration 160: 0.210736\n",
      "Cost after iteration 170: 0.205923\n",
      "Cost after iteration 180: 0.204913\n",
      "Cost after iteration 190: 0.193045\n",
      "current loc and des is 260 and  267  total cound is 307 percent is 0.614\n",
      "cur masked name is  pate_04  eval value is:  0.614\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693082\n",
      "Cost after iteration 10: 0.606331\n",
      "Cost after iteration 20: 0.368423\n",
      "Cost after iteration 30: 0.324298\n",
      "Cost after iteration 40: 0.290500\n",
      "Cost after iteration 50: 0.281721\n",
      "Cost after iteration 60: 0.261162\n",
      "Cost after iteration 70: 0.253079\n",
      "Cost after iteration 80: 0.245362\n",
      "Cost after iteration 90: 0.243695\n",
      "Cost after iteration 100: 0.235964\n",
      "Cost after iteration 110: 0.234983\n",
      "Cost after iteration 120: 0.226316\n",
      "Cost after iteration 130: 0.224701\n",
      "Cost after iteration 140: 0.221995\n",
      "Cost after iteration 150: 0.216863\n",
      "Cost after iteration 160: 0.210558\n",
      "Cost after iteration 170: 0.205951\n",
      "Cost after iteration 180: 0.198170\n",
      "Cost after iteration 190: 0.195165\n",
      "current loc and des is 267 and  274  total cound is 94 percent is 0.188\n",
      "cur masked name is  pate_05  eval value is:  0.188\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693297\n",
      "Cost after iteration 10: 0.600763\n",
      "Cost after iteration 20: 0.366742\n",
      "Cost after iteration 30: 0.321227\n",
      "Cost after iteration 40: 0.287759\n",
      "Cost after iteration 50: 0.275029\n",
      "Cost after iteration 60: 0.260434\n",
      "Cost after iteration 70: 0.252154\n",
      "Cost after iteration 80: 0.245446\n",
      "Cost after iteration 90: 0.240823\n",
      "Cost after iteration 100: 0.233773\n",
      "Cost after iteration 110: 0.232052\n",
      "Cost after iteration 120: 0.227896\n",
      "Cost after iteration 130: 0.220193\n",
      "Cost after iteration 140: 0.214482\n",
      "Cost after iteration 150: 0.219053\n",
      "Cost after iteration 160: 0.205507\n",
      "Cost after iteration 170: 0.201259\n",
      "Cost after iteration 180: 0.197830\n",
      "Cost after iteration 190: 0.190140\n",
      "current loc and des is 274 and  280  total cound is 176 percent is 0.352\n",
      "cur masked name is  stress_01  eval value is:  0.352\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 10: 0.602379\n",
      "Cost after iteration 20: 0.366447\n",
      "Cost after iteration 30: 0.318851\n",
      "Cost after iteration 40: 0.285729\n",
      "Cost after iteration 50: 0.281740\n",
      "Cost after iteration 60: 0.260108\n",
      "Cost after iteration 70: 0.252315\n",
      "Cost after iteration 80: 0.244432\n",
      "Cost after iteration 90: 0.243589\n",
      "Cost after iteration 100: 0.235165\n",
      "Cost after iteration 110: 0.229579\n",
      "Cost after iteration 120: 0.224059\n",
      "Cost after iteration 130: 0.225406\n",
      "Cost after iteration 140: 0.220312\n",
      "Cost after iteration 150: 0.217616\n",
      "Cost after iteration 160: 0.204355\n",
      "Cost after iteration 170: 0.208463\n",
      "Cost after iteration 180: 0.197274\n",
      "Cost after iteration 190: 0.192505\n",
      "current loc and des is 280 and  286  total cound is 205 percent is 0.41\n",
      "cur masked name is  stress_02  eval value is:  0.41\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693432\n",
      "Cost after iteration 10: 0.604937\n",
      "Cost after iteration 20: 0.367456\n",
      "Cost after iteration 30: 0.321406\n",
      "Cost after iteration 40: 0.287767\n",
      "Cost after iteration 50: 0.278768\n",
      "Cost after iteration 60: 0.263036\n",
      "Cost after iteration 70: 0.255579\n",
      "Cost after iteration 80: 0.246682\n",
      "Cost after iteration 90: 0.248409\n",
      "Cost after iteration 100: 0.238032\n",
      "Cost after iteration 110: 0.237240\n",
      "Cost after iteration 120: 0.227500\n",
      "Cost after iteration 130: 0.222620\n",
      "Cost after iteration 140: 0.224425\n",
      "Cost after iteration 150: 0.216443\n",
      "Cost after iteration 160: 0.208452\n",
      "Cost after iteration 170: 0.201170\n",
      "Cost after iteration 180: 0.200778\n",
      "Cost after iteration 190: 0.194764\n",
      "current loc and des is 286 and  292  total cound is 222 percent is 0.444\n",
      "cur masked name is  stress_03  eval value is:  0.444\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693446\n",
      "Cost after iteration 10: 0.600895\n",
      "Cost after iteration 20: 0.365627\n",
      "Cost after iteration 30: 0.320852\n",
      "Cost after iteration 40: 0.287578\n",
      "Cost after iteration 50: 0.279768\n",
      "Cost after iteration 60: 0.261570\n",
      "Cost after iteration 70: 0.254193\n",
      "Cost after iteration 80: 0.247192\n",
      "Cost after iteration 90: 0.249806\n",
      "Cost after iteration 100: 0.236258\n",
      "Cost after iteration 110: 0.232175\n",
      "Cost after iteration 120: 0.227713\n",
      "Cost after iteration 130: 0.222848\n",
      "Cost after iteration 140: 0.223277\n",
      "Cost after iteration 150: 0.209681\n",
      "Cost after iteration 160: 0.212788\n",
      "Cost after iteration 170: 0.203796\n",
      "Cost after iteration 180: 0.202137\n",
      "Cost after iteration 190: 0.195240\n",
      "current loc and des is 292 and  298  total cound is 158 percent is 0.316\n",
      "cur masked name is  stress_04  eval value is:  0.316\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693081\n",
      "Cost after iteration 10: 0.601233\n",
      "Cost after iteration 20: 0.369645\n",
      "Cost after iteration 30: 0.322685\n",
      "Cost after iteration 40: 0.288623\n",
      "Cost after iteration 50: 0.281794\n",
      "Cost after iteration 60: 0.260722\n",
      "Cost after iteration 70: 0.252854\n",
      "Cost after iteration 80: 0.245909\n",
      "Cost after iteration 90: 0.240623\n",
      "Cost after iteration 100: 0.234816\n",
      "Cost after iteration 110: 0.230990\n",
      "Cost after iteration 120: 0.225079\n",
      "Cost after iteration 130: 0.220758\n",
      "Cost after iteration 140: 0.219024\n",
      "Cost after iteration 150: 0.214711\n",
      "Cost after iteration 160: 0.209296\n",
      "Cost after iteration 170: 0.202075\n",
      "Cost after iteration 180: 0.205044\n",
      "Cost after iteration 190: 0.192236\n",
      "current loc and des is 298 and  304  total cound is 151 percent is 0.302\n",
      "cur masked name is  nfc_01  eval value is:  0.302\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693280\n",
      "Cost after iteration 10: 0.606989\n",
      "Cost after iteration 20: 0.367321\n",
      "Cost after iteration 30: 0.322875\n",
      "Cost after iteration 40: 0.289408\n",
      "Cost after iteration 50: 0.275775\n",
      "Cost after iteration 60: 0.260223\n",
      "Cost after iteration 70: 0.254285\n",
      "Cost after iteration 80: 0.246313\n",
      "Cost after iteration 90: 0.249013\n",
      "Cost after iteration 100: 0.235885\n",
      "Cost after iteration 110: 0.235102\n",
      "Cost after iteration 120: 0.227778\n",
      "Cost after iteration 130: 0.220275\n",
      "Cost after iteration 140: 0.220977\n",
      "Cost after iteration 150: 0.221934\n",
      "Cost after iteration 160: 0.209706\n",
      "Cost after iteration 170: 0.204625\n",
      "Cost after iteration 180: 0.197269\n",
      "Cost after iteration 190: 0.193225\n",
      "current loc and des is 304 and  310  total cound is 228 percent is 0.456\n",
      "cur masked name is  nfc_02  eval value is:  0.456\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693950\n",
      "Cost after iteration 10: 0.601889\n",
      "Cost after iteration 20: 0.364801\n",
      "Cost after iteration 30: 0.319313\n",
      "Cost after iteration 40: 0.286534\n",
      "Cost after iteration 50: 0.272739\n",
      "Cost after iteration 60: 0.257825\n",
      "Cost after iteration 70: 0.250781\n",
      "Cost after iteration 80: 0.244833\n",
      "Cost after iteration 90: 0.248138\n",
      "Cost after iteration 100: 0.234861\n",
      "Cost after iteration 110: 0.237509\n",
      "Cost after iteration 120: 0.224681\n",
      "Cost after iteration 130: 0.220073\n",
      "Cost after iteration 140: 0.217037\n",
      "Cost after iteration 150: 0.214271\n",
      "Cost after iteration 160: 0.210558\n",
      "Cost after iteration 170: 0.208937\n",
      "Cost after iteration 180: 0.201866\n",
      "Cost after iteration 190: 0.194573\n",
      "current loc and des is 310 and  316  total cound is 193 percent is 0.386\n",
      "cur masked name is  nfc_03  eval value is:  0.386\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.692949\n",
      "Cost after iteration 10: 0.605789\n",
      "Cost after iteration 20: 0.366900\n",
      "Cost after iteration 30: 0.320939\n",
      "Cost after iteration 40: 0.288173\n",
      "Cost after iteration 50: 0.270423\n",
      "Cost after iteration 60: 0.261413\n",
      "Cost after iteration 70: 0.252049\n",
      "Cost after iteration 80: 0.248063\n",
      "Cost after iteration 90: 0.242571\n",
      "Cost after iteration 100: 0.239563\n",
      "Cost after iteration 110: 0.231880\n",
      "Cost after iteration 120: 0.225719\n",
      "Cost after iteration 130: 0.227932\n",
      "Cost after iteration 140: 0.221735\n",
      "Cost after iteration 150: 0.211265\n",
      "Cost after iteration 160: 0.207024\n",
      "Cost after iteration 170: 0.204231\n",
      "Cost after iteration 180: 0.203060\n",
      "Cost after iteration 190: 0.193581\n",
      "current loc and des is 316 and  322  total cound is 209 percent is 0.418\n",
      "cur masked name is  nfc_04  eval value is:  0.418\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693092\n",
      "Cost after iteration 10: 0.610098\n",
      "Cost after iteration 20: 0.366341\n",
      "Cost after iteration 30: 0.322091\n",
      "Cost after iteration 40: 0.288205\n",
      "Cost after iteration 50: 0.279876\n",
      "Cost after iteration 60: 0.262131\n",
      "Cost after iteration 70: 0.254666\n",
      "Cost after iteration 80: 0.246295\n",
      "Cost after iteration 90: 0.248487\n",
      "Cost after iteration 100: 0.236755\n",
      "Cost after iteration 110: 0.236691\n",
      "Cost after iteration 120: 0.227097\n",
      "Cost after iteration 130: 0.224606\n",
      "Cost after iteration 140: 0.222397\n",
      "Cost after iteration 150: 0.220299\n",
      "Cost after iteration 160: 0.208753\n",
      "Cost after iteration 170: 0.203119\n",
      "Cost after iteration 180: 0.197680\n",
      "Cost after iteration 190: 0.192140\n",
      "current loc and des is 322 and  328  total cound is 211 percent is 0.422\n",
      "cur masked name is  nfc_05  eval value is:  0.422\n",
      "n_x is 334 n_y is  334\n",
      "Cost after iteration 0: 0.693131\n",
      "Cost after iteration 10: 0.607371\n",
      "Cost after iteration 20: 0.366720\n",
      "Cost after iteration 30: 0.322656\n",
      "Cost after iteration 40: 0.289100\n",
      "Cost after iteration 50: 0.282949\n",
      "Cost after iteration 60: 0.261333\n",
      "Cost after iteration 70: 0.253162\n",
      "Cost after iteration 80: 0.246213\n",
      "Cost after iteration 90: 0.248837\n",
      "Cost after iteration 100: 0.236988\n",
      "Cost after iteration 110: 0.237951\n",
      "Cost after iteration 120: 0.226648\n",
      "Cost after iteration 130: 0.223588\n",
      "Cost after iteration 140: 0.221391\n",
      "Cost after iteration 150: 0.222243\n",
      "Cost after iteration 160: 0.208244\n",
      "Cost after iteration 170: 0.206430\n",
      "Cost after iteration 180: 0.204050\n",
      "Cost after iteration 190: 0.194238\n",
      "current loc and des is 328 and  334  total cound is 185 percent is 0.37\n",
      "cur masked name is  nfc_06  eval value is:  0.37\n"
     ]
    }
   ],
   "source": [
    "di = evaluate(x,y,og_pred,test,2000,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Question in Analyze the Data\n",
    "# for Computation consuming reason,\n",
    "# only run Neural network 70 epochs, our Model did much better job with higher epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['mood_02,','intrinsic_01','intrinsic_02','intrinsic_03','intrinsic_04'\n",
    "    ,'intrinsic_08','intrinsic_10','intrinsic_13','intrinsic_15','mcfiller1'\n",
    "     ,'mcfiller2','mcfiller3','mcdv1','pate_03','pate_04','stress_02','stress_02','stress_03'\n",
    "     ,'nfc_02','nfc_04','nfc_05'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current masked key is intrinsic_01  and other key is age\n",
      "cur masked loc 0 cur des  4\n",
      "n_x is 374 n_y is  374\n",
      "Cost after iteration 0: 0.693207\n",
      "Cost after iteration 10: 0.540423\n",
      "Cost after iteration 20: 0.332753\n",
      "Cost after iteration 30: 0.290126\n",
      "Cost after iteration 40: 0.259258\n",
      "Cost after iteration 50: 0.246185\n",
      "Cost after iteration 60: 0.238312\n",
      "current loc and des is 131 and  136  total cound is 417 percent is 0.4633333333333333\n",
      "curmasked key is intrinsic_01 cur_iter_key is age percent is 0.4633333333333333\n",
      "current masked key is intrinsic_01  and other key is mood_01\n",
      "cur masked loc 4 cur des  12\n",
      "n_x is 374 n_y is  374\n",
      "Cost after iteration 0: 0.693195\n"
     ]
    }
   ],
   "source": [
    "final = Analyzethedata(x,y,og_pred,a,test,dic_look,2000,70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create testing attribut arra\n",
    "arr = []\n",
    "og_arr = []\n",
    "og_train = {} #init global dictionary\n",
    "og_pred = {}\n",
    "arr.append(['age',1])\n",
    "arr.append(['mood_01',2]) #Today I generally feel\n",
    "arr.append(['mood_02',2])\n",
    "arr.append(['major',0])\n",
    "arr.append(['big5_01',2]) #I see myself as: Extraverted, enthusiastic.\n",
    "arr.append(['big5_02',2]) #I see myself as: Critical, quarrelsome.\n",
    "arr.append(['big5_03',2]) #I see myself as: Dependable, self-disciplined.\n",
    "arr.append(['big5_04',2]) #I see myself as: Anxious, easily upset.\n",
    "arr.append(['big5_05',2]) #I see myself as: Open to new experiences, complex.\n",
    "arr.append(['big5_06',2]) #I see myself as: Reserved, quiet.\n",
    "arr.append(['big5_07',2]) #I see myself as: Sympathetic, warm.\n",
    "arr.append(['big5_08',2]) #I see myself as: Disorganized, careless.\n",
    "arr.append(['big5_09',2]) #I see myself as: Calm, emotionally stable.\n",
    "arr.append(['big5_10',2]) #I see myself as: Conventional, uncreative.\n",
    "arr.append(['highpower',0])\n",
    "arr.append(['intrinsic_01',2])\n",
    "arr.append(['intrinsic_02',2])\n",
    "arr.append(['intrinsic_03',2])\n",
    "arr.append(['intrinsic_04',2])\n",
    "arr.append(['intrinsic_05',2])\n",
    "arr.append(['intrinsic_06',2])\n",
    "arr.append(['intrinsic_07',2])\n",
    "arr.append(['intrinsic_08',2])\n",
    "arr.append(['intrinsic_09',2])\n",
    "arr.append(['intrinsic_10',2])\n",
    "arr.append(['intrinsic_11',2])\n",
    "arr.append(['intrinsic_12',2])\n",
    "arr.append(['intrinsic_13',2])\n",
    "arr.append(['intrinsic_14',2])\n",
    "arr.append(['intrinsic_15',2])\n",
    "arr.append(['lowpower',0])\n",
    "arr.append(['mcfiller1',2])\n",
    "arr.append(['mcfiller2',2])\n",
    "arr.append(['mcfiller3',2])\n",
    "arr.append(['mcmost1',2])\n",
    "arr.append(['mcmost2',2])\n",
    "arr.append(['mcmost3',2])\n",
    "arr.append(['mcmost4',2])\n",
    "arr.append(['mcmost5',2])\n",
    "arr.append(['mcsome1',2])\n",
    "arr.append(['mcsome2',2])\n",
    "arr.append(['mcsome3',2])\n",
    "arr.append(['mcsome4',2])\n",
    "arr.append(['mcsome5',2])\n",
    "arr.append(['mcdv1',2])\n",
    "arr.append(['mcdv2',2])\n",
    "arr.append(['pate_01',2])\n",
    "arr.append(['pate_02',2])\n",
    "arr.append(['pate_03',2])\n",
    "arr.append(['pate_04',2])\n",
    "arr.append(['pate_05',2])\n",
    "arr.append(['stress_01',2])\n",
    "arr.append(['stress_02',2])\n",
    "arr.append(['stress_03',2])\n",
    "arr.append(['stress_04',2])\n",
    "arr.append(['nfc_01',2])\n",
    "arr.append(['nfc_02',2])\n",
    "arr.append(['nfc_03',2])\n",
    "arr.append(['nfc_04',2])\n",
    "arr.append(['nfc_05',2])\n",
    "arr.append(['nfc_06',2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer question in bonus 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create testing attribut arra\n",
    "arr = []\n",
    "og_arr = []\n",
    "og_train = {} #init global dictionary\n",
    "og_pred = {}\n",
    "arr.append(['age',1])\n",
    "arr.append(['mood_01',2]) #Today I generally feel\n",
    "arr.append(['mood_02',2])\n",
    "arr.append(['major',0])\n",
    "arr.append(['big5_01',2]) #I see myself as: Extraverted, enthusiastic.\n",
    "arr.append(['big5_02',2]) #I see myself as: Critical, quarrelsome.\n",
    "arr.append(['big5_03',2]) #I see myself as: Dependable, self-disciplined.\n",
    "arr.append(['big5_04',2]) #I see myself as: Anxious, easily upset.\n",
    "arr.append(['big5_05',2]) #I see myself as: Open to new experiences, complex.\n",
    "arr.append(['big5_06',2]) #I see myself as: Reserved, quiet.\n",
    "arr.append(['big5_07',2]) #I see myself as: Sympathetic, warm.\n",
    "arr.append(['big5_08',2]) #I see myself as: Disorganized, careless.\n",
    "arr.append(['big5_09',2]) #I see myself as: Calm, emotionally stable.\n",
    "arr.append(['big5_10',2]) #I see myself as: Conventional, uncreative.\n",
    "arr.append(['highpower',0])\n",
    "arr.append(['intrinsic_01',2])\n",
    "arr.append(['intrinsic_02',2])\n",
    "arr.append(['intrinsic_03',2])\n",
    "arr.append(['intrinsic_04',2])\n",
    "arr.append(['intrinsic_05',2])\n",
    "arr.append(['intrinsic_06',2])\n",
    "arr.append(['intrinsic_07',2])\n",
    "arr.append(['intrinsic_08',2])\n",
    "arr.append(['intrinsic_09',2])\n",
    "arr.append(['intrinsic_10',2])\n",
    "arr.append(['intrinsic_11',2])\n",
    "arr.append(['intrinsic_12',2])\n",
    "arr.append(['intrinsic_13',2])\n",
    "arr.append(['intrinsic_14',2])\n",
    "arr.append(['intrinsic_15',2])\n",
    "arr.append(['lowpower',0])\n",
    "arr.append(['mcfiller1',2])\n",
    "arr.append(['mcfiller2',2])\n",
    "arr.append(['mcfiller3',2])\n",
    "arr.append(['mcmost1',2])\n",
    "arr.append(['mcmost2',2])\n",
    "arr.append(['mcmost3',2])\n",
    "arr.append(['mcmost4',2])\n",
    "arr.append(['mcmost5',2])\n",
    "arr.append(['mcsome1',2])\n",
    "arr.append(['mcsome2',2])\n",
    "arr.append(['mcsome3',2])\n",
    "arr.append(['mcsome4',2])\n",
    "arr.append(['mcsome5',2])\n",
    "arr.append(['mcdv1',2])\n",
    "arr.append(['mcdv2',2])\n",
    "arr.append(['pate_01',2])\n",
    "arr.append(['pate_02',2])\n",
    "arr.append(['pate_03',2])\n",
    "arr.append(['pate_04',2])\n",
    "arr.append(['pate_05',2])\n",
    "arr.append(['stress_01',2])\n",
    "arr.append(['stress_02',2])\n",
    "arr.append(['stress_03',2])\n",
    "arr.append(['stress_04',2])\n",
    "arr.append(['nfc_01',2])\n",
    "arr.append(['nfc_02',2])\n",
    "arr.append(['nfc_03',2])\n",
    "arr.append(['nfc_04',2])\n",
    "arr.append(['nfc_05',2])\n",
    "arr.append(['nfc_06',2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur key is age\n",
      "cur key is mood_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 12)\n",
      "cur key is mood_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 20)\n",
      "cur key is major\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n",
      "/usr/lib/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "class_arr of nlp [0, 2, 4, 6, 7, 9, 12, 13, 17, 20, 23, 27, 29]\n",
      "(2998, 34)\n",
      "cur key is big5_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 42)\n",
      "cur key is big5_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 50)\n",
      "cur key is big5_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 58)\n",
      "cur key is big5_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 66)\n",
      "cur key is big5_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 74)\n",
      "cur key is big5_06\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 82)\n",
      "cur key is big5_07\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 90)\n",
      "cur key is big5_08\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 98)\n",
      "cur key is big5_09\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 106)\n",
      "cur key is big5_10\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 114)\n",
      "cur key is highpower\n",
      "nan\n",
      "class_arr of nlp [0, 2, 3, 4, 5, 7, 10, 13, 14, 17, 18, 20, 23, 25, 26, 29]\n",
      "(2998, 131)\n",
      "cur key is intrinsic_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 136)\n",
      "cur key is intrinsic_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 141)\n",
      "cur key is intrinsic_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 54.0]\n",
      "(2998, 147)\n",
      "cur key is intrinsic_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 77.0]\n",
      "(2998, 153)\n",
      "cur key is intrinsic_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 159)\n",
      "cur key is intrinsic_06\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 164)\n",
      "cur key is intrinsic_07\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 170)\n",
      "cur key is intrinsic_08\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 176)\n",
      "cur key is intrinsic_09\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 181)\n",
      "cur key is intrinsic_10\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 186)\n",
      "cur key is intrinsic_11\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 192)\n",
      "cur key is intrinsic_12\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 197)\n",
      "cur key is intrinsic_13\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 202)\n",
      "cur key is intrinsic_14\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 208)\n",
      "cur key is intrinsic_15\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 213)\n",
      "cur key is lowpower\n",
      "nan\n",
      "class_arr of nlp [0, 6, 9, 11, 19, 24, 27, 28]\n",
      "(2998, 222)\n",
      "cur key is mcfiller1\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 227)\n",
      "cur key is mcfiller2\n",
      "class_arr [0.0, 1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 233)\n",
      "cur key is mcfiller3\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 238)\n",
      "cur key is mcmost1\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 241)\n",
      "cur key is mcmost2\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 244)\n",
      "cur key is mcmost3\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 247)\n",
      "cur key is mcmost4\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 250)\n",
      "cur key is mcmost5\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 253)\n",
      "cur key is mcsome1\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 256)\n",
      "cur key is mcsome2\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 259)\n",
      "cur key is mcsome3\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 262)\n",
      "cur key is mcsome4\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 265)\n",
      "cur key is mcsome5\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 268)\n",
      "cur key is mcdv1\n",
      "class_arr [-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0]\n",
      "(2998, 276)\n",
      "cur key is mcdv2\n",
      "class_arr [-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0]\n",
      "(2998, 284)\n",
      "cur key is pate_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 290)\n",
      "cur key is pate_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 296)\n",
      "cur key is pate_03\n",
      "class_arr [1.0, 2.0, 3.0]\n",
      "(2998, 300)\n",
      "cur key is pate_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "(2998, 307)\n",
      "cur key is pate_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "(2998, 314)\n",
      "cur key is stress_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 320)\n",
      "cur key is stress_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 326)\n",
      "cur key is stress_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 332)\n",
      "cur key is stress_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 338)\n",
      "cur key is nfc_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 344)\n",
      "cur key is nfc_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 350)\n",
      "cur key is nfc_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 356)\n",
      "cur key is nfc_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 362)\n",
      "cur key is nfc_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 368)\n",
      "cur key is nfc_06\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 374)\n"
     ]
    }
   ],
   "source": [
    "# predict_df = df3.iloc[:100]\n",
    "trainning_df = df3\n",
    "# trainning_df = trainning_df.reset_index(drop = True)\n",
    "\n",
    "# X_train = semi_auto_append_attri(arr,predict_df,og)\n",
    "X_train = new_semi_auto_append_attri(arr,trainning_df,og_pred)\n",
    "# print('start training dataset')\n",
    "# X_tra = semi_auto_append_attri(arr,trainning_df,og_train)\n",
    "\n",
    "X_train_new = X_train[:2400]\n",
    "#slice nparr\n",
    "x_real_train = X_train[:2400]\n",
    "x_pred = X_train[2400:]\n",
    "test = createlist_list_string(og_pred)\n",
    "x,y = enlarge_dataset(X_train_new,X_train_new,og_pred,test)\n",
    "dic_look = create_dictionary_for_every_location_of_feature(og_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find location of where lowpoer sits in the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[213, 222]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_look.get('lowpower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(2000,200,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_x is 374 n_y is  374\n",
      "Cost after iteration 0: 0.693431\n",
      "Cost after iteration 10: 0.524898\n",
      "Cost after iteration 20: 0.365324\n",
      "Cost after iteration 30: 0.306432\n",
      "Cost after iteration 40: 0.275571\n",
      "Cost after iteration 50: 0.259901\n",
      "Cost after iteration 60: 0.246456\n",
      "Cost after iteration 70: 0.238691\n",
      "Cost after iteration 80: 0.234972\n",
      "Cost after iteration 90: 0.232786\n",
      "Cost after iteration 100: 0.224932\n",
      "Cost after iteration 110: 0.219633\n",
      "Cost after iteration 120: 0.216549\n",
      "Cost after iteration 130: 0.211898\n",
      "Cost after iteration 140: 0.210094\n",
      "Cost after iteration 150: 0.202698\n",
      "Cost after iteration 160: 0.203223\n",
      "Cost after iteration 170: 0.199677\n",
      "Cost after iteration 180: 0.193286\n",
      "Cost after iteration 190: 0.187340\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train_new,X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = nn.predict(x_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recover Ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_classification_based_on_prob(ans,og_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## len(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_unanswered = []\n",
    "for i in range(len(x_pred)):\n",
    "    if x_pred[i][221] == 0:\n",
    "        dic_unanswered.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 28,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 45,\n",
       " 50,\n",
       " 53,\n",
       " 54,\n",
       " 57,\n",
       " 58,\n",
       " 63,\n",
       " 65,\n",
       " 67,\n",
       " 68,\n",
       " 72,\n",
       " 73,\n",
       " 75,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 83,\n",
       " 84,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 101,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 112,\n",
       " 114,\n",
       " 115,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 125,\n",
       " 126,\n",
       " 128,\n",
       " 129,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 144,\n",
       " 145,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 151,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 159,\n",
       " 160,\n",
       " 162,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 168,\n",
       " 170,\n",
       " 171,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 182,\n",
       " 184,\n",
       " 185,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 197,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 204,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 212,\n",
       " 216,\n",
       " 218,\n",
       " 221,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 255,\n",
       " 256,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 266,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 272,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 285,\n",
       " 287,\n",
       " 288,\n",
       " 292,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 299,\n",
       " 300,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 311,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 328,\n",
       " 331,\n",
       " 334,\n",
       " 336,\n",
       " 337,\n",
       " 340,\n",
       " 342,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 351,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 359,\n",
       " 360,\n",
       " 362,\n",
       " 367,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 376,\n",
       " 378,\n",
       " 380,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 401,\n",
       " 404,\n",
       " 405,\n",
       " 412,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 436,\n",
       " 438,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 444,\n",
       " 446,\n",
       " 447,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 457,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 470,\n",
       " 471,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 488,\n",
       " 492,\n",
       " 493,\n",
       " 495,\n",
       " 496,\n",
       " 498,\n",
       " 500,\n",
       " 501,\n",
       " 503,\n",
       " 506,\n",
       " 507,\n",
       " 510,\n",
       " 512,\n",
       " 514,\n",
       " 515,\n",
       " 519,\n",
       " 520,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 531,\n",
       " 533,\n",
       " 534,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 544,\n",
       " 549,\n",
       " 556,\n",
       " 557,\n",
       " 559,\n",
       " 561,\n",
       " 562,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 568,\n",
       " 572,\n",
       " 573,\n",
       " 575,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 588,\n",
       " 589,\n",
       " 594,\n",
       " 597]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_unanswered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have interpolate the unanswed feature of 'lowpower'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0][213:221]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 6: 1, 9: 2, 11: 3, 19: 4, 24: 5, 27: 6, 28: 7}"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_pred.get('lowpower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find out the clustered answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "class_arr of nlp [0, 6, 9, 11, 19, 24, 27, 28]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n",
      "/usr/lib/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    }
   ],
   "source": [
    "di = {}\n",
    "Ans,lab = Bonus_GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df3,'lowpower',di,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  28,   42,  100,  125,  188,  195,  223,  271,  275,  279,  334,\n",
       "         370,  372,  413,  419,  484,  529,  590,  641,  781,  849,  852,\n",
       "         854,  907,  944,  959,  965,  974,  981,  986, 1039, 1041, 1136,\n",
       "        1142, 1147, 1165, 1172, 1176, 1178, 1180, 1226, 1230, 1251, 1364,\n",
       "        1365, 1373, 1394, 1421, 1430, 1442, 1450, 1550, 1560, 1572, 1587,\n",
       "        1592, 1629, 1651, 1660, 1673, 1677, 1706, 1725, 1787, 1790, 1806,\n",
       "        1807, 1825, 1826, 1840, 1892, 1960, 1969, 1972, 1973, 1978, 2002,\n",
       "        2005, 2008, 2025, 2040, 2061, 2075, 2078, 2093, 2113, 2147, 2170,\n",
       "        2185, 2192, 2195, 2201, 2211, 2214, 2216, 2312, 2323, 2340, 2342,\n",
       "        2352, 2408, 2410, 2417, 2444, 2447, 2451, 2482, 2513, 2522, 2532,\n",
       "        2596, 2605, 2646, 2665, 2667, 2726, 2781, 2848, 2855, 2869, 2909,\n",
       "        2921, 2960, 2978, 2991, 2993]),)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(lab == 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choose one answer which clusterd with the same section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally we use neural network to interpoloate an unanswed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I used to work at Applebee's. This is a corporate business so even though we have many managers, they are controlled by the corporate office. I do not agree with the way that things were supposed to be done (according to corporate) and it was very frustrating. They are very strict and have a lot of power not only over my managers, but my fellow employees and myself too.\""
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['lowpower'][28]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part two assess the quality of these prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur key is age\n",
      "cur key is mood_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 12)\n",
      "cur key is mood_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 20)\n",
      "cur key is major\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:79: RuntimeWarning: Mean of empty slice.\n",
      "/usr/lib/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "class_arr of nlp [0, 2, 4, 6, 7, 9, 12, 13, 17, 20, 23, 27, 29]\n",
      "(2998, 34)\n",
      "cur key is big5_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 42)\n",
      "cur key is big5_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 50)\n",
      "cur key is big5_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 58)\n",
      "cur key is big5_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 66)\n",
      "cur key is big5_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 74)\n",
      "cur key is big5_06\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 82)\n",
      "cur key is big5_07\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 90)\n",
      "cur key is big5_08\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 98)\n",
      "cur key is big5_09\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 106)\n",
      "cur key is big5_10\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 114)\n",
      "cur key is highpower\n",
      "nan\n",
      "class_arr of nlp [0, 2, 3, 4, 5, 7, 10, 13, 14, 17, 18, 20, 23, 25, 26, 29]\n",
      "(2998, 131)\n",
      "cur key is intrinsic_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 136)\n",
      "cur key is intrinsic_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 141)\n",
      "cur key is intrinsic_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 54.0]\n",
      "(2998, 147)\n",
      "cur key is intrinsic_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 77.0]\n",
      "(2998, 153)\n",
      "cur key is intrinsic_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 159)\n",
      "cur key is intrinsic_06\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 164)\n",
      "cur key is intrinsic_07\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 170)\n",
      "cur key is intrinsic_08\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 176)\n",
      "cur key is intrinsic_09\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 181)\n",
      "cur key is intrinsic_10\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 186)\n",
      "cur key is intrinsic_11\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 192)\n",
      "cur key is intrinsic_12\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 197)\n",
      "cur key is intrinsic_13\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 202)\n",
      "cur key is intrinsic_14\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 208)\n",
      "cur key is intrinsic_15\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 213)\n",
      "cur key is lowpower\n",
      "nan\n",
      "class_arr of nlp [0, 6, 9, 11, 19, 24, 27, 28]\n",
      "(2998, 222)\n",
      "cur key is mcfiller1\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 227)\n",
      "cur key is mcfiller2\n",
      "class_arr [0.0, 1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 233)\n",
      "cur key is mcfiller3\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 238)\n",
      "cur key is mcmost1\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 241)\n",
      "cur key is mcmost2\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 244)\n",
      "cur key is mcmost3\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 247)\n",
      "cur key is mcmost4\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 250)\n",
      "cur key is mcmost5\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 253)\n",
      "cur key is mcsome1\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 256)\n",
      "cur key is mcsome2\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 259)\n",
      "cur key is mcsome3\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 262)\n",
      "cur key is mcsome4\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 265)\n",
      "cur key is mcsome5\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 268)\n",
      "cur key is mcdv1\n",
      "class_arr [-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0]\n",
      "(2998, 276)\n",
      "cur key is mcdv2\n",
      "class_arr [-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0]\n",
      "(2998, 284)\n",
      "cur key is pate_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 290)\n",
      "cur key is pate_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 296)\n",
      "cur key is pate_03\n",
      "class_arr [1.0, 2.0, 3.0]\n",
      "(2998, 300)\n",
      "cur key is pate_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "(2998, 307)\n",
      "cur key is pate_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "(2998, 314)\n",
      "cur key is stress_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 320)\n",
      "cur key is stress_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 326)\n",
      "cur key is stress_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 332)\n",
      "cur key is stress_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 338)\n",
      "cur key is nfc_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 344)\n",
      "cur key is nfc_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 350)\n",
      "cur key is nfc_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 356)\n",
      "cur key is nfc_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 362)\n",
      "cur key is nfc_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 368)\n",
      "cur key is nfc_06\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 374)\n"
     ]
    }
   ],
   "source": [
    "#create testing attribut arra\n",
    "arr = []\n",
    "og_arr = []\n",
    "og_train = {} #init global dictionary\n",
    "og_pred = {}\n",
    "arr.append(['age',1])\n",
    "arr.append(['mood_01',2]) #Today I generally feel\n",
    "arr.append(['mood_02',2])\n",
    "arr.append(['major',0])\n",
    "arr.append(['big5_01',2]) #I see myself as: Extraverted, enthusiastic.\n",
    "arr.append(['big5_02',2]) #I see myself as: Critical, quarrelsome.\n",
    "arr.append(['big5_03',2]) #I see myself as: Dependable, self-disciplined.\n",
    "arr.append(['big5_04',2]) #I see myself as: Anxious, easily upset.\n",
    "arr.append(['big5_05',2]) #I see myself as: Open to new experiences, complex.\n",
    "arr.append(['big5_06',2]) #I see myself as: Reserved, quiet.\n",
    "arr.append(['big5_07',2]) #I see myself as: Sympathetic, warm.\n",
    "arr.append(['big5_08',2]) #I see myself as: Disorganized, careless.\n",
    "arr.append(['big5_09',2]) #I see myself as: Calm, emotionally stable.\n",
    "arr.append(['big5_10',2]) #I see myself as: Conventional, uncreative.\n",
    "arr.append(['highpower',0])\n",
    "arr.append(['intrinsic_01',2])\n",
    "arr.append(['intrinsic_02',2])\n",
    "arr.append(['intrinsic_03',2])\n",
    "arr.append(['intrinsic_04',2])\n",
    "arr.append(['intrinsic_05',2])\n",
    "arr.append(['intrinsic_06',2])\n",
    "arr.append(['intrinsic_07',2])\n",
    "arr.append(['intrinsic_08',2])\n",
    "arr.append(['intrinsic_09',2])\n",
    "arr.append(['intrinsic_10',2])\n",
    "arr.append(['intrinsic_11',2])\n",
    "arr.append(['intrinsic_12',2])\n",
    "arr.append(['intrinsic_13',2])\n",
    "arr.append(['intrinsic_14',2])\n",
    "arr.append(['intrinsic_15',2])\n",
    "arr.append(['lowpower',0])\n",
    "arr.append(['mcfiller1',2])\n",
    "arr.append(['mcfiller2',2])\n",
    "arr.append(['mcfiller3',2])\n",
    "arr.append(['mcmost1',2])\n",
    "arr.append(['mcmost2',2])\n",
    "arr.append(['mcmost3',2])\n",
    "arr.append(['mcmost4',2])\n",
    "arr.append(['mcmost5',2])\n",
    "arr.append(['mcsome1',2])\n",
    "arr.append(['mcsome2',2])\n",
    "arr.append(['mcsome3',2])\n",
    "arr.append(['mcsome4',2])\n",
    "arr.append(['mcsome5',2])\n",
    "arr.append(['mcdv1',2])\n",
    "arr.append(['mcdv2',2])\n",
    "arr.append(['pate_01',2])\n",
    "arr.append(['pate_02',2])\n",
    "arr.append(['pate_03',2])\n",
    "arr.append(['pate_04',2])\n",
    "arr.append(['pate_05',2])\n",
    "arr.append(['stress_01',2])\n",
    "arr.append(['stress_02',2])\n",
    "arr.append(['stress_03',2])\n",
    "arr.append(['stress_04',2])\n",
    "arr.append(['nfc_01',2])\n",
    "arr.append(['nfc_02',2])\n",
    "arr.append(['nfc_03',2])\n",
    "arr.append(['nfc_04',2])\n",
    "arr.append(['nfc_05',2])\n",
    "arr.append(['nfc_06',2])\n",
    "# predict_df = df3.iloc[:100]\n",
    "trainning_df = df3\n",
    "# trainning_df = trainning_df.reset_index(drop = True)\n",
    "\n",
    "# X_train = semi_auto_append_attri(arr,predict_df,og)\n",
    "X_train = new_semi_auto_append_attri(arr,trainning_df,og_pred)\n",
    "# print('start training dataset')\n",
    "# X_tra = semi_auto_append_attri(arr,trainning_df,og_train)\n",
    "\n",
    "X_train_new = X_train[:2400]\n",
    "#slice nparr\n",
    "x_real_train = X_train[:2400]\n",
    "x_pred = X_train[2400:]\n",
    "test = createlist_list_string(og_pred)\n",
    "x,y = enlarge_dataset(X_train_new,X_train_new,og_pred,test)\n",
    "dic_look = create_dictionary_for_every_location_of_feature(og_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(2000,1000,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_x is 374 n_y is  374\n",
      "Cost after iteration 0: 0.693403\n",
      "Cost after iteration 10: 0.450383\n",
      "Cost after iteration 20: 0.344276\n",
      "Cost after iteration 30: 0.310549\n",
      "Cost after iteration 40: 0.272484\n",
      "Cost after iteration 50: 0.253822\n",
      "Cost after iteration 60: 0.245454\n",
      "Cost after iteration 70: 0.236306\n",
      "Cost after iteration 80: 0.233161\n",
      "Cost after iteration 90: 0.228167\n",
      "Cost after iteration 100: 0.228479\n",
      "Cost after iteration 110: 0.215981\n",
      "Cost after iteration 120: 0.210314\n",
      "Cost after iteration 130: 0.205959\n",
      "Cost after iteration 140: 0.197639\n",
      "Cost after iteration 150: 0.195877\n",
      "Cost after iteration 160: 0.191094\n",
      "Cost after iteration 170: 0.187702\n",
      "Cost after iteration 180: 0.178968\n",
      "Cost after iteration 190: 0.178878\n",
      "Cost after iteration 200: 0.172602\n",
      "Cost after iteration 210: 0.165447\n",
      "Cost after iteration 220: 0.161357\n",
      "Cost after iteration 230: 0.156524\n",
      "Cost after iteration 240: 0.154319\n",
      "Cost after iteration 250: 0.148069\n",
      "Cost after iteration 260: 0.143200\n",
      "Cost after iteration 270: 0.139292\n",
      "Cost after iteration 280: 0.140870\n",
      "Cost after iteration 290: 0.135486\n",
      "Cost after iteration 300: 0.126120\n",
      "Cost after iteration 310: 0.125945\n",
      "Cost after iteration 320: 0.127772\n",
      "Cost after iteration 330: 0.119645\n",
      "Cost after iteration 340: 0.115118\n",
      "Cost after iteration 350: 0.114582\n",
      "Cost after iteration 360: 0.110141\n",
      "Cost after iteration 370: 0.106569\n",
      "Cost after iteration 380: 0.105973\n",
      "Cost after iteration 390: 0.099652\n",
      "Cost after iteration 400: 0.101324\n",
      "Cost after iteration 410: 0.096235\n",
      "Cost after iteration 420: 0.093117\n",
      "Cost after iteration 430: 0.089921\n",
      "Cost after iteration 440: 0.089484\n",
      "Cost after iteration 450: 0.085549\n",
      "Cost after iteration 460: 0.083890\n",
      "Cost after iteration 470: 0.081653\n",
      "Cost after iteration 480: 0.079931\n",
      "Cost after iteration 490: 0.078230\n",
      "Cost after iteration 500: 0.075864\n",
      "Cost after iteration 510: 0.074706\n",
      "Cost after iteration 520: 0.073394\n",
      "Cost after iteration 530: 0.070743\n",
      "Cost after iteration 540: 0.069280\n",
      "Cost after iteration 550: 0.068324\n",
      "Cost after iteration 560: 0.067024\n",
      "Cost after iteration 570: 0.065689\n",
      "Cost after iteration 580: 0.064171\n",
      "Cost after iteration 590: 0.063200\n",
      "Cost after iteration 600: 0.062032\n",
      "Cost after iteration 610: 0.061242\n",
      "Cost after iteration 620: 0.060416\n",
      "Cost after iteration 630: 0.059015\n",
      "Cost after iteration 640: 0.058348\n",
      "Cost after iteration 650: 0.057178\n",
      "Cost after iteration 660: 0.056468\n",
      "Cost after iteration 670: 0.055559\n",
      "Cost after iteration 680: 0.054466\n",
      "Cost after iteration 690: 0.053754\n",
      "Cost after iteration 700: 0.052940\n",
      "Cost after iteration 710: 0.052336\n",
      "Cost after iteration 720: 0.051408\n",
      "Cost after iteration 730: 0.050753\n",
      "Cost after iteration 740: 0.050144\n",
      "Cost after iteration 750: 0.049432\n",
      "Cost after iteration 760: 0.048441\n",
      "Cost after iteration 770: 0.047878\n",
      "Cost after iteration 780: 0.047191\n",
      "Cost after iteration 790: 0.046786\n",
      "Cost after iteration 800: 0.046032\n",
      "Cost after iteration 810: 0.045251\n",
      "Cost after iteration 820: 0.044831\n",
      "Cost after iteration 830: 0.044316\n",
      "Cost after iteration 840: 0.043882\n",
      "Cost after iteration 850: 0.043160\n",
      "Cost after iteration 860: 0.042679\n",
      "Cost after iteration 870: 0.041833\n",
      "Cost after iteration 880: 0.041950\n",
      "Cost after iteration 890: 0.041244\n",
      "Cost after iteration 900: 0.040605\n",
      "Cost after iteration 910: 0.039835\n",
      "Cost after iteration 920: 0.039998\n",
      "Cost after iteration 930: 0.039261\n",
      "Cost after iteration 940: 0.038555\n",
      "Cost after iteration 950: 0.038169\n",
      "Cost after iteration 960: 0.037856\n",
      "Cost after iteration 970: 0.037465\n",
      "Cost after iteration 980: 0.037061\n",
      "Cost after iteration 990: 0.036453\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train_new,X_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find location of where one hot encoding array represents lowpower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[213, 222]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_look.get('lowpower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the index of respondent who does answer the Natural language feature 'lowpower' question in the testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexlist = []\n",
    "for i in range(len(x_pred)):\n",
    "    if x_pred[i][221] == 1:\n",
    "        indexlist.append(i)\n",
    "len(indexlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We Mask their answer manually and utilize neural network to interpolate these answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred_masked = np.copy(x_pred)\n",
    "for i in range(len(indexlist)):\n",
    "    pointer = indexlist[i]\n",
    "    for j in range(213,222):\n",
    "        x_pred_masked[pointer][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = nn.predict(x_pred_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_classification_based_on_prob(ans,og_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count how many masked respond could be interpolated correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Num of correctly interpolated NLP answer is 51 \n",
      "  percentage of count/total 0.20318725099601595\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(indexlist)):\n",
    "    pointer =  indexlist[i]\n",
    "    if np.array_equal(ans[pointer][213:221],x_pred[pointer][213:221]):\n",
    "        count+=1\n",
    "        \n",
    "print('  Num of correctly interpolated NLP answer is', count,'\\n',' percentage of count/total', count/len(indexlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO : CREATE FUNCTION TO RECOVER CLASSIFICATION DATASET INTO HUMAN-Fridendly DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_answer(predict_arr,dic):\n",
    "    reverse_dic = create_reverse_dictionary(dic)\n",
    "    #init an dic as final anser array\n",
    "    ans_arr = []\n",
    "    #create array with all dict keys based on insertion order\n",
    "    dic_arr = list(dic.keys())\n",
    "    #recover predicted candidate sequentilaay\n",
    "    for i in range(len(predict_arr)):\n",
    "        tmp_ans_array = {} # cur dict to store anser for this candidate\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        #loop through dictionary array to recover each attribute column\n",
    "        for j in range(len(dic_arr)):\n",
    "            cur_dict_key = dic_arr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_reverse_dic_arr = reverse_dic.get(cur_dict_key)\n",
    "            cur_dict_length = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_length #update current cutting position\n",
    "            #restore\n",
    "            tmp_cutting_arr = predict_arr[i][cur_loc:cur_des]\n",
    "            tag = tmp_cutting_arr[-1]\n",
    "            if tag == 0:\n",
    "                tmp_ans_array[cur_dict_key] = 'NA'\n",
    "            else:\n",
    "                #find the position where tag is 1\n",
    "                loc = 0\n",
    "                for pos in range(len(tmp_cutting_arr)):\n",
    "                    if tmp_cutting_arr[pos] == 1:\n",
    "                        loc = pos\n",
    "                        break\n",
    "                #now based on location, create cover anser\n",
    "                #case one loc is 0\n",
    "                if loc == 0:\n",
    "                    ans =\"<\" + str(cur_reverse_dic_arr.get(loc))\n",
    "                    tmp_ans_array[cur_dict_key] = ans\n",
    "                else:\n",
    "                    last_loc = loc-1\n",
    "                    ans=\"\"\n",
    "                    ans+=str(cur_reverse_dic_arr.get(last_loc))+\" < \"\n",
    "                    ans+=str(cur_reverse_dic_arr.get(loc))\n",
    "                    tmp_ans_array[cur_dict_key] = ans\n",
    "#                 ans =cur_reverse_dic_arr.get(loc)\n",
    "#                 tmp_ans_array[cur_dict_key] = ans\n",
    "        \n",
    "            cur_loc+=cur_dict_length\n",
    "        #append current tmp candidate array to finaly answer\n",
    "        ans_arr.append(tmp_ans_array)\n",
    "    \n",
    "    return ans_arr\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE FUNCTION FOR CROSS VALIDATION\n",
    "Assess the accuracy by count how many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(arr_all_candidate,num_fold,og_pred,size_hidden_layer,iter_num,learning_rate):\n",
    "    cost = 0\n",
    "    #create num_fold equal size fold from input nparray\n",
    "    list_of_fold = np.array_split(arr_all_candidate,num_fold)\n",
    "    \n",
    "    #need to append training dataset together, while leave testing set alone\n",
    "    for i in range(num_fold):\n",
    "        x_pred = list_of_fold[i]\n",
    "        \n",
    "        tmp = 0\n",
    "        for j in range(0,num_fold):\n",
    "            if j == i:\n",
    "                continue\n",
    "            else:\n",
    "                \n",
    "                if tmp == 0:\n",
    "                    ans = np.asarray(list_of_fold[j])\n",
    "                    tmp+=1\n",
    "                \n",
    "                elif tmp > 0:\n",
    "                    \n",
    "                    ans = cro_new_append(ans,list_of_fold[j])\n",
    "\n",
    "            \n",
    "        #finished rebuilding training dataset\n",
    "        #start training neural network\n",
    "        nn = NeuralNetwork(size_hidden_layer,iter_num,learning_rate) #size of input layer\n",
    "        \n",
    "        nn.fit(ans,ans)\n",
    "        predict_ans = nn.predict(x_pred)\n",
    "        find_classification_based_on_distance(predict_ans,og_pred)\n",
    "        #coun\n",
    "        tmpcost = cost_function(predict_ans,x_pred)\n",
    "        cost+=tmpcost\n",
    "        print('#### current cutting position is : ', i, 'current cost is: ',tmpcost)\n",
    "    print('final avg cost is : ', cost/num_fold)\n",
    "    return cost/num_fold\n",
    "\n",
    "\n",
    "\n",
    "def cost_function(x,y):\n",
    "    #compute the average num of wrongly predicted column as cost function\n",
    "    total_num = len(x)\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        tmpcount = 0\n",
    "        for j in range(len(x[i])):\n",
    "            if x[i][j]!=y[i][j]:\n",
    "                tmpcount+=1\n",
    "        count+=tmpcount\n",
    "    avg_count = count/total_num\n",
    "    return avg_count\n",
    "def cross_valid_append_arr(old,new):\n",
    "    ans = np.append([old],[new],axis = 0)\n",
    "    return ans\n",
    "\n",
    "def cro_new_append(old,new):\n",
    "    #num_col\n",
    "    num_col = len(old[0])\n",
    "    new_ans = []\n",
    "    for i in range(len(old)):\n",
    "        new_ans.append(old[i])\n",
    "    for i in range(len(new)):\n",
    "        new_ans.append(new[i])\n",
    "    return np.asarray(new_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
