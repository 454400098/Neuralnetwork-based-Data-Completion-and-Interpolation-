{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO MACHINE LEARNING LIBARY USED FOR TRAINING/REGRESSION/CLASSFICATION\n",
    "# sklean.feature_extraction ONLY USED for language data into Vector transformation\n",
    "# preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer#ONLY USED FOR TRANSFORM FROM LANGUAGE TO VECTOR\n",
    "from numpy import linalg as LA\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (17,55,59,61,65,68,69,70,83,90,91,92,93,120,121,122,123,126,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./Tab.delimited.Cleaned.dataset.WITH.variable.labels.csv', sep='\\t',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,11,12,19,20,129,132,169,230) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.read_csv('./ML3AllSites.csv', sep=',',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, n_h, n_iterate=10, learning_rate=1):\n",
    "        self.n_x = None  # size of the input layer\n",
    "        self.n_h = n_h  # size of the hidden layer\n",
    "        self.n_y = None # size of the output layer\n",
    "        self.W1 = None\n",
    "        self.W2 = None\n",
    "        self.b1 = None\n",
    "        self.b2 = None\n",
    "        self.A1 = None\n",
    "        self.A2 = None  # sigmoid output of the second activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterate = n_iterate\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.W1 = np.random.randn(self.n_h, self.n_x) * 0.01\n",
    "        self.b1 = np.zeros((self.n_h, 1))\n",
    "        self.W2 = np.random.randn(self.n_y, self.n_h) * 0.01\n",
    "        self.b2 = np.zeros((self.n_y, 1))\n",
    "\n",
    "    def relu(self, z):\n",
    "        return z * (z > 0)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def compute_cost(self, Y):\n",
    "        #cost = np.linalg.norm(self.A2 - Y)\n",
    "        cost = - (Y * np.log(self.A2) + (1-Y) * np.log(1-self.A2)).mean()\n",
    "        return np.squeeze(cost) \n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        self.A2 = self.sigmoid(self.W2 @ self.A1 + self.b2)\n",
    "\n",
    "    def backward_propagation(self, X, Y):\n",
    "        m = X.shape[1]\n",
    "\n",
    "        dZ2 = self.A2 - Y\n",
    "        dW2 = dZ2 @ self.A1.T / m\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "        dZ1 = self.W2.T @ dZ2 * (self.A1 > 0)\n",
    "        dW1 = dZ1 @ X.T / m\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        X, Y = X.T, Y.T\n",
    "        self.n_x = X.shape[0]\n",
    "        self.n_y = Y.shape[0]\n",
    "        self.initialize_parameters()\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(0, self.n_iterate):\n",
    "            self.forward_propagation(X)\n",
    "            self.backward_propagation(X, Y)\n",
    "            if i % 1000 == 0:\n",
    "                cost = self.compute_cost(Y)\n",
    "                self.learning_rate = 5 * cost\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.T\n",
    "        A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        A2 = self.sigmoid(self.W2 @ A1 + self.b2)\n",
    "        return A2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to perform kmeans clustering for each attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=64):\n",
    "        self.n_clusters = n_clusters  # number of clusters\n",
    "        self.centers = None  # to record the centers\n",
    "        self.labels = None\n",
    "        self.Y = None\n",
    "\n",
    "    def random_center(self,Y): #only for 1d dataset\n",
    "    # randomly generate n_cluster clusters in the raange of X\n",
    "        self.centers = np.random.rand(self.n_clusters, len(Y))\n",
    "        for i in range(self.n_clusters):\n",
    "            self.centers[i] = Y[i]\n",
    "            \n",
    "            \n",
    "    def random_center2(self,Y):\n",
    "    # randomly generate n_cluster clusters in the raange of X\n",
    "        self.centers = np.random.rand(self.n_clusters, len(Y[0]))\n",
    "        for i in range(self.n_clusters):\n",
    "            self.centers[i] = Y[i]\n",
    "#         for j in range(0):\n",
    "#             Y_j_min = self.Y[:,j].min()\n",
    "#             Y_j_max = self.Y[:,j].max()\n",
    "#             self.centers[:,j] = Y_j_min + (Y_j_max - Y_j_min) * self.centers[:,j]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def dist(self, point1, point2): #old one\n",
    "        return 2*(point1[0]-point2[0])**2 + 4*(point1[1]-point2[1])**2 + 3*(point1[2]-point2[2])**2\n",
    "\n",
    "    def dist2(self,point1,point2):\n",
    "        return LA.norm(point1-point2)**2\n",
    "    \n",
    "    def fit(self, Y):\n",
    "        self.Y = Y\n",
    "        self.labels = np.zeros(Y.shape[0], dtype='uint8')  # record the current labels of each sample of X\n",
    "        self.random_center(Y)\n",
    "        diff = 1\n",
    "        \n",
    "        while diff > 1e-3:\n",
    "            old_center = self.centers.copy()\n",
    "\n",
    "            # go through all samples and label them using the nearest label\n",
    "            for i in range(Y.shape[0]):\n",
    "                distance = np.zeros(self.n_clusters)\n",
    "                for j in range(self.n_clusters):\n",
    "                    distance[j] = self.dist2(Y[i], self.centers[j])\n",
    "                self.labels[i] = np.argmin(distance)\n",
    "                \n",
    "                \n",
    "            # update the centers\n",
    "            for i in range(self.n_clusters):\n",
    "                self.centers[i] = Y[self.labels==i].mean(axis=0)\n",
    "                \n",
    "\n",
    "            # update the difference\n",
    "            diff = np.linalg.norm(self.centers - old_center)\n",
    "            print(diff)\n",
    "        return self\n",
    "    \n",
    "    def fit2(self, Y):\n",
    "        self.Y = Y\n",
    "        self.labels = np.zeros(Y.shape[0], dtype='uint8')  # record the current labels of each sample of X\n",
    "        self.random_center2(Y)\n",
    "        diff = 1\n",
    "        \n",
    "        while diff > 1e-3:\n",
    "            old_center = self.centers.copy()\n",
    "\n",
    "            # go through all samples and label them using the nearest label\n",
    "            for i in range(Y.shape[0]):\n",
    "                distance = np.zeros(self.n_clusters)\n",
    "                for j in range(self.n_clusters):\n",
    "                    distance[j] = self.dist2(Y[i], self.centers[j])\n",
    "                self.labels[i] = np.argmin(distance)\n",
    "                \n",
    "                \n",
    "            # update the centers\n",
    "            for i in range(self.n_clusters):\n",
    "                self.centers[i] = Y[self.labels==i].mean(axis=0)\n",
    "                \n",
    "\n",
    "            # update the difference\n",
    "            diff = np.linalg.norm(self.centers - old_center)\n",
    "            print(diff)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, Y):\n",
    "        out = np.zeros(Y.shape)\n",
    "        for i in range(self.n_clusters):\n",
    "            out[self.labels==i] = self.centers[i]\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oneHOTENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncoding(Y, kmeans):\n",
    "    out = np.zeros((len(Y), kmeans.n_clusters))\n",
    "    for i in range(len(Y)):\n",
    "        out[i, Y[i]] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test clustering numbered data:\n",
    "    step1 : get single column\n",
    "    step2 : sort in increasing order\n",
    "    step3 : based on distribution, cut data into 4 section with equally number of candidate\n",
    "    step4 : if data is null, set isnon to be true for that column\n",
    "    step5 : append new datacol to input_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isint(value):\n",
    "  try:\n",
    "    int(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "def isfloat(value):\n",
    "  try:\n",
    "    isfloat(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "def getdistriarr(df,arr_str):\n",
    "    arr = df[arr_str].values\n",
    "    di = []\n",
    "    di_og = []  #RETURN OG SINCE di will be sort\n",
    "    for i in range(len(arr)):\n",
    "        if isint(arr[i]) == True:\n",
    "            di.append(int(arr[i]))\n",
    "            di_og.append(int(arr[i]))\n",
    "        else:\n",
    "            di.append(-1)\n",
    "            di_og.append(-1)\n",
    "    di.sort()\n",
    "    ans_arr = []\n",
    "    leng = len(di)\n",
    "    ans_arr.append(di[int(leng/3)])\n",
    "#     ans_arr.append(di[int(leng/2)])\n",
    "    ans_arr.append(di[int(leng*0.66)])\n",
    "    ans_arr.append(di[int(leng-1)])\n",
    "    return di_og,ans_arr\n",
    "\n",
    "def oneHotEncoding_return_arr(att_arr,class_arr): #one \n",
    "    #add one more column for each row since last col used as flag\n",
    "    choice = len(class_arr)\n",
    "    out = np.zeros((len(att_arr),len(class_arr)+1))\n",
    "    for i in range(len(att_arr)):\n",
    "        loc = 0\n",
    "        #first jude if data is null\n",
    "        if att_arr[i] == -1:\n",
    "            out[i][-1] = 0\n",
    "            continue\n",
    "        else:\n",
    "            out[i][-1] = 1\n",
    "            for j in range(len(class_arr)):\n",
    "                if att_arr[i] < class_arr[j]:\n",
    "                    out[i][j] = 1\n",
    "                    break\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "def GET_ONE_HOT_ARRAY_ONLY_DIGIT_COLUMN(df,attr_name,og_dic):\n",
    "    att_arr,class_arr = getdistriarr(df,attr_name)\n",
    "    dic_ans = create_dic_for_classification(class_arr)\n",
    "    append_arr_of_dic_to_overall(og_dic,dic_ans,attr_name)\n",
    "\n",
    "    return oneHotEncoding_return_arr(att_arr,class_arr)\n",
    "\n",
    "def create_dic_for_classification(class_arr):\n",
    "    cur_dic = {}\n",
    "    total_len = len(class_arr)\n",
    "    for i in range(len(class_arr)):\n",
    "        cur_dic[class_arr[i]] = i\n",
    "    return cur_dic\n",
    "\n",
    "def append_arr_of_dic_to_overall(og,dic,att_name):\n",
    "    og[att_name] = dic\n",
    "    \n",
    "    \n",
    "def append_arr(old,new):\n",
    "    #both old and new has the same num of row\n",
    "    #create a new nparray\n",
    "    \n",
    "    newdim = len(old[0])+len(new[0])\n",
    "    ans = np.zeros((len(old),newdim))\n",
    "    print(ans.shape)\n",
    "    for i in range(len(old)):\n",
    "        ans[i] = np.append(old[i],new[i])\n",
    "    return ans\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP TWO CONVERT NLP DATASET INTO VECTOR:\n",
    "    STEP1: vectorize dataset\n",
    "    STEP2: apply kmeans clustering to language data\n",
    "    STEP3: ONEHOT ENCODING\n",
    "    STEP4: APPEND CLASSIFICATION DATASET INTO old CLASSFICAITION ARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLPDATAPRO(allsentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(allsentences)\n",
    "    ans = X.toarray()\n",
    "    return ans\n",
    "\n",
    "def createtextarr_ONEHOT_ENCODING(attri_col): #ONEHOT_ENCODING\n",
    "    df2 = df[attri_col]\n",
    "    arr = df2.values\n",
    "    for i in range(len(arr)):\n",
    "        di.append(arr[i])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP THREE CONVERT TEXT_DATASET WITH FIXED classification into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processnull(arr):\n",
    "    arrnew = []\n",
    "    for i in range(len(arr)):\n",
    "        if type(arr[i]) == float and math.isnan(arr[i]) :\n",
    "            arrnew.append('Nan')\n",
    "        else:\n",
    "            arrnew.append(arr[i])\n",
    "    return arrnew\n",
    "\n",
    "def returnuniqiearr(og):\n",
    "    ar = np.asarray(og)\n",
    "    return np.unique(ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: create a function to append text to old vecto(remember to append flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: TEST NN TONIGHT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onehoted_text_fixed_classification(attribu_name,df,km_num):\n",
    "    df_cur = df[attribu_name]\n",
    "    attribute_arr = df_cur.values\n",
    "    arr_no_null = processnull(attribute_arr) #create new arr with format fit by NLP(CountVectorizer)\n",
    "    NLP_ARR = NLPDATAPRO(arr_no_null)\n",
    "    #BEFORE ONEHOT_ENCODING\n",
    "    #APPLY KMEANS CLUSTERING SINCE TOO MANY DIFFERENT CLASSIFICATION\n",
    "    KM_TMP = KMeans(km_num)\n",
    "    KM_TMP.fit2(NLP_ARR)\n",
    "    labels_arr = KM_TMP.labels\n",
    "    return labels_arr\n",
    "    \n",
    "def onehot_enco(labelarr,df,att_name):\n",
    "    og_text_arr = df[att_name].values\n",
    "    #create dic to remeber loc for each value in the uniqe arr\n",
    "    uni_arr = np.unique(labelarr)\n",
    "    loc_dic = {}\n",
    "    for i in range(len(uni_arr)):\n",
    "        loc_dic[uni_arr[i]] = i\n",
    "    #based on the lenth of dic,create onehot_encod\n",
    "    choice = len(loc_dic)\n",
    "    out = np.zeros((len(labelarr),choice+1))\n",
    "    for i in range(len(labelarr)):\n",
    "        #first junde if data is null\n",
    "        if type(og_text_arr[i])!=str and math.isnan(og_text_arr[i]) == True:\n",
    "            out[i][-1] = 0\n",
    "            continue\n",
    "        else:\n",
    "            out[i][-1] = 1\n",
    "            cur_loc = loc_dic.get(labelarr[i])\n",
    "            out[i][cur_loc] = 1\n",
    "            \n",
    "    return out,loc_dic\n",
    "\n",
    "def GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,attr_name,og_dic,km_num):\n",
    "    labels_arr = create_onehoted_text_fixed_classification(attr_name,df,km_num)\n",
    "    out_arr,loc_dic = onehot_enco(labels_arr,df,attr_name)\n",
    "    #append dic to global array of dic\n",
    "    append_arr_of_dic_to_overall(og_dic,loc_dic,attr_name)\n",
    "    return out_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL WORK:\n",
    "    1. MAINTAIN AN ARRAY OF DICTIONARY S.T EVERY DIC\n",
    "       CONTAINS MAPPING FROM CLASSIFICAITON TO ACTUAL INDEX AFTER KMEANS\n",
    "    \n",
    "    2. AFTER TRAINING, WE WILL UTILIZED THE ARR_DIC TO RECOVER CANDIDATE ANSER\n",
    "    \n",
    "    3. TODO: REWRITE COST FUNCTION OF NERUAL NETWORK\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINDING:\n",
    "    AFTER combine several attributes column, num of candidate with dinct\n",
    "    ans grows extremly fast.\n",
    "    Need to apply Kmeans clustering again to the array of candidate before\n",
    "    threw into Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE A SCRIPT TO APPEND ASSIGNED ATTRIBUTE TO INPUT COLUMN\n",
    "\n",
    "(API)INSTRUCTION:<br>\n",
    "USER WHO WANTS TO CREATE PREPROCESSED DATASET ONLY NEED TO CREATE\n",
    "ARRAY[i]<br>\n",
    "       ARRAY[i][0] = 'name of attribute'<br>\n",
    "       ARRAY[i][1] = 1 : it is a digit column<br>\n",
    "       ARRAY[i][1] = 0 : it is a NLP column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_auto_append_attri(arr_of_attribute,df,og_dic):\n",
    "    for i in range(len(arr_of_attribute)):\n",
    "        name = arr_of_attribute[i][0]\n",
    "        isdigit = arr_of_attribute[i][1]\n",
    "        if isdigit == 1:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_DIGIT_COLUMN(df,name,og_dic)\n",
    "        else:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,name,og_dic,20)\n",
    "        #append old with new\n",
    "        if i == 0:\n",
    "            old = attr_arr\n",
    "        else:\n",
    "            old = append_arr(old,attr_arr)\n",
    "#             print(old.shape)\n",
    "    return old\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Kmeans Clustering to the overal column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_whole_attribute_get_Y_for_Neural_NetWork(X,num_cluster):\n",
    "    km = KMeans(num_cluster)\n",
    "    km.fit2(X)\n",
    "    \n",
    "    #APPLY ONEHOT_ENCODING TO NEURAL_NETWORKAGAIN\n",
    "    \n",
    "    return oneHotEncoding(km.labels,km),km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLY NEURAL NETWORK TO SOLVE THIS QUESTION:\n",
    "\n",
    "REMEBER TO CONSIDER FLAG FOR EACH COLUMN\n",
    "\n",
    "# QUESTION: SHOULD I APPLY ONE HOT ENCODING TO EVERY ATTRIBUT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predicted_data(pred,kmeans):\n",
    "    ans = []\n",
    "    for i in range(len(pred)):\n",
    "        ans.append(kmeans.centers[np.argmax(pred[i])])\n",
    "    newarr = np.asarray(ans)\n",
    "    return newarr\n",
    "\n",
    "def find_classification_based_on_distance(pred,dic):\n",
    "    #create array with all dict keys based on insertion order\n",
    "    dicarr = list(dic.keys())\n",
    "    #isolate target awated_array used to classification\n",
    "    for i in range(len(pred)):\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "#             print('cur dic: ',cur_dict_key)\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_lengh = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_lengh  #update current cutting position\n",
    "            #restore dimension of onehotencoding for current dictionary\n",
    "            one_hot_cur_dic = np.zeros((cur_dict_lengh,cur_dict_lengh))\n",
    "#             print('before any processing cur loc is ', cur_loc, ' cur des is : ', cur_des)\n",
    "            for q in range(0,cur_dict_lengh-1):\n",
    "                one_hot_cur_dic[q][q] = 1\n",
    "                one_hot_cur_dic[q][-1] = 1\n",
    "            #isolate target awaited_classification array\n",
    "            cur_isolated_arr = pred[i][cur_loc:cur_des]\n",
    "            #compute classification with smallest distance\n",
    "            arr_store_distance= []\n",
    "            for h in range (len(one_hot_cur_dic)):\n",
    "                arr_store_distance.append(np.linalg.norm(cur_isolated_arr-one_hot_cur_dic[h]))\n",
    "            #transfer dic to arr s.t unilized argmim\n",
    "            dis_np_array = np.asarray(arr_store_distance)\n",
    "            index = np.argmin(dis_np_array)\n",
    "#             if cur_dict_key == 'big5_10':\n",
    "#                 print(index)\n",
    "#                 print('cur onehot_array is ', one_hot_cur_dic[3] )\n",
    "#                 print('cur loc is ', cur_loc, ' cur des is : ', cur_des)\n",
    "            # change value in output array\n",
    "            for k in range (cur_dict_lengh):\n",
    "                pred[i][cur_loc+k] = one_hot_cur_dic[index][k]\n",
    "                \n",
    "            cur_loc += cur_dict_lengh #update cur_loc not used in curretn loop\n",
    "\n",
    "def create_reverse_dictionary(dic):\n",
    "    reverse = {}\n",
    "    dic_arr = list(dic.keys())\n",
    "    for i in range(len(dic_arr)):\n",
    "        cur_reverse_dict = {}\n",
    "        cur_dic_key = dic_arr[i]\n",
    "        cur_key_arr = list(dic.get(cur_dic_key).keys())\n",
    "        cur_index_arr = list(dic.get(cur_dic_key).values())\n",
    "        #reverse key_value pair in originaly dictionary\n",
    "        for j in range(len(cur_key_arr)):\n",
    "            cur_reverse_dict[cur_index_arr[j]] = cur_key_arr[j]\n",
    "        #append current dic to ans dic\n",
    "        reverse[cur_dic_key] = cur_reverse_dict\n",
    "    return reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create testing attribut arra\n",
    "arr = []\n",
    "og_arr = []\n",
    "og_train = {} #init global dictionary\n",
    "og_pred = {}\n",
    "# arr.append(['age',1])\n",
    "# arr.append(['mood_01',1]) #Today I generally feel\n",
    "# arr.append(['major',0])\n",
    "# arr.append(['big5_01',1]) #I see myself as: Extraverted, enthusiastic.\n",
    "# arr.append(['big5_02',1]) #I see myself as: Critical, quarrelsome.\n",
    "# arr.append(['big5_03',1]) #I see myself as: Dependable, self-disciplined.\n",
    "# arr.append(['big5_04',1]) #I see myself as: Anxious, easily upset.\n",
    "# arr.append(['big5_05',1]) #I see myself as: Open to new experiences, complex.\n",
    "# arr.append(['big5_06',1]) #I see myself as: Reserved, quiet.\n",
    "# arr.append(['big5_07',1]) #I see myself as: Sympathetic, warm.\n",
    "# arr.append(['big5_08',1]) #I see myself as: Disorganized, careless.\n",
    "# arr.append(['big5_09',1]) #I see myself as: Calm, emotionally stable.\n",
    "# arr.append(['big5_10',1]) #I see myself as: Conventional, uncreative.\n",
    "arr.append(['intrinsic_01',1])\n",
    "arr.append(['intrinsic_02',1])\n",
    "arr.append(['intrinsic_03',1])\n",
    "arr.append(['intrinsic_04',1])\n",
    "arr.append(['intrinsic_05',1])\n",
    "arr.append(['intrinsic_06',1])\n",
    "arr.append(['intrinsic_07',1])\n",
    "arr.append(['intrinsic_08',1])\n",
    "arr.append(['intrinsic_09',1])\n",
    "arr.append(['intrinsic_10',1])\n",
    "arr.append(['intrinsic_11',1])\n",
    "arr.append(['intrinsic_12',1])\n",
    "arr.append(['intrinsic_13',1])\n",
    "arr.append(['intrinsic_14',1])\n",
    "arr.append(['intrinsic_15',1])\n",
    "arr.append(['mcfiller1',1])\n",
    "arr.append(['mcfiller2',1])\n",
    "arr.append(['mcfiller3',1])\n",
    "arr.append(['mcmost1',1])\n",
    "arr.append(['mcmost2',1])\n",
    "arr.append(['mcmost3',1])\n",
    "arr.append(['mcmost4',1])\n",
    "arr.append(['mcmost5',1])\n",
    "arr.append(['mcsome1',1])\n",
    "arr.append(['mcsome2',1])\n",
    "arr.append(['mcsome3',1])\n",
    "arr.append(['mcsome4',1])\n",
    "arr.append(['mcsome5',1])\n",
    "arr.append(['mood_01',1])\n",
    "arr.append(['mood_02',1])\n",
    "arr.append(['pate_01',1])\n",
    "arr.append(['pate_02',1])\n",
    "arr.append(['pate_03',1])\n",
    "arr.append(['pate_04',1])\n",
    "arr.append(['pate_05',1])\n",
    "arr.append(['stress_01',1])\n",
    "arr.append(['stress_02',1])\n",
    "arr.append(['stress_03',1])\n",
    "arr.append(['stress_04',1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_df = df3.iloc[:100]\n",
    "trainning_df = df3\n",
    "# trainning_df = trainning_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2998, 8)\n",
      "(2998, 12)\n",
      "(2998, 16)\n",
      "(2998, 20)\n",
      "(2998, 24)\n",
      "(2998, 28)\n",
      "(2998, 32)\n",
      "(2998, 36)\n",
      "(2998, 40)\n",
      "(2998, 44)\n",
      "(2998, 48)\n",
      "(2998, 52)\n",
      "(2998, 56)\n",
      "(2998, 60)\n",
      "(2998, 64)\n",
      "(2998, 68)\n",
      "(2998, 72)\n",
      "(2998, 76)\n",
      "(2998, 80)\n",
      "(2998, 84)\n",
      "(2998, 88)\n",
      "(2998, 92)\n",
      "(2998, 96)\n",
      "(2998, 100)\n",
      "(2998, 104)\n",
      "(2998, 108)\n",
      "(2998, 112)\n",
      "(2998, 116)\n",
      "(2998, 120)\n",
      "(2998, 124)\n",
      "(2998, 128)\n",
      "(2998, 132)\n",
      "(2998, 136)\n",
      "(2998, 140)\n",
      "(2998, 144)\n",
      "(2998, 148)\n",
      "(2998, 152)\n",
      "(2998, 156)\n"
     ]
    }
   ],
   "source": [
    "# X_train = semi_auto_append_attri(arr,predict_df,og)\n",
    "X_train = semi_auto_append_attri(arr,trainning_df,og_pred)\n",
    "# print('start training dataset')\n",
    "# X_tra = semi_auto_append_attri(arr,trainning_df,og_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = X_train[:2900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice nparr\n",
    "x_pred = X_train[2000:]\n",
    "x_real_train = X_train[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(50,2000,0.1) #size of input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693364\n",
      "Cost after iteration 1000: 0.242710\n"
     ]
    }
   ],
   "source": [
    "nn.fit(x_real_train,x_real_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = nn.predict(x_real_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.78931662e-07, 6.27177440e-07, 3.36431586e-09, 3.55241808e-11,\n",
       "       5.76263540e-07, 6.27923373e-07, 8.02761892e-09, 3.34815095e-11,\n",
       "       6.30768129e-07, 2.48011237e-07, 1.02548450e-08, 3.55867181e-11,\n",
       "       3.91865294e-07, 6.26629804e-07, 4.91504088e-09, 3.99301138e-11,\n",
       "       1.29912377e-06, 2.23170925e-07, 1.70943286e-08, 3.99553203e-11,\n",
       "       1.10309129e-06, 9.59220388e-08, 1.49668270e-07, 3.76894783e-11,\n",
       "       8.45057906e-07, 8.84504945e-08, 6.60132048e-08, 3.77225209e-11,\n",
       "       5.29264353e-07, 6.29607916e-07, 2.75435560e-09, 3.77139103e-11,\n",
       "       1.61669688e-06, 2.33960572e-07, 9.02661471e-08, 3.77228310e-11,\n",
       "       6.53435167e-07, 1.58140198e-07, 6.26329420e-07, 3.99388537e-11,\n",
       "       1.61557911e-06, 3.43696928e-07, 7.83150413e-09, 3.99284768e-11,\n",
       "       6.55424282e-07, 9.98459475e-08, 1.75446605e-07, 4.21979833e-11,\n",
       "       1.86598023e-06, 3.59881385e-07, 6.90166639e-08, 3.99256093e-11,\n",
       "       8.76957533e-07, 2.37966365e-07, 2.14141950e-08, 4.70354027e-11,\n",
       "       7.90324987e-07, 1.58017173e-07, 6.23592338e-07, 3.77291076e-11,\n",
       "       6.25597563e-07, 3.84075684e-08, 8.22928137e-08, 4.46837246e-13,\n",
       "       6.25276251e-07, 6.28554686e-07, 5.07645748e-12, 6.37979926e-13,\n",
       "       6.25586904e-07, 6.25792952e-07, 5.56408952e-11, 2.53496041e-11,\n",
       "       6.26404499e-07, 6.29303298e-04, 6.30399347e-07, 1.14084889e-03,\n",
       "       6.27307065e-07, 6.24945355e-07, 6.15373243e-04, 1.14000126e-03,\n",
       "       6.28543082e-07, 8.86323068e-07, 6.26734955e-07, 1.13971804e-03,\n",
       "       6.30716548e-07, 6.28498769e-07, 5.88758160e-07, 1.14000126e-03,\n",
       "       6.26524229e-07, 2.09439044e-06, 6.20613708e-07, 1.13943448e-03,\n",
       "       6.28740063e-07, 6.28269055e-07, 2.04111329e-07, 4.71580938e-08,\n",
       "       6.26732385e-07, 6.30542456e-07, 4.66616478e-07, 4.66120611e-08,\n",
       "       6.34608880e-07, 6.32153900e-07, 3.58515264e-07, 4.61006780e-08,\n",
       "       6.29490218e-07, 6.28681176e-07, 1.68798268e-07, 4.66412239e-08,\n",
       "       6.27830391e-07, 6.31082165e-07, 2.82857123e-07, 4.70785907e-08,\n",
       "       1.01526173e-06, 1.52547308e-07, 3.05729966e-08, 1.50891069e-11,\n",
       "       7.13519471e-07, 1.26028089e-07, 5.04757130e-08, 1.50899339e-11,\n",
       "       1.38894881e-06, 1.18120772e-07, 9.01769871e-08, 2.24061104e-11,\n",
       "       1.78060562e-06, 2.73521826e-07, 5.19865402e-08, 1.92838210e-11,\n",
       "       6.24863844e-07, 6.27269550e-07, 4.20094910e-11, 2.08277992e-11,\n",
       "       6.29362658e-07, 6.23884548e-07, 1.78340658e-10, 2.24233278e-11,\n",
       "       4.11064909e-07, 1.99892203e-07, 7.57105338e-08, 2.57960824e-11,\n",
       "       9.47730883e-07, 2.65559484e-07, 2.74346123e-08, 2.40790590e-11,\n",
       "       6.50256939e-07, 1.84066937e-07, 1.01059325e-07, 2.40962451e-11,\n",
       "       4.05706658e-07, 6.24245187e-07, 9.00484555e-09, 2.76295512e-11,\n",
       "       8.30821725e-07, 2.24822654e-07, 4.29883072e-08, 2.58388821e-11])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_classification_based_on_distance(ans,og_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 1., 1.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_real_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECOVERED_ANS = recover_answer(ans,og_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intrinsic_01': 'NA',\n",
       " 'intrinsic_02': 'NA',\n",
       " 'intrinsic_03': '1 < 2',\n",
       " 'intrinsic_04': 'None < 3',\n",
       " 'intrinsic_05': '3 < 5',\n",
       " 'intrinsic_06': '3 < 4',\n",
       " 'intrinsic_07': '3 < 7',\n",
       " 'intrinsic_08': 'None < 3',\n",
       " 'intrinsic_09': '3 < 4',\n",
       " 'intrinsic_10': 'NA',\n",
       " 'intrinsic_11': 'NA',\n",
       " 'intrinsic_12': '<2',\n",
       " 'intrinsic_13': '<2',\n",
       " 'intrinsic_14': '<2',\n",
       " 'intrinsic_15': '<3',\n",
       " 'mcfiller1': '1 < 2',\n",
       " 'mcfiller2': 'NA',\n",
       " 'mcfiller3': '<None',\n",
       " 'mcmost1': 'NA',\n",
       " 'mcmost2': '<-1',\n",
       " 'mcmost3': '<-1',\n",
       " 'mcmost4': 'NA',\n",
       " 'mcmost5': 'NA',\n",
       " 'mcsome1': 'NA',\n",
       " 'mcsome2': 'NA',\n",
       " 'mcsome3': 'NA',\n",
       " 'mcsome4': 'NA',\n",
       " 'mcsome5': 'NA',\n",
       " 'mood_01': 'NA',\n",
       " 'mood_02': 'NA',\n",
       " 'pate_01': 'NA',\n",
       " 'pate_02': '4 < 5',\n",
       " 'pate_03': 'None < 1',\n",
       " 'pate_04': 'NA',\n",
       " 'pate_05': '<2',\n",
       " 'stress_01': '<2',\n",
       " 'stress_02': '<3',\n",
       " 'stress_03': '<None',\n",
       " 'stress_04': '2 < 3'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RECOVERED_ANS[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO : CREATE FUNCTION TO RECOVER CLASSIFICATION DATASET INTO HUMAN-Fridendly DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_answer(predict_arr,dic):\n",
    "    reverse_dic = create_reverse_dictionary(dic)\n",
    "    #init an dic as final anser array\n",
    "    ans_arr = []\n",
    "    #create array with all dict keys based on insertion order\n",
    "    dic_arr = list(dic.keys())\n",
    "    #recover predicted candidate sequentilaay\n",
    "    for i in range(len(predict_arr)):\n",
    "        tmp_ans_array = {} # cur dict to store anser for this candidate\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        #loop through dictionary array to recover each attribute column\n",
    "        for j in range(len(dic_arr)):\n",
    "            cur_dict_key = dic_arr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_reverse_dic_arr = reverse_dic.get(cur_dict_key)\n",
    "            cur_dict_length = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_length #update current cutting position\n",
    "            #restore\n",
    "            tmp_cutting_arr = predict_arr[i][cur_loc:cur_des]\n",
    "            tag = tmp_cutting_arr[-1]\n",
    "            if tag == 0:\n",
    "                tmp_ans_array[cur_dict_key] = 'NA'\n",
    "            else:\n",
    "                #find the position where tag is 1\n",
    "                loc = 0\n",
    "                for pos in range(len(tmp_cutting_arr)):\n",
    "                    if tmp_cutting_arr[pos] == 1:\n",
    "                        loc = pos\n",
    "                        break\n",
    "                #now based on location, create cover anser\n",
    "                #case one loc is 0\n",
    "                if loc == 0:\n",
    "                    ans =\"<\" + str(cur_reverse_dic_arr.get(loc))\n",
    "                    tmp_ans_array[cur_dict_key] = ans\n",
    "                else:\n",
    "                    last_loc = loc-1\n",
    "                    ans=\"\"\n",
    "                    ans+=str(cur_reverse_dic_arr.get(last_loc))+\" < \"\n",
    "                    ans+=str(cur_reverse_dic_arr.get(loc))\n",
    "                    tmp_ans_array[cur_dict_key] = ans\n",
    "#                 ans =cur_reverse_dic_arr.get(loc)\n",
    "#                 tmp_ans_array[cur_dict_key] = ans\n",
    "        \n",
    "            cur_loc+=cur_dict_length\n",
    "        #append current tmp candidate array to finaly answer\n",
    "        ans_arr.append(tmp_ans_array)\n",
    "    \n",
    "    return ans_arr\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUT OF 198 \"untrained\" ATTRIBUTE BY NEURAL NETWORK\", THERE ARE 34 Candidate be classied into right section without error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 34\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "for index in range(len(x_pred)):\n",
    "    for i in range(len(x_pred[index])):\n",
    "        if x_pred[index][i] != ans_after[index][i]:\n",
    "            count+=1\n",
    "    if count<=5:\n",
    "        total+=1\n",
    "    count = 0\n",
    "print('total',total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 87\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "for index in range(len(x_pred)):\n",
    "    for i in range(len(x_pred[index])):\n",
    "        if x_pred[index][i] != ans[index][i]:\n",
    "            count+=1\n",
    "    if count<=5:\n",
    "        total+=1\n",
    "    count = 0\n",
    "print('total',total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START WORKING ON: SENTIMENT ANALYSIS FOR NLP COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THOUGHTS: Right now, i am attaching a flag for every after_one_hot_encoding_attribute_vector<br>\n",
    "Means: the model will likely(100%) predict something that is not valid\n",
    "TODO: rewrite dataset representation, create a single flag array, when computing the cost function\n",
    "we use sqart(y-x)*flag\n",
    "\n",
    "Problem: if flag == 0, cost = 0? WTF???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': {18: 0, 19: 1, 149: 2},\n",
       " 'mood_01': {2: 0, 3: 1, 7: 2},\n",
       " 'major': {0: 0, 2: 1, 4: 2, 6: 3, 7: 4, 9: 5, 12: 6, 13: 7, 17: 8},\n",
       " 'big5_01': {3: 0, 6: 1, 7: 2},\n",
       " 'big5_02': {2: 0, 5: 1, 7: 2},\n",
       " 'big5_03': {5: 0, 6: 1, 7: 2},\n",
       " 'big5_04': {2: 0, 5: 1, 7: 2},\n",
       " 'big5_05': {5: 0, 6: 1, 7: 2},\n",
       " 'big5_06': {2: 0, 5: 1, 7: 2},\n",
       " 'big5_07': {5: 0, 6: 1, 7: 2},\n",
       " 'big5_08': {1: 0, 3: 1, 7: 2},\n",
       " 'big5_09': {4: 0, 6: 1, 7: 2},\n",
       " 'big5_10': {2: 0, 3: 1, 7: 2}}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1.])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pred[0][0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoughts: two column in manylab 3 contains NLP COLUMN, HIGHPOWER & LOWPOWER, I could analysis these two column to tag sentiment for each candidate, to evaalute whether they like highpower and lowpower or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT CLASS: ENJOR > NEURTAL > NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2998, 58)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE FUNCTION FOR CROSS VALIDATION\n",
    "Assess the accuracy by count how many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(arr_all_candidate,num_fold,og_pred,size_hidden_layer,iter_num,learning_rate):\n",
    "    cost = 0\n",
    "    #create num_fold equal size fold from input nparray\n",
    "    list_of_fold = np.array_split(arr_all_candidate,num_fold)\n",
    "    \n",
    "    #need to append training dataset together, while leave testing set alone\n",
    "    for i in range(num_fold):\n",
    "        x_pred = list_of_fold[i]\n",
    "        \n",
    "        tmp = 0\n",
    "        for j in range(0,num_fold):\n",
    "            if j == i:\n",
    "                continue\n",
    "            else:\n",
    "                \n",
    "                if tmp == 0:\n",
    "                    ans = np.asarray(list_of_fold[j])\n",
    "                    tmp+=1\n",
    "                \n",
    "                elif tmp > 0:\n",
    "                    \n",
    "                    ans = cro_new_append(ans,list_of_fold[j])\n",
    "\n",
    "            \n",
    "        #finished rebuilding training dataset\n",
    "        #start training neural network\n",
    "        nn = NeuralNetwork(size_hidden_layer,iter_num,learning_rate) #size of input layer\n",
    "        \n",
    "        nn.fit(ans,ans)\n",
    "        predict_ans = nn.predict(x_pred)\n",
    "        find_classification_based_on_distance(predict_ans,og_pred)\n",
    "        #coun\n",
    "        tmpcost = cost_function(predict_ans,x_pred)\n",
    "        cost+=tmpcost\n",
    "        print('#### current cutting position is : ', i, 'current cost is: ',tmpcost)\n",
    "    print('final avg cost is : ', cost/num_fold)\n",
    "    return cost/num_fold\n",
    "\n",
    "\n",
    "\n",
    "def cost_function(x,y):\n",
    "    #compute the average num of wrongly predicted column as cost function\n",
    "    total_num = len(x)\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        tmpcount = 0\n",
    "        for j in range(len(x[i])):\n",
    "            if x[i][j]!=y[i][j]:\n",
    "                tmpcount+=1\n",
    "        count+=tmpcount\n",
    "    avg_count = count/total_num\n",
    "    return avg_count\n",
    "def cross_valid_append_arr(old,new):\n",
    "    ans = np.append([old],[new],axis = 0)\n",
    "    return ans\n",
    "\n",
    "def cro_new_append(old,new):\n",
    "    #num_col\n",
    "    num_col = len(old[0])\n",
    "    new_ans = []\n",
    "    for i in range(len(old)):\n",
    "        new_ans.append(old[i])\n",
    "    for i in range(len(new)):\n",
    "        new_ans.append(new[i])\n",
    "    return np.asarray(new_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693194\n",
      "Cost after iteration 1000: 0.253771\n",
      "Cost after iteration 2000: 0.253741\n",
      "Cost after iteration 3000: 0.253473\n",
      "#### current cutting position is :  0 current cost is:  59.275862068965516\n",
      "Cost after iteration 0: 0.693277\n",
      "Cost after iteration 1000: 0.272021\n",
      "Cost after iteration 2000: 0.272006\n",
      "Cost after iteration 3000: 0.271998\n",
      "#### current cutting position is :  1 current cost is:  51.95862068965517\n",
      "Cost after iteration 0: 0.693307\n",
      "Cost after iteration 1000: 0.270589\n",
      "Cost after iteration 2000: 0.270540\n",
      "Cost after iteration 3000: 0.270510\n",
      "#### current cutting position is :  2 current cost is:  37.210344827586205\n",
      "Cost after iteration 0: 0.693301\n",
      "Cost after iteration 1000: 0.261230\n",
      "Cost after iteration 2000: 0.261212\n",
      "Cost after iteration 3000: 0.261202\n",
      "#### current cutting position is :  3 current cost is:  61.37931034482759\n",
      "Cost after iteration 0: 0.693077\n",
      "Cost after iteration 1000: 0.394900\n",
      "Cost after iteration 2000: 0.394881\n",
      "Cost after iteration 3000: 0.394872\n",
      "#### current cutting position is :  4 current cost is:  67.5551724137931\n",
      "Cost after iteration 0: 0.692956\n",
      "Cost after iteration 1000: 0.265439\n",
      "Cost after iteration 2000: 0.265420\n",
      "Cost after iteration 3000: 0.265409\n",
      "#### current cutting position is :  5 current cost is:  58.672413793103445\n",
      "Cost after iteration 0: 0.693083\n",
      "Cost after iteration 1000: 0.264477\n",
      "Cost after iteration 2000: 0.264445\n",
      "Cost after iteration 3000: 0.249242\n",
      "#### current cutting position is :  6 current cost is:  59.94137931034483\n",
      "Cost after iteration 0: 0.693236\n",
      "Cost after iteration 1000: 0.265505\n",
      "Cost after iteration 2000: 0.265491\n",
      "Cost after iteration 3000: 0.265483\n",
      "#### current cutting position is :  7 current cost is:  61.12068965517241\n",
      "Cost after iteration 0: 0.693036\n",
      "Cost after iteration 1000: 0.262683\n",
      "Cost after iteration 2000: 0.262614\n",
      "Cost after iteration 3000: 0.262562\n",
      "#### current cutting position is :  8 current cost is:  62.04137931034483\n",
      "Cost after iteration 0: 0.693195\n",
      "Cost after iteration 1000: 0.267351\n",
      "Cost after iteration 2000: 0.267334\n",
      "Cost after iteration 3000: 0.267323\n",
      "#### current cutting position is :  9 current cost is:  57.05172413793103\n",
      "final avg cost is :  57.620689655172406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57.620689655172406"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(X_train_new,10,og_pred,60,4000,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
