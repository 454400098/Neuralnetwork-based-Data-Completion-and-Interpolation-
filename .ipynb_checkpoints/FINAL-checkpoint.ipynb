{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO MACHINE LEARNING LIBARY USED FOR TRAINING/REGRESSION/CLASSFICATION\n",
    "# sklean.feature_extraction ONLY USED for language data into Vector transformation\n",
    "# preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer#ONLY USED FOR TRANSFORM FROM LANGUAGE TO VECTOR\n",
    "from numpy import linalg as LA\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (17,55,59,61,65,68,69,70,83,90,91,92,93,120,121,122,123,126,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./Tab.delimited.Cleaned.dataset.WITH.variable.labels.csv', sep='\\t',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,11,12,19,20,129,132,169,230) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.read_csv('./ML3AllSites.csv', sep=',',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, n_h, n_iterate=10, learning_rate=1, dic = []):\n",
    "        self.n_x = None  # size of the input layer\n",
    "        self.n_h = n_h  # size of the hidden layer\n",
    "        self.n_y = None # size of the output layer\n",
    "        self.W1 = None\n",
    "        self.W2 = None\n",
    "        self.b1 = None\n",
    "        self.b2 = None\n",
    "        self.A1 = None\n",
    "        self.A2 = None  # sigmoid output of the second activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterate = n_iterate\n",
    "#         self.dic = dic\n",
    "        self.dic = dic\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        self.W1 = np.random.randn(self.n_h, self.n_x) * 0.01\n",
    "        self.b1 = np.zeros((self.n_h, 1))\n",
    "        self.W2 = np.random.randn(self.n_y, self.n_h) * 0.01\n",
    "        self.b2 = np.zeros((self.n_y, 1))\n",
    "    \n",
    "    def MSE(self,Y):\n",
    "        cost = 0\n",
    "        for i in range(len(Y)):\n",
    "            for j in range(len(Y[0])):\n",
    "                cost+=(Y[i][j]-self.A2[i][j])**2\n",
    "        cost = cost/len(Y)\n",
    "        return cost\n",
    "        \n",
    "    def relu(self, z):\n",
    "        return z * (z > 0)\n",
    "    \n",
    "#     def softmax(self,z):\n",
    "#         exps = np.exp(z-np.max(z,axis= 1,keepdims = True))\n",
    "#         return exps/np.sum(exps,axis = 1, keepdims = True)\n",
    "    \n",
    "    \n",
    "\n",
    "#     def softmax(self,A):  \n",
    "#         expA = np.exp(A)\n",
    "#         return expA / expA.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    \n",
    "    def cro_entro(self,Y):\n",
    "        cost = - (Y * np.log(self.A2) + (1-Y) * np.log(1-self.A2)).mean()\n",
    "        return cost\n",
    "    \n",
    "    def compute_cost(self, Y):\n",
    "        #cost = np.linalg.norm(self.A2 - Y)\n",
    "        cost = - (Y * np.log(self.A2) + (1-Y) * np.log(1-self.A2)).mean()\n",
    "#         print('shape cost is: ' ,cost.shape)\n",
    "        return np.squeeze(cost) \n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        self.A2 = self.sigmoid(self.W2 @ self.A1 + self.b2)\n",
    "#         self.A2 = self.soft_max_For_multi_attribute(self.A2,self.dic)\n",
    "        \n",
    "        return self.A2\n",
    "    \n",
    "    def backward_propagation(self, X, Y):\n",
    "        m = X.shape[1]\n",
    "        dZ2 = self.A2 - Y\n",
    "#         print('cur shape of dz2 :',dZ2.shape)\n",
    "#         print('cur cost of cros entro', c)\n",
    "        dW2 = dZ2 @ self.A1.T / m\n",
    "#         print('shape of dw2 is',dW2.shape)\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "        dZ1 = self.W2.T @ dZ2 * (self.A1 > 0)\n",
    "        dW1 = dZ1 @ X.T / m\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        X, Y = X.T, Y.T\n",
    "        self.n_x = X.shape[0]\n",
    "        self.n_y = Y.shape[0]\n",
    "        self.initialize_parameters()\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(0, self.n_iterate):\n",
    "            self.forward_propagation(X)\n",
    "            self.backward_propagation(X, Y)\n",
    "            if i % 10 == 0:\n",
    "                cost = self.compute_cost(Y)\n",
    "#                  self.learning_rate = 5 * cost\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    def fit_continue(self,X,Y):\n",
    "        X, Y = X.T, Y.T\n",
    "        self.n_x = X.shape[0]\n",
    "        self.n_y = Y.shape[0]\n",
    "#         if self.W1 is None:\n",
    "#             self.initialize_parameters()\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(0, self.n_iterate):\n",
    "            self.forward_propagation(X)\n",
    "            self.backward_propagation(X, Y)\n",
    "            if i % 10 == 0:\n",
    "                cost = self.compute_cost(Y)\n",
    "#                 self.learning_rate = 5 * cost\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        X = X.T\n",
    "        A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        A2 = self.soft_max_For_multi_attribute(self.W2 @ self.A1 + self.b2,self.dic)\n",
    "        return A2.T\n",
    "    \n",
    "    def soft_max_For_multi_attribute(self,arr,dic):\n",
    "        #create array with all dict keys based on insertion order\n",
    "        tmp_arr = np.copy(arr)\n",
    "        \n",
    "        dicarr = list(dic.keys())\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_length = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_length    \n",
    "            #find the targeted array need to perform softmax\n",
    "            #create a dic to contain the location of those onehotencoding attribute\n",
    "            loc_dic = []\n",
    "            for i in range(cur_dict_length-1):\n",
    "                loc_dic.append(cur_loc+i)\n",
    "            #find targed array\n",
    "            target = tmp_arr[:,loc_dic]\n",
    "            #get softmaxed arr\n",
    "            arr_soft_max = self.api_softmax(target)\n",
    "            #reassig value back to orignal arr\n",
    "            tmp_arr[:,loc_dic] = arr_soft_max\n",
    "            #update cur_loc\n",
    "            cur_loc +=cur_dict_length\n",
    "            \n",
    "        return tmp_arr\n",
    "\n",
    "    def api_softmax(self,z):\n",
    "        exps = np.exp(z)\n",
    "        return exps/np.sum(exps,axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=64):\n",
    "        self.n_clusters = n_clusters  # number of clusters\n",
    "        self.centers = None  # to record the centers\n",
    "        self.labels = None\n",
    "        self.Y = None\n",
    "\n",
    "    def random_center(self,Y): #only for 1d dataset\n",
    "    # randomly generate n_cluster clusters in the raange of X\n",
    "        self.centers = np.random.rand(self.n_clusters, len(Y))\n",
    "        for i in range(self.n_clusters):\n",
    "            self.centers[i] = Y[i]\n",
    "            \n",
    "            \n",
    "    def random_center2(self,Y):\n",
    "    # randomly generate n_cluster clusters in the raange of X\n",
    "        self.centers = np.random.rand(self.n_clusters, len(Y[0]))\n",
    "        for i in range(self.n_clusters):\n",
    "            self.centers[i] = Y[i]\n",
    "#         for j in range(0):\n",
    "#             Y_j_min = self.Y[:,j].min()\n",
    "#             Y_j_max = self.Y[:,j].max()\n",
    "#             self.centers[:,j] = Y_j_min + (Y_j_max - Y_j_min) * self.centers[:,j]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def dist(self, point1, point2): #old one\n",
    "        return 2*(point1[0]-point2[0])**2 + 4*(point1[1]-point2[1])**2 + 3*(point1[2]-point2[2])**2\n",
    "\n",
    "    def dist2(self,point1,point2):\n",
    "        return LA.norm(point1-point2)**2\n",
    "    \n",
    "    def fit(self, Y):\n",
    "        self.Y = Y\n",
    "        self.labels = np.zeros(Y.shape[0], dtype='uint8')  # record the current labels of each sample of X\n",
    "        self.random_center(Y)\n",
    "        diff = 1\n",
    "        \n",
    "        while diff > 1e-3:\n",
    "            old_center = self.centers.copy()\n",
    "\n",
    "            # go through all samples and label them using the nearest label\n",
    "            for i in range(Y.shape[0]):\n",
    "                distance = np.zeros(self.n_clusters)\n",
    "                for j in range(self.n_clusters):\n",
    "                    distance[j] = self.dist2(Y[i], self.centers[j])\n",
    "                self.labels[i] = np.argmin(distance)\n",
    "                \n",
    "                \n",
    "            # update the centers\n",
    "            for i in range(self.n_clusters):\n",
    "                self.centers[i] = Y[self.labels==i].mean(axis=0)\n",
    "                \n",
    "\n",
    "            # update the difference\n",
    "            diff = np.linalg.norm(self.centers - old_center)\n",
    "            print(diff)\n",
    "        return self\n",
    "    \n",
    "    def fit2(self, Y):\n",
    "        self.Y = Y\n",
    "        self.labels = np.zeros(Y.shape[0], dtype='uint8')  # record the current labels of each sample of X\n",
    "        self.random_center2(Y)\n",
    "        diff = 1\n",
    "        \n",
    "        while diff > 1e-3:\n",
    "            old_center = self.centers.copy()\n",
    "\n",
    "            # go through all samples and label them using the nearest label\n",
    "            for i in range(Y.shape[0]):\n",
    "                distance = np.zeros(self.n_clusters)\n",
    "                for j in range(self.n_clusters):\n",
    "                    distance[j] = self.dist2(Y[i], self.centers[j])\n",
    "                self.labels[i] = np.argmin(distance)\n",
    "                \n",
    "                \n",
    "            # update the centers\n",
    "            for i in range(self.n_clusters):\n",
    "                self.centers[i] = Y[self.labels==i].mean(axis=0)\n",
    "                \n",
    "\n",
    "            # update the difference\n",
    "            diff = np.linalg.norm(self.centers - old_center)\n",
    "            print(diff)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, Y):\n",
    "        out = np.zeros(Y.shape)\n",
    "        for i in range(self.n_clusters):\n",
    "            out[self.labels==i] = self.centers[i]\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oneHOTENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncoding(Y, kmeans):\n",
    "    out = np.zeros((len(Y), kmeans.n_clusters))\n",
    "    for i in range(len(Y)):\n",
    "        out[i, Y[i]] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test clustering numbered data:\n",
    "    step1 : get single column\n",
    "    step2 : sort in increasing order\n",
    "    step3 : based on distribution, cut data into 4 section with equally number of candidate\n",
    "    step4 : if data is null, set isnon to be true for that column\n",
    "    step5 : append new datacol to input_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isint(value):\n",
    "  try:\n",
    "    int(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "def isfloat(value):\n",
    "  try:\n",
    "    float(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "def getdistriarr(df,arr_str):\n",
    "    arr = df[arr_str].values\n",
    "    di = []\n",
    "    di_og = []  #RETURN OG SINCE di will be sort\n",
    "    for i in range(len(arr)):\n",
    "        if isint(arr[i]) == True:\n",
    "            di.append(int(arr[i]))\n",
    "            di_og.append(int(arr[i]))\n",
    "        else:\n",
    "            di.append(-1)\n",
    "            di_og.append(-1)\n",
    "    di.sort()\n",
    "    ans_arr = []\n",
    "    leng = len(di)\n",
    "    ans_arr.append(di[int(leng/3)])\n",
    "#     ans_arr.append(di[int(leng/2)])\n",
    "    ans_arr.append(di[int(leng*0.66)])\n",
    "    ans_arr.append(di[int(leng-1)])\n",
    "    return di_og,ans_arr\n",
    "\n",
    "def oneHotEncoding_return_arr(att_arr,class_arr): #one \n",
    "    #add one more column for each row since last col used as flag\n",
    "    choice = len(class_arr)\n",
    "    out = np.zeros((len(att_arr),len(class_arr)+1))\n",
    "    for i in range(len(att_arr)):\n",
    "        loc = 0\n",
    "        #first jude if data is null\n",
    "        if att_arr[i] == -1:\n",
    "            out[i][-1] = 0\n",
    "            continue\n",
    "        else:\n",
    "            out[i][-1] = 1\n",
    "            for j in range(len(class_arr)):\n",
    "                if att_arr[i] == class_arr[j]:\n",
    "                    out[i][j] = 1\n",
    "                    break\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "def GET_ONE_HOT_ARRAY_ONLY_DIGIT_COLUMN(df,attr_name,og_dic):\n",
    "    att_arr,class_arr = getdistriarr(df,attr_name)\n",
    "    dic_ans = create_dic_for_classification(class_arr)\n",
    "    append_arr_of_dic_to_overall(og_dic,dic_ans,attr_name)\n",
    "\n",
    "    return oneHotEncoding_return_arr(att_arr,class_arr)\n",
    "\n",
    "def create_dic_for_classification(class_arr):\n",
    "    cur_dic = {}\n",
    "    total_len = len(class_arr)\n",
    "    for i in range(len(class_arr)):\n",
    "        cur_dic[class_arr[i]] = i\n",
    "    return cur_dic\n",
    "\n",
    "def append_arr_of_dic_to_overall(og,dic,att_name):\n",
    "    og[att_name] = dic\n",
    "    \n",
    "    \n",
    "def append_arr(old,new):\n",
    "    #both old and new has the same num of row\n",
    "    #create a new nparray\n",
    "    \n",
    "    newdim = len(old[0])+len(new[0])\n",
    "    ans = np.zeros((len(old),newdim))\n",
    "    print(ans.shape)\n",
    "    for i in range(len(old)):\n",
    "        ans[i] = np.append(old[i],new[i])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP TWO CONVERT NLP DATASET INTO VECTOR:\n",
    "    STEP1: vectorize dataset\n",
    "    STEP2: apply kmeans clustering to language data\n",
    "    STEP3: ONEHOT ENCODING\n",
    "    STEP4: APPEND CLASSIFICATION DATASET INTO old CLASSFICAITION ARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLPDATAPRO(allsentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(allsentences)\n",
    "    ans = X.toarray()\n",
    "    return ans\n",
    "\n",
    "def createtextarr_ONEHOT_ENCODING(attri_col): #ONEHOT_ENCODING\n",
    "    df2 = df[attri_col]\n",
    "    arr = df2.values\n",
    "    for i in range(len(arr)):\n",
    "        di.append(arr[i])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP THREE CONVERT TEXT_DATASET WITH FIXED classification into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processnull(arr):\n",
    "    arrnew = []\n",
    "    for i in range(len(arr)):\n",
    "        if type(arr[i]) == float and math.isnan(arr[i]) :\n",
    "            arrnew.append('Nan')\n",
    "        else:\n",
    "            arrnew.append(arr[i])\n",
    "    return arrnew\n",
    "\n",
    "def returnuniqiearr(og):\n",
    "    ar = np.asarray(og)\n",
    "    return np.unique(ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: create a function to append text to old vecto(remember to append flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: TEST NN TONIGHT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onehoted_text_fixed_classification(attribu_name,df,km_num):\n",
    "    df_cur = df[attribu_name]\n",
    "    attribute_arr = df_cur.values\n",
    "    arr_no_null = processnull(attribute_arr) #create new arr with format fit by NLP(CountVectorizer)\n",
    "    NLP_ARR = NLPDATAPRO(arr_no_null)\n",
    "    #BEFORE ONEHOT_ENCODING\n",
    "    #APPLY KMEANS CLUSTERING SINCE TOO MANY DIFFERENT CLASSIFICATION\n",
    "    KM_TMP = KMeans(km_num)\n",
    "    KM_TMP.fit2(NLP_ARR)\n",
    "    labels_arr = KM_TMP.labels\n",
    "    return labels_arr\n",
    "    \n",
    "def onehot_enco(labelarr,df,att_name):\n",
    "    og_text_arr = df[att_name].values\n",
    "    #create dic to remeber loc for each value in the uniqe arr\n",
    "    uni_arr = np.unique(labelarr)\n",
    "    loc_dic = {}\n",
    "    for i in range(len(uni_arr)):\n",
    "        loc_dic[uni_arr[i]] = i\n",
    "    #based on the lenth of dic,create onehot_encod\n",
    "    choice = len(loc_dic)\n",
    "    out = np.zeros((len(labelarr),choice+1))\n",
    "    for i in range(len(labelarr)):\n",
    "        #first junde if data is null\n",
    "        if type(og_text_arr[i])!=str and math.isnan(og_text_arr[i]) == True:\n",
    "            out[i][-1] = 0\n",
    "            continue\n",
    "        else:\n",
    "            out[i][-1] = 1\n",
    "            cur_loc = loc_dic.get(labelarr[i])\n",
    "            out[i][cur_loc] = 1\n",
    "            \n",
    "    return out,loc_dic\n",
    "\n",
    "def GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,attr_name,og_dic,km_num):\n",
    "    labels_arr = create_onehoted_text_fixed_classification(attr_name,df,km_num)\n",
    "    out_arr,loc_dic = onehot_enco(labels_arr,df,attr_name)\n",
    "    #append dic to global array of dic\n",
    "    append_arr_of_dic_to_overall(og_dic,loc_dic,attr_name)\n",
    "    return out_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL WORK:\n",
    "    1. MAINTAIN AN ARRAY OF DICTIONARY S.T EVERY DIC\n",
    "       CONTAINS MAPPING FROM CLASSIFICAITON TO ACTUAL INDEX AFTER KMEANS\n",
    "    \n",
    "    2. AFTER TRAINING, WE WILL UTILIZED THE ARR_DIC TO RECOVER CANDIDATE ANSER\n",
    "    \n",
    "    3. TODO: REWRITE COST FUNCTION OF NERUAL NETWORK\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINDING:\n",
    "    AFTER combine several attributes column, num of candidate with dinct\n",
    "    ans grows extremly fast.\n",
    "    Need to apply Kmeans clustering again to the array of candidate before\n",
    "    threw into Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE A SCRIPT TO APPEND ASSIGNED ATTRIBUTE TO INPUT COLUMN\n",
    "\n",
    "(API)INSTRUCTION:<br>\n",
    "USER WHO WANTS TO CREATE PREPROCESSED DATASET ONLY NEED TO CREATE\n",
    "ARRAY[i]<br>\n",
    "       ARRAY[i][0] = 'name of attribute'<br>\n",
    "       ARRAY[i][1] = 1 : it is a digit column<br>\n",
    "       ARRAY[i][1] = 0 : it is a NLP column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_auto_append_attri(arr_of_attribute,df,og_dic):\n",
    "    for i in range(len(arr_of_attribute)):\n",
    "        name = arr_of_attribute[i][0]\n",
    "        isdigit = arr_of_attribute[i][1]\n",
    "        if isdigit == 1:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_DIGIT_COLUMN(df,name,og_dic)\n",
    "        else:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,name,og_dic,20)\n",
    "        #append old with new\n",
    "        if i == 0:\n",
    "            old = attr_arr\n",
    "        else:\n",
    "            old = append_arr(old,attr_arr)\n",
    "#             print(old.shape)\n",
    "    return old\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_semi_auto_append_attri(arr_of_attribute,df,og_dic):\n",
    "    for i in range(len(arr_of_attribute)):\n",
    "        name = arr_of_attribute[i][0]\n",
    "        isdigit = arr_of_attribute[i][1]\n",
    "        if isdigit == 1:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_DIGIT_COLUMN(df,name,og_dic)\n",
    "        elif isdigit == 0:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,name,og_dic,20)\n",
    "        elif isdigit == 2:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_FIXED_CHOICE_COLUMN(df,name,og_dic)\n",
    "        #append old with new\n",
    "        if i == 0:\n",
    "            old = attr_arr\n",
    "        else:\n",
    "            old = append_arr(old,attr_arr)\n",
    "#             print(old.shape)\n",
    "    return old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_choice_get_distinct_arr(df,arr_str):\n",
    "    arr = df[arr_str].values\n",
    "    uni_arr = np.unique(arr)\n",
    "    di = []\n",
    "#     di_og = [] #RETURN OG SINCE di will be sorted\n",
    "    div_arr = []\n",
    "    for i in range(len(uni_arr)):\n",
    "        if math.isnan(uni_arr[i])!=True:\n",
    "            div_arr.append(uni_arr[i])\n",
    "    for i in range(len(arr)):\n",
    "        if math.isnan(arr[i])!= True:\n",
    "            di.append(int(arr[i]))\n",
    "        else:\n",
    "            di.append(-1)\n",
    "    return di,div_arr\n",
    "\n",
    "def GET_ONE_HOT_ARRAY_ONLY_FIXED_CHOICE_COLUMN(df,attr_name,og_dic):\n",
    "    att_arr,class_arr = fix_choice_get_distinct_arr(df,attr_name)\n",
    "    print('class_arr',class_arr)\n",
    "    dic_ans = create_dic_for_classification(class_arr)\n",
    "    append_arr_of_dic_to_overall(og_dic,dic_ans,attr_name)\n",
    "    return oneHotEncoding_return_arr(att_arr,class_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Kmeans Clustering to the overal column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_whole_attribute_get_Y_for_Neural_NetWork(X,num_cluster):\n",
    "    km = KMeans(num_cluster)\n",
    "    km.fit2(X)\n",
    "    \n",
    "    #APPLY ONEHOT_ENCODING TO NEURAL_NETWORKAGAIN\n",
    "    \n",
    "    return oneHotEncoding(km.labels,km),km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLY NEURAL NETWORK TO SOLVE THIS QUESTION:\n",
    "\n",
    "REMEBER TO CONSIDER FLAG FOR EACH COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predicted_data(pred,kmeans):\n",
    "    ans = []\n",
    "    for i in range(len(pred)):\n",
    "        ans.append(kmeans.centers[np.argmax(pred[i])])\n",
    "    newarr = np.asarray(ans)\n",
    "    return newarr\n",
    "\n",
    "def find_classification_based_on_distance(pred,dic):\n",
    "    #create array with all dict keys based on insertion order\n",
    "    dicarr = list(dic.keys())\n",
    "    #isolate target awated_array used to classification\n",
    "    for i in range(len(pred)):\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "#             print('cur dic: ',cur_dict_key)\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_lengh = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_lengh  #update current cutting position\n",
    "            #restore dimension of onehotencoding for current dictionary\n",
    "            one_hot_cur_dic = np.zeros((cur_dict_lengh,cur_dict_lengh))\n",
    "#             print('before any processing cur loc is ', cur_loc, ' cur des is : ', cur_des)\n",
    "            for q in range(0,cur_dict_lengh-1):\n",
    "                one_hot_cur_dic[q][q] = 1\n",
    "                one_hot_cur_dic[q][-1] = 1\n",
    "            #isolate target awaited_classification array\n",
    "            cur_isolated_arr = pred[i][cur_loc:cur_des]\n",
    "            #compute classification with smallest distance\n",
    "            arr_store_distance= []\n",
    "            for h in range (len(one_hot_cur_dic)):\n",
    "                arr_store_distance.append(np.linalg.norm(cur_isolated_arr-one_hot_cur_dic[h]))\n",
    "            #transfer dic to arr s.t unilized argmim\n",
    "            dis_np_array = np.asarray(arr_store_distance)\n",
    "            index = np.argmin(dis_np_array)\n",
    "#             if cur_dict_key == 'big5_10':\n",
    "#                 print(index)\n",
    "#                 print('cur onehot_array is ', one_hot_cur_dic[3] )\n",
    "#                 print('cur loc is ', cur_loc, ' cur des is : ', cur_des)\n",
    "            # change value in output array\n",
    "            for k in range (cur_dict_lengh):\n",
    "                pred[i][cur_loc+k] = one_hot_cur_dic[index][k]\n",
    "                \n",
    "            cur_loc += cur_dict_lengh #update cur_loc not used in curretn loop\n",
    "\n",
    "def find_classification_based_on_prob(pred,dic):\n",
    "    dicarr = list(dic.keys())\n",
    "    for i in range(len(pred)):\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_lengh = len(cur_dict_arr)  + 1\n",
    "            cur_des += cur_dict_lengh\n",
    "            \n",
    "            cur_isolated_arr = pred[i][cur_loc:cur_des-1]\n",
    "            #find index of max value\n",
    "            index = np.argmax(cur_isolated_arr)\n",
    "            \n",
    "            for k in range(cur_dict_lengh):\n",
    "                pred[i][cur_loc+k] = 0\n",
    "            \n",
    "            pred[i][cur_loc + index] = 1\n",
    "            cur_loc +=cur_dict_lengh\n",
    "            \n",
    "            \n",
    "def create_reverse_dictionary(dic):\n",
    "    reverse = {}\n",
    "    dic_arr = list(dic.keys())\n",
    "    for i in range(len(dic_arr)):\n",
    "        cur_reverse_dict = {}\n",
    "        cur_dic_key = dic_arr[i]\n",
    "        cur_key_arr = list(dic.get(cur_dic_key).keys())\n",
    "        cur_index_arr = list(dic.get(cur_dic_key).values())\n",
    "        #reverse key_value pair in originaly dictionary\n",
    "        for j in range(len(cur_key_arr)):\n",
    "            cur_reverse_dict[cur_index_arr[j]] = cur_key_arr[j]\n",
    "        #append current dic to ans dic\n",
    "        reverse[cur_dic_key] = cur_reverse_dict\n",
    "    return reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create  function to knock out some attribute inorder to create more dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingset_generator(train,dic,arr_str):\n",
    "    #first find the location \n",
    "    #knock out\n",
    "    #return dataset\n",
    "    dicarr = list(dic.keys())\n",
    "    changed_arr = np.copy(train)\n",
    "    y_arr = np.copy(train)\n",
    "    #outermost loop : loop through those keys that need to be cleared\n",
    "    for d in range(len(arr_str)):\n",
    "        cur_key = arr_str[d]\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_length = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_length\n",
    "            #test if we find what we want\n",
    "            if cur_dict_key == cur_key:\n",
    "                for i in range(len(train)):\n",
    "                    for k in range(cur_dict_length):\n",
    "                        changed_arr[i][cur_loc + k] = 0\n",
    "                        \n",
    "                break\n",
    "            cur_loc +=cur_dict_length\n",
    "    return changed_arr,y_arr\n",
    "\n",
    "def append_row(old,new):\n",
    "    ans = np.vstack([old,new])\n",
    "    return ans\n",
    "\n",
    "\n",
    "def enlarge_dataset(train,y,dic,arr_of_arr_str):\n",
    "    list_of_new_x = []\n",
    "    list_of_new_y = []\n",
    "    new_x = np.copy(train)\n",
    "    new_y = np.copy(y)\n",
    "    for i in range(len(arr_of_arr_str)):\n",
    "        cur_str = arr_of_arr_str[i]\n",
    "        changed_arr,y_arr = trainingset_generator(train,dic,cur_str)\n",
    "        #append\n",
    "        new_x = append_row(new_x,changed_arr)\n",
    "        new_y = append_row(new_y,y_arr)\n",
    "        list_of_new_x.append(changed_arr)\n",
    "        list_of_new_y.append(y_arr)\n",
    "#     return new_x,new_y\n",
    "    return list_of_new_x,list_of_new_y\n",
    "\n",
    "def createlist_list_string(dic):\n",
    "    ans = []\n",
    "    key_arr = list(dic.keys())\n",
    "    for i in range(len(key_arr)):\n",
    "        tmp = []\n",
    "        tmp.append(key_arr[i])\n",
    "        ans.append(tmp)\n",
    "        \n",
    "    #degree 2\n",
    "#     for i in range(len(key_arr)):\n",
    "#         for j in range(i,len(key_arr)):\n",
    "#             tmp = []\n",
    "#             tmp.append(key_arr[i])\n",
    "#             tmp.append(key_arr[j])\n",
    "#             ans.append(tmp)\n",
    "    return ans\n",
    "\n",
    "def train_list_of_dataset(nn,x,y):\n",
    "    nn.fit(y,y)\n",
    "    for i in range(0,len(x)):\n",
    "        print('current iter', i)\n",
    "        nn.fit_continue(x[i],y)\n",
    "    \n",
    "    return nn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create testing attribut arra\n",
    "arr = []\n",
    "og_arr = []\n",
    "og_train = {} #init global dictionary\n",
    "og_pred = {}\n",
    "# arr.append(['age',1])\n",
    "# arr.append(['mood_01',2]) #Today I generally feel\n",
    "# arr.append(['major',0])\n",
    "# arr.append(['big5_01',2]) #I see myself as: Extraverted, enthusiastic.\n",
    "# arr.append(['big5_02',2]) #I see myself as: Critical, quarrelsome.\n",
    "# arr.append(['big5_03',2]) #I see myself as: Dependable, self-disciplined.\n",
    "# arr.append(['big5_04',2]) #I see myself as: Anxious, easily upset.\n",
    "# arr.append(['big5_05',2]) #I see myself as: Open to new experiences, complex.\n",
    "# arr.append(['big5_06',2]) #I see myself as: Reserved, quiet.\n",
    "# arr.append(['big5_07',2]) #I see myself as: Sympathetic, warm.\n",
    "# arr.append(['big5_08',2]) #I see myself as: Disorganized, careless.\n",
    "# arr.append(['big5_09',2]) #I see myself as: Calm, emotionally stable.\n",
    "# arr.append(['big5_10',2]) #I see myself as: Conventional, uncreative.\n",
    "arr.append(['intrinsic_01',2])\n",
    "arr.append(['intrinsic_02',2])\n",
    "arr.append(['intrinsic_03',2])\n",
    "arr.append(['intrinsic_04',2])\n",
    "arr.append(['intrinsic_05',2])\n",
    "arr.append(['intrinsic_06',2])\n",
    "arr.append(['intrinsic_07',2])\n",
    "arr.append(['intrinsic_08',2])\n",
    "arr.append(['intrinsic_09',2])\n",
    "arr.append(['intrinsic_10',2])\n",
    "arr.append(['intrinsic_11',2])\n",
    "arr.append(['intrinsic_12',2])\n",
    "arr.append(['intrinsic_13',2])\n",
    "arr.append(['intrinsic_14',2])\n",
    "arr.append(['intrinsic_15',2])\n",
    "arr.append(['mcfiller1',2])\n",
    "arr.append(['mcfiller2',2])\n",
    "arr.append(['mcfiller3',2])\n",
    "arr.append(['mcmost1',2])\n",
    "arr.append(['mcmost2',2])\n",
    "arr.append(['mcmost3',2])\n",
    "arr.append(['mcmost4',2])\n",
    "arr.append(['mcmost5',2])\n",
    "arr.append(['mcsome1',2])\n",
    "arr.append(['mcsome2',2])\n",
    "arr.append(['mcsome3',2])\n",
    "arr.append(['mcsome4',2])\n",
    "arr.append(['mcsome5',2])\n",
    "arr.append(['mood_01',2])\n",
    "arr.append(['mood_02',2])\n",
    "arr.append(['pate_01',2])\n",
    "arr.append(['pate_02',2])\n",
    "arr.append(['pate_03',2])\n",
    "arr.append(['pate_04',2])\n",
    "arr.append(['pate_05',2])\n",
    "arr.append(['stress_01',2])\n",
    "arr.append(['stress_02',2])\n",
    "arr.append(['stress_03',2])\n",
    "arr.append(['stress_04',2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_df = df3.iloc[:100]\n",
    "trainning_df = df3\n",
    "# trainning_df = trainning_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 10)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 54.0]\n",
      "(2998, 16)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 77.0]\n",
      "(2998, 22)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 28)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 33)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 39)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 45)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 50)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 55)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 61)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 66)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 71)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 77)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 82)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 87)\n",
      "class_arr [0.0, 1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 93)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 98)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 101)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 104)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 107)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 110)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 113)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 116)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 119)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 122)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 125)\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 128)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 136)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 144)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 150)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 156)\n",
      "class_arr [1.0, 2.0, 3.0]\n",
      "(2998, 160)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "(2998, 167)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "(2998, 174)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 180)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 186)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 192)\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 198)\n"
     ]
    }
   ],
   "source": [
    "# X_train = semi_auto_append_attri(arr,predict_df,og)\n",
    "X_train = new_semi_auto_append_attri(arr,trainning_df,og_pred)\n",
    "# print('start training dataset')\n",
    "# X_tra = semi_auto_append_attri(arr,trainning_df,og_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = X_train[:2900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice nparr\n",
    "x_pred = X_train[2900:]\n",
    "x_real_train = X_train[:2900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = createlist_list_string(og_pred)\n",
    "x,y = enlarge_dataset(X_train_new,X_train_new,og_pred,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(200000,100,0.1,og_pred) #size of input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.703647\n",
      "Cost after iteration 10: 0.292429\n",
      "Cost after iteration 20: 0.215618\n",
      "Cost after iteration 30: 0.178420\n",
      "Cost after iteration 40: 0.141007\n",
      "Cost after iteration 50: 0.113522\n",
      "Cost after iteration 60: 0.090491\n",
      "Cost after iteration 70: 0.062856\n",
      "Cost after iteration 80: 0.062789\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train_new,X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = nn.predict(x_real_train)\n",
    "find_classification_based_on_distance(ans,og_pred)\n",
    "print('unique ans', len(np.unique(ans,axis = 0)))\n",
    "print('unique input',len(np.unique(x_real_train,axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "p = []\n",
    "for i in range(len(x_real_train)):\n",
    "    for j in range(len(x_real_train[0])):\n",
    "        if x_real_train[i][j]!=ans[i][j]:\n",
    "            count+=1\n",
    "           \n",
    "print(count/len(x_real_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique ans 2278\n",
      "unique input 2393\n"
     ]
    }
   ],
   "source": [
    "find_classification_based_on_prob(ans,og_pred)\n",
    "print('unique ans', len(np.unique(ans,axis = 0)))\n",
    "print('unique input',len(np.unique(x_real_train,axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-399-e9ce81a5d2d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# find_classification_based_on_distance(ans,og_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfind_classification_based_on_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mog_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unique ans'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unique input'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-383-4cf5ef133f36>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoft_max_For_multi_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "ans = nn.predict(x)\n",
    "# find_classification_based_on_distance(ans,og_pred)\n",
    "find_classification_based_on_prob(ans,og_pred)\n",
    "print('unique ans', len(np.unique(ans,axis = 0)))\n",
    "print('unique input',len(np.unique(x,axis = 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = train_list_of_dataset(nn,x,X_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THOUGHTS: Might need another algorithm to autocomple those column that is nan, based on knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO : CREATE FUNCTION TO RECOVER CLASSIFICATION DATASET INTO HUMAN-Fridendly DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_answer(predict_arr,dic):\n",
    "    reverse_dic = create_reverse_dictionary(dic)\n",
    "    #init an dic as final anser array\n",
    "    ans_arr = []\n",
    "    #create array with all dict keys based on insertion order\n",
    "    dic_arr = list(dic.keys())\n",
    "    #recover predicted candidate sequentilaay\n",
    "    for i in range(len(predict_arr)):\n",
    "        tmp_ans_array = {} # cur dict to store anser for this candidate\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        #loop through dictionary array to recover each attribute column\n",
    "        for j in range(len(dic_arr)):\n",
    "            cur_dict_key = dic_arr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_reverse_dic_arr = reverse_dic.get(cur_dict_key)\n",
    "            cur_dict_length = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_length #update current cutting position\n",
    "            #restore\n",
    "            tmp_cutting_arr = predict_arr[i][cur_loc:cur_des]\n",
    "            tag = tmp_cutting_arr[-1]\n",
    "            if tag == 0:\n",
    "                tmp_ans_array[cur_dict_key] = 'NA'\n",
    "            else:\n",
    "                #find the position where tag is 1\n",
    "                loc = 0\n",
    "                for pos in range(len(tmp_cutting_arr)):\n",
    "                    if tmp_cutting_arr[pos] == 1:\n",
    "                        loc = pos\n",
    "                        break\n",
    "                #now based on location, create cover anser\n",
    "                #case one loc is 0\n",
    "                if loc == 0:\n",
    "                    ans =\"<\" + str(cur_reverse_dic_arr.get(loc))\n",
    "                    tmp_ans_array[cur_dict_key] = ans\n",
    "                else:\n",
    "                    last_loc = loc-1\n",
    "                    ans=\"\"\n",
    "                    ans+=str(cur_reverse_dic_arr.get(last_loc))+\" < \"\n",
    "                    ans+=str(cur_reverse_dic_arr.get(loc))\n",
    "                    tmp_ans_array[cur_dict_key] = ans\n",
    "#                 ans =cur_reverse_dic_arr.get(loc)\n",
    "#                 tmp_ans_array[cur_dict_key] = ans\n",
    "        \n",
    "            cur_loc+=cur_dict_length\n",
    "        #append current tmp candidate array to finaly answer\n",
    "        ans_arr.append(tmp_ans_array)\n",
    "    \n",
    "    return ans_arr\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE FUNCTION FOR CROSS VALIDATION\n",
    "Assess the accuracy by count how many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(arr_all_candidate,num_fold,og_pred,size_hidden_layer,iter_num,learning_rate):\n",
    "    cost = 0\n",
    "    #create num_fold equal size fold from input nparray\n",
    "    list_of_fold = np.array_split(arr_all_candidate,num_fold)\n",
    "    \n",
    "    #need to append training dataset together, while leave testing set alone\n",
    "    for i in range(num_fold):\n",
    "        x_pred = list_of_fold[i]\n",
    "        \n",
    "        tmp = 0\n",
    "        for j in range(0,num_fold):\n",
    "            if j == i:\n",
    "                continue\n",
    "            else:\n",
    "                \n",
    "                if tmp == 0:\n",
    "                    ans = np.asarray(list_of_fold[j])\n",
    "                    tmp+=1\n",
    "                \n",
    "                elif tmp > 0:\n",
    "                    \n",
    "                    ans = cro_new_append(ans,list_of_fold[j])\n",
    "\n",
    "            \n",
    "        #finished rebuilding training dataset\n",
    "        #start training neural network\n",
    "        nn = NeuralNetwork(size_hidden_layer,iter_num,learning_rate) #size of input layer\n",
    "        \n",
    "        nn.fit(ans,ans)\n",
    "        predict_ans = nn.predict(x_pred)\n",
    "        find_classification_based_on_distance(predict_ans,og_pred)\n",
    "        #coun\n",
    "        tmpcost = cost_function(predict_ans,x_pred)\n",
    "        cost+=tmpcost\n",
    "        print('#### current cutting position is : ', i, 'current cost is: ',tmpcost)\n",
    "    print('final avg cost is : ', cost/num_fold)\n",
    "    return cost/num_fold\n",
    "\n",
    "\n",
    "\n",
    "def cost_function(x,y):\n",
    "    #compute the average num of wrongly predicted column as cost function\n",
    "    total_num = len(x)\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        tmpcount = 0\n",
    "        for j in range(len(x[i])):\n",
    "            if x[i][j]!=y[i][j]:\n",
    "                tmpcount+=1\n",
    "        count+=tmpcount\n",
    "    avg_count = count/total_num\n",
    "    return avg_count\n",
    "def cross_valid_append_arr(old,new):\n",
    "    ans = np.append([old],[new],axis = 0)\n",
    "    return ans\n",
    "\n",
    "def cro_new_append(old,new):\n",
    "    #num_col\n",
    "    num_col = len(old[0])\n",
    "    new_ans = []\n",
    "    for i in range(len(old)):\n",
    "        new_ans.append(old[i])\n",
    "    for i in range(len(new)):\n",
    "        new_ans.append(new[i])\n",
    "    return np.asarray(new_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation(X_train_new,10,og_pred,60,4000,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
