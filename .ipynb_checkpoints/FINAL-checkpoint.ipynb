{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The most powerful NN setting:\n",
    "1. only 1 hidden layer\n",
    "2. 2000 neurons\n",
    "3. learning rate: 0.1\n",
    "# create double hidden layer NN class: -> complete autoencoder today, 5.04\n",
    "# TODO(5.02.2019): ADD Lasso Regularization  \n",
    "# NO MACHINE LEARNING LIBARY USED FOR TRAINING/REGRESSION/CLASSFICATION\n",
    "# sklean.feature_extraction ONLY USED for language data into Vector transformation\n",
    "# preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer#ONLY USED FOR TRANSFORM FROM LANGUAGE TO VECTOR\n",
    "from numpy import linalg as LA\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (17,55,59,61,65,68,69,70,83,90,91,92,93,120,121,122,123,126,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./Tab.delimited.Cleaned.dataset.WITH.variable.labels.csv', sep='\\t',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,11,12,19,20,129,132,169,230) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.read_csv('./ML3AllSites.csv', sep=',',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, n_h, n_iterate=10, learning_rate=1, dic = []):\n",
    "        self.n_x = None  # size of the input layer\n",
    "        self.n_h = n_h  # size of the hidden layer\n",
    "        self.n_y = None # size of the output layer\n",
    "        self.W1 = None\n",
    "        self.W2 = None\n",
    "        self.b1 = None\n",
    "        self.b2 = None\n",
    "        self.A1 = None\n",
    "        self.A2 = None  # sigmoid output of the second activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterate = n_iterate\n",
    "#         self.dic = dic\n",
    "        self.dic = dic\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        self.W1 = np.random.randn(self.n_h, self.n_x) * 0.01\n",
    "        self.b1 = np.zeros((self.n_h, 1))\n",
    "        self.W2 = np.random.randn(self.n_y, self.n_h) * 0.01\n",
    "        self.b2 = np.zeros((self.n_y, 1))\n",
    "\n",
    "    \n",
    "    def MSE(self,Y):\n",
    "        cost = 0\n",
    "        for i in range(len(Y)):\n",
    "            for j in range(len(Y[0])):\n",
    "                cost+=(Y[i][j]-self.A2[i][j])**2\n",
    "        cost = cost/len(Y)\n",
    "        return cost\n",
    "        \n",
    "    def relu(self, z):\n",
    "        return z * (z > 0)\n",
    "    \n",
    "#     def softmax(self,z):\n",
    "#         exps = np.exp(z-np.max(z,axis= 1,keepdims = True))\n",
    "#         return exps/np.sum(exps,axis = 1, keepdims = True)\n",
    "    \n",
    "    \n",
    "\n",
    "#     def softmax(self,A):  \n",
    "#         expA = np.exp(A)\n",
    "#         return expA / expA.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    \n",
    "    def cro_entro(self,Y):\n",
    "        cost = - (Y * np.log(self.A2) + (1-Y) * np.log(1-self.A2)).mean()\n",
    "        return cost\n",
    "    \n",
    "    #changed for mult layer\n",
    "    def compute_cost(self, Y):# cross entrophy\n",
    "        #cost = np.linalg.norm(self.A2 - Y)\n",
    "        cost = - (Y * np.log(self.A2) + (1-Y) * np.log(1-self.A2)).mean()\n",
    "#         print('shape cost is: ' ,cost.shape)\n",
    "        return np.squeeze(cost) \n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        #dropout function for output in hidden layer \n",
    "        u1 = np.random.binomial(1,0.5,size = self.A1.shape)\n",
    "        self.A1 *= u1\n",
    "    \n",
    "        self.A2 = self.sigmoid(self.W2 @ self.A1 + self.b2)\n",
    "\n",
    "#         u2 = np.random.binomial(1,0.5,size = self.A2.shape)\n",
    "#         self.A2 *= u2\n",
    "#         self.A2 = self.soft_max_For_multi_attribute(self.A2,self.dic)\n",
    "\n",
    "        \n",
    "    \n",
    "    def backward_propagation(self, X, Y):\n",
    "        m = X.shape[1]\n",
    "\n",
    "        dZ2 = self.A2 - Y\n",
    "#         print('cur shape of dz2 :',dZ2.shape)\n",
    "#         print('cur cost of cros entro', c)\n",
    "        dW2 = dZ2 @ self.A1.T / m\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "        dZ1 = self.W2.T @ dZ2 * (self.A1 > 0)\n",
    "        dW1 = dZ1 @ X.T / m\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        X, Y = X.T, Y.T\n",
    "        self.n_x = X.shape[0]\n",
    "        self.n_y = Y.shape[0]\n",
    "        self.initialize_parameters()\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(0, self.n_iterate):\n",
    "            self.forward_propagation(X)\n",
    "            self.backward_propagation(X, Y)\n",
    "            if i % 10 == 0:\n",
    "                cost = self.compute_cost(Y)\n",
    "#                  self.learning_rate = 5 * cost\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    def fit_continue(self,X,Y):\n",
    "        X, Y = X.T, Y.T\n",
    "        self.n_x = X.shape[0]\n",
    "        self.n_y = Y.shape[0]\n",
    "#         if self.W1 is None:\n",
    "#             self.initialize_parameters()\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(0, self.n_iterate):\n",
    "            self.forward_propagation(X)\n",
    "            self.backward_propagation(X, Y)\n",
    "            if i % 10 == 0:\n",
    "                cost = self.compute_cost(Y)\n",
    "#                 self.learning_rate = 5 * cost\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        X = X.T\n",
    "        A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        A2 = self.soft_max_For_multi_attribute(self.W2 @ self.A1 + self.b2,self.dic)\n",
    "        return A2.T\n",
    "    \n",
    "    def predict_one_layer(self,X):\n",
    "        X = X.T\n",
    "        A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        return A1.T\n",
    "    \n",
    "    def soft_max_For_multi_attribute(self,arr,dic):\n",
    "        #create array with all dict keys based on insertion order\n",
    "        tmp_arr = np.copy(arr)\n",
    "        \n",
    "        dicarr = list(dic.keys())\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_length = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_length    \n",
    "            #find the targeted array need to perform softmax\n",
    "            #create a dic to contain the location of those onehotencoding attribute\n",
    "            loc_dic = []\n",
    "            for i in range(cur_dict_length-1):\n",
    "                loc_dic.append(cur_loc+i)\n",
    "            #find targed array\n",
    "            target = tmp_arr[:,loc_dic]\n",
    "            #get softmaxed arr\n",
    "            arr_soft_max = self.api_softmax(target)\n",
    "            #reassig value back to orignal arr\n",
    "            tmp_arr[:,loc_dic] = arr_soft_max\n",
    "            #update cur_loc\n",
    "            cur_loc +=cur_dict_length\n",
    "            \n",
    "        return tmp_arr\n",
    "\n",
    "    def api_softmax(self,z):\n",
    "        exps = np.exp(z)\n",
    "        return exps/np.sum(exps,axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=64):\n",
    "        self.n_clusters = n_clusters  # number of clusters\n",
    "        self.centers = None  # to record the centers\n",
    "        self.labels = None\n",
    "        self.Y = None\n",
    "\n",
    "    def random_center(self,Y): #only for 1d dataset\n",
    "    # randomly generate n_cluster clusters in the raange of X\n",
    "        self.centers = np.random.rand(self.n_clusters, len(Y))\n",
    "        for i in range(self.n_clusters):\n",
    "            self.centers[i] = Y[i]\n",
    "            \n",
    "            \n",
    "    def random_center2(self,Y):\n",
    "    # randomly generate n_cluster clusters in the raange of X\n",
    "        self.centers = np.random.rand(self.n_clusters, len(Y[0]))\n",
    "        for i in range(self.n_clusters):\n",
    "            self.centers[i] = Y[i]\n",
    "#         for j in range(0):\n",
    "#             Y_j_min = self.Y[:,j].min()\n",
    "#             Y_j_max = self.Y[:,j].max()\n",
    "#             self.centers[:,j] = Y_j_min + (Y_j_max - Y_j_min) * self.centers[:,j]\n",
    "            \n",
    "            \n",
    "            \n",
    "    def dist(self, point1, point2): #old one\n",
    "        return 2*(point1[0]-point2[0])**2 + 4*(point1[1]-point2[1])**2 + 3*(point1[2]-point2[2])**2\n",
    "\n",
    "    def dist2(self,point1,point2):\n",
    "        return LA.norm(point1-point2)**2\n",
    "    \n",
    "    def fit(self, Y):\n",
    "        self.Y = Y\n",
    "        self.labels = np.zeros(Y.shape[0], dtype='uint8')  # record the current labels of each sample of X\n",
    "        self.random_center(Y)\n",
    "        diff = 1\n",
    "        \n",
    "        while diff > 1e-3:\n",
    "            old_center = self.centers.copy()\n",
    "\n",
    "            # go through all samples and label them using the nearest label\n",
    "            for i in range(Y.shape[0]):\n",
    "                distance = np.zeros(self.n_clusters)\n",
    "                for j in range(self.n_clusters):\n",
    "                    distance[j] = self.dist2(Y[i], self.centers[j])\n",
    "                self.labels[i] = np.argmin(distance)\n",
    "                \n",
    "                \n",
    "            # update the centers\n",
    "            for i in range(self.n_clusters):\n",
    "                self.centers[i] = Y[self.labels==i].mean(axis=0)\n",
    "                \n",
    "\n",
    "            # update the difference\n",
    "            diff = np.linalg.norm(self.centers - old_center)\n",
    "            print(diff)\n",
    "        return self\n",
    "    \n",
    "    def fit2(self, Y):\n",
    "        self.Y = Y\n",
    "        self.labels = np.zeros(Y.shape[0], dtype='uint8')  # record the current labels of each sample of X\n",
    "        self.random_center2(Y)\n",
    "        diff = 1\n",
    "        \n",
    "        while diff > 1e-3:\n",
    "            old_center = self.centers.copy()\n",
    "\n",
    "            # go through all samples and label them using the nearest label\n",
    "            for i in range(Y.shape[0]):\n",
    "                distance = np.zeros(self.n_clusters)\n",
    "                for j in range(self.n_clusters):\n",
    "                    distance[j] = self.dist2(Y[i], self.centers[j])\n",
    "                self.labels[i] = np.argmin(distance)\n",
    "                \n",
    "                \n",
    "            # update the centers\n",
    "            for i in range(self.n_clusters):\n",
    "                self.centers[i] = Y[self.labels==i].mean(axis=0)\n",
    "                \n",
    "\n",
    "            # update the difference\n",
    "            diff = np.linalg.norm(self.centers - old_center)\n",
    "            print(diff)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, Y):\n",
    "        out = np.zeros(Y.shape)\n",
    "        for i in range(self.n_clusters):\n",
    "            out[self.labels==i] = self.centers[i]\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oneHOTENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncoding(Y, kmeans):\n",
    "    out = np.zeros((len(Y), kmeans.n_clusters))\n",
    "    for i in range(len(Y)):\n",
    "        out[i, Y[i]] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test clustering numbered data:\n",
    "    step1 : get single column\n",
    "    step2 : sort in increasing order\n",
    "    step3 : based on distribution, cut data into 4 section with equally number of candidate\n",
    "    step4 : if data is null, set isnon to be true for that column\n",
    "    step5 : append new datacol to input_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isint(value):\n",
    "  try:\n",
    "    int(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "def isfloat(value):\n",
    "  try:\n",
    "    float(value)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "def getdistriarr(df,arr_str):\n",
    "    arr = df[arr_str].values\n",
    "    di = []\n",
    "    di_og = []  #RETURN OG SINCE di will be sort\n",
    "    for i in range(len(arr)):\n",
    "        if isint(arr[i]) == True:\n",
    "            di.append(int(arr[i]))\n",
    "            di_og.append(int(arr[i]))\n",
    "        else:\n",
    "            di.append(-1)\n",
    "            di_og.append(-1)\n",
    "    di.sort()\n",
    "    ans_arr = []\n",
    "    leng = len(di)\n",
    "    ans_arr.append(di[int(leng/3)])\n",
    "#     ans_arr.append(di[int(leng/2)])\n",
    "    ans_arr.append(di[int(leng*0.66)])\n",
    "    ans_arr.append(di[int(leng-1)])\n",
    "    return di_og,ans_arr\n",
    "\n",
    "def oneHotEncoding_return_arr(att_arr,class_arr): #one \n",
    "    #add one more column for each row since last col used as flag\n",
    "    choice = len(class_arr)\n",
    "    out = np.zeros((len(att_arr),len(class_arr)+1))\n",
    "    for i in range(len(att_arr)):\n",
    "        loc = 0\n",
    "        #first jude if data is null\n",
    "        if att_arr[i] == -1:\n",
    "            out[i][-1] = 0\n",
    "            continue\n",
    "        else:\n",
    "            out[i][-1] = 1\n",
    "            for j in range(len(class_arr)):\n",
    "                if att_arr[i] == class_arr[j]:\n",
    "                    out[i][j] = 1\n",
    "                    break\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "def GET_ONE_HOT_ARRAY_ONLY_DIGIT_COLUMN(df,attr_name,og_dic):\n",
    "    att_arr,class_arr = getdistriarr(df,attr_name)\n",
    "    dic_ans = create_dic_for_classification(class_arr)\n",
    "    append_arr_of_dic_to_overall(og_dic,dic_ans,attr_name)\n",
    "\n",
    "    return oneHotEncoding_return_arr(att_arr,class_arr)\n",
    "\n",
    "def create_dic_for_classification(class_arr):\n",
    "    cur_dic = {}\n",
    "    total_len = len(class_arr)\n",
    "    for i in range(len(class_arr)):\n",
    "        cur_dic[class_arr[i]] = i\n",
    "    return cur_dic\n",
    "\n",
    "def append_arr_of_dic_to_overall(og,dic,att_name):\n",
    "    og[att_name] = dic\n",
    "    \n",
    "    \n",
    "def append_arr(old,new):\n",
    "    #both old and new has the same num of row\n",
    "    #create a new nparray\n",
    "    \n",
    "    newdim = len(old[0])+len(new[0])\n",
    "    ans = np.zeros((len(old),newdim))\n",
    "    print(ans.shape)\n",
    "    for i in range(len(old)):\n",
    "        ans[i] = np.append(old[i],new[i])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP TWO CONVERT NLP DATASET INTO VECTOR:\n",
    "    STEP1: vectorize dataset\n",
    "    STEP2: apply kmeans clustering to language data\n",
    "    STEP3: ONEHOT ENCODING\n",
    "    STEP4: APPEND CLASSIFICATION DATASET INTO old CLASSFICAITION ARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLPDATAPRO(allsentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(allsentences)\n",
    "    ans = X.toarray()\n",
    "    return ans\n",
    "\n",
    "def createtextarr_ONEHOT_ENCODING(attri_col): #ONEHOT_ENCODING\n",
    "    df2 = df[attri_col]\n",
    "    arr = df2.values\n",
    "    for i in range(len(arr)):\n",
    "        di.append(arr[i])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP THREE CONVERT TEXT_DATASET WITH FIXED classification into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processnull(arr):\n",
    "    arrnew = []\n",
    "    for i in range(len(arr)):\n",
    "        if type(arr[i]) == float and math.isnan(arr[i]) :\n",
    "            arrnew.append('Nan')\n",
    "        else:\n",
    "            arrnew.append(arr[i])\n",
    "    return arrnew\n",
    "\n",
    "def returnuniqiearr(og):\n",
    "    ar = np.asarray(og)\n",
    "    return np.unique(ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: create a function to append text to old vecto(remember to append flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: TEST NN TONIGHT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onehoted_text_fixed_classification(attribu_name,df,km_num):\n",
    "    df_cur = df[attribu_name]\n",
    "    attribute_arr = df_cur.values\n",
    "    arr_no_null = processnull(attribute_arr) #create new arr with format fit by NLP(CountVectorizer)\n",
    "    NLP_ARR = NLPDATAPRO(arr_no_null)\n",
    "    #BEFORE ONEHOT_ENCODING\n",
    "    #APPLY KMEANS CLUSTERING SINCE TOO MANY DIFFERENT CLASSIFICATION\n",
    "    KM_TMP = KMeans(km_num)\n",
    "    KM_TMP.fit2(NLP_ARR)\n",
    "    labels_arr = KM_TMP.labels\n",
    "    return labels_arr\n",
    "    \n",
    "def onehot_enco(labelarr,df,att_name):\n",
    "    og_text_arr = df[att_name].values\n",
    "    #create dic to remeber loc for each value in the uniqe arr\n",
    "    uni_arr = np.unique(labelarr)\n",
    "    loc_dic = {}\n",
    "    for i in range(len(uni_arr)):\n",
    "        loc_dic[uni_arr[i]] = i\n",
    "    #based on the lenth of dic,create onehot_encod\n",
    "    choice = len(loc_dic)\n",
    "    out = np.zeros((len(labelarr),choice+1))\n",
    "    for i in range(len(labelarr)):\n",
    "        #first junde if data is null\n",
    "        if type(og_text_arr[i])!=str and math.isnan(og_text_arr[i]) == True:\n",
    "            out[i][-1] = 0\n",
    "            continue\n",
    "        else:\n",
    "            out[i][-1] = 1\n",
    "            cur_loc = loc_dic.get(labelarr[i])\n",
    "            out[i][cur_loc] = 1\n",
    "            \n",
    "    return out,loc_dic\n",
    "\n",
    "def GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,attr_name,og_dic,km_num):\n",
    "    labels_arr = create_onehoted_text_fixed_classification(attr_name,df,km_num)\n",
    "    out_arr,loc_dic = onehot_enco(labels_arr,df,attr_name)\n",
    "    #append dic to global array of dic\n",
    "    append_arr_of_dic_to_overall(og_dic,loc_dic,attr_name)\n",
    "    return out_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL WORK:\n",
    "    1. MAINTAIN AN ARRAY OF DICTIONARY S.T EVERY DIC\n",
    "       CONTAINS MAPPING FROM CLASSIFICAITON TO ACTUAL INDEX AFTER KMEANS\n",
    "    \n",
    "    2. AFTER TRAINING, WE WILL UTILIZED THE ARR_DIC TO RECOVER CANDIDATE ANSER\n",
    "    \n",
    "    3. TODO: REWRITE COST FUNCTION OF NERUAL NETWORK\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINDING:\n",
    "    AFTER combine several attributes column, num of candidate with dinct\n",
    "    ans grows extremly fast.\n",
    "    Need to apply Kmeans clustering again to the array of candidate before\n",
    "    threw into Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE A SCRIPT TO APPEND ASSIGNED ATTRIBUTE TO INPUT COLUMN\n",
    "\n",
    "(API)INSTRUCTION:<br>\n",
    "USER WHO WANTS TO CREATE PREPROCESSED DATASET ONLY NEED TO CREATE\n",
    "ARRAY[i]<br>\n",
    "       ARRAY[i][0] = 'name of attribute'<br>\n",
    "       ARRAY[i][1] = 1 : it is a digit column<br>\n",
    "       ARRAY[i][1] = 0 : it is a NLP column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_auto_append_attri(arr_of_attribute,df,og_dic):\n",
    "    for i in range(len(arr_of_attribute)):\n",
    "        name = arr_of_attribute[i][0]\n",
    "        isdigit = arr_of_attribute[i][1]\n",
    "        if isdigit == 1:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_DIGIT_COLUMN(df,name,og_dic)\n",
    "        else:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,name,og_dic,20)\n",
    "        #append old with new\n",
    "        if i == 0:\n",
    "            old = attr_arr\n",
    "        else:\n",
    "            old = append_arr(old,attr_arr)\n",
    "#             print(old.shape)\n",
    "    return old\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_semi_auto_append_attri(arr_of_attribute,df,og_dic):\n",
    "    for i in range(len(arr_of_attribute)):\n",
    "        name = arr_of_attribute[i][0]\n",
    "        print('cur key is', name)\n",
    "        isdigit = arr_of_attribute[i][1]\n",
    "        if isdigit == 1:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_DIGIT_COLUMN(df,name,og_dic)\n",
    "        elif isdigit == 0:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_NLP_COLUMN(df,name,og_dic,20)\n",
    "        elif isdigit == 2:\n",
    "            attr_arr = GET_ONE_HOT_ARRAY_ONLY_FIXED_CHOICE_COLUMN(df,name,og_dic)\n",
    "        #append old with new\n",
    "        if i == 0:\n",
    "            old = attr_arr\n",
    "        else:\n",
    "            old = append_arr(old,attr_arr)\n",
    "#             print(old.shape)\n",
    "    return old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_choice_get_distinct_arr(df,arr_str):\n",
    "    arr = df[arr_str].values\n",
    "    uni_arr = np.unique(arr)\n",
    "    di = []\n",
    "#     di_og = [] #RETURN OG SINCE di will be sorted\n",
    "    div_arr = []\n",
    "    for i in range(len(uni_arr)):\n",
    "        if math.isnan(uni_arr[i])!=True:\n",
    "            div_arr.append(uni_arr[i])\n",
    "    for i in range(len(arr)):\n",
    "        if math.isnan(arr[i])!= True:\n",
    "            di.append(int(arr[i]))\n",
    "        else:\n",
    "            di.append(-1)\n",
    "    return di,div_arr\n",
    "\n",
    "def GET_ONE_HOT_ARRAY_ONLY_FIXED_CHOICE_COLUMN(df,attr_name,og_dic):\n",
    "    att_arr,class_arr = fix_choice_get_distinct_arr(df,attr_name)\n",
    "    print('class_arr',class_arr)\n",
    "    dic_ans = create_dic_for_classification(class_arr)\n",
    "    append_arr_of_dic_to_overall(og_dic,dic_ans,attr_name)\n",
    "    return oneHotEncoding_return_arr(att_arr,class_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Kmeans Clustering to the overal column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_whole_attribute_get_Y_for_Neural_NetWork(X,num_cluster):\n",
    "    km = KMeans(num_cluster)\n",
    "    km.fit2(X)\n",
    "    \n",
    "    #APPLY ONEHOT_ENCODING TO NEURAL_NETWORKAGAIN\n",
    "    \n",
    "    return oneHotEncoding(km.labels,km),km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLY NEURAL NETWORK TO SOLVE THIS QUESTION:\n",
    "\n",
    "REMEBER TO CONSIDER FLAG FOR EACH COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predicted_data(pred,kmeans):\n",
    "    ans = []\n",
    "    for i in range(len(pred)):\n",
    "        ans.append(kmeans.centers[np.argmax(pred[i])])\n",
    "    newarr = np.asarray(ans)\n",
    "    return newarr\n",
    "\n",
    "def find_classification_based_on_distance(pred,dic):\n",
    "    #create array with all dict keys based on insertion order\n",
    "    dicarr = list(dic.keys())\n",
    "    #isolate target awated_array used to classification\n",
    "    for i in range(len(pred)):\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "#             print('cur dic: ',cur_dict_key)\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_lengh = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_lengh  #update current cutting position\n",
    "            #restore dimension of onehotencoding for current dictionary\n",
    "            one_hot_cur_dic = np.zeros((cur_dict_lengh,cur_dict_lengh))\n",
    "#             print('before any processing cur loc is ', cur_loc, ' cur des is : ', cur_des)\n",
    "            for q in range(0,cur_dict_lengh-1):\n",
    "                one_hot_cur_dic[q][q] = 1\n",
    "                one_hot_cur_dic[q][-1] = 1\n",
    "            #isolate target awaited_classification array\n",
    "            cur_isolated_arr = pred[i][cur_loc:cur_des]\n",
    "            #compute classification with smallest distance\n",
    "            arr_store_distance= []\n",
    "            for h in range (len(one_hot_cur_dic)):\n",
    "                arr_store_distance.append(np.linalg.norm(cur_isolated_arr-one_hot_cur_dic[h]))\n",
    "            #transfer dic to arr s.t unilized argmim\n",
    "            dis_np_array = np.asarray(arr_store_distance)\n",
    "            index = np.argmin(dis_np_array)\n",
    "#             if cur_dict_key == 'big5_10':\n",
    "#                 print(index)\n",
    "#                 print('cur onehot_array is ', one_hot_cur_dic[3] )\n",
    "#                 print('cur loc is ', cur_loc, ' cur des is : ', cur_des)\n",
    "            # change value in output array\n",
    "            for k in range (cur_dict_lengh):\n",
    "                pred[i][cur_loc+k] = one_hot_cur_dic[index][k]\n",
    "                \n",
    "            cur_loc += cur_dict_lengh #update cur_loc not used in curretn loop\n",
    "\n",
    "def find_classification_based_on_prob(pred,dic):\n",
    "    dicarr = list(dic.keys())\n",
    "    for i in range(len(pred)):\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_lengh = len(cur_dict_arr)  + 1\n",
    "            cur_des += cur_dict_lengh\n",
    "            \n",
    "            cur_isolated_arr = pred[i][cur_loc:cur_des-1]\n",
    "            #find index of max value\n",
    "            index = np.argmax(cur_isolated_arr)\n",
    "            \n",
    "            for k in range(cur_dict_lengh):\n",
    "                pred[i][cur_loc+k] = 0\n",
    "            \n",
    "            pred[i][cur_loc + index] = 1\n",
    "            cur_loc +=cur_dict_lengh\n",
    "    \n",
    "            \n",
    "def create_reverse_dictionary(dic):\n",
    "    reverse = {}\n",
    "    dic_arr = list(dic.keys())\n",
    "    for i in range(len(dic_arr)):\n",
    "        cur_reverse_dict = {}\n",
    "        cur_dic_key = dic_arr[i]\n",
    "        cur_key_arr = list(dic.get(cur_dic_key).keys())\n",
    "        cur_index_arr = list(dic.get(cur_dic_key).values())\n",
    "        #reverse key_value pair in originaly dictionary\n",
    "        for j in range(len(cur_key_arr)):\n",
    "            cur_reverse_dict[cur_index_arr[j]] = cur_key_arr[j]\n",
    "        #append current dic to ans dic\n",
    "        reverse[cur_dic_key] = cur_reverse_dict\n",
    "    return reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create  function to knock out some attribute inorder to create more dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingset_generator(train,dic,arr_str):\n",
    "    #first find the location \n",
    "    #knock out\n",
    "    #return dataset\n",
    "    dicarr = list(dic.keys())\n",
    "    changed_arr = np.copy(train)\n",
    "    y_arr = np.copy(train)\n",
    "    #outermost loop : loop through those keys that need to be cleared\n",
    "    for d in range(len(arr_str)):\n",
    "        cur_key = arr_str[d]\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_length = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_length\n",
    "            #test if we find what we want\n",
    "            if cur_dict_key == cur_key:\n",
    "                for i in range(len(train)):\n",
    "                    for k in range(cur_dict_length):\n",
    "                        changed_arr[i][cur_loc + k] = 0\n",
    "                        \n",
    "                break\n",
    "            cur_loc +=cur_dict_length\n",
    "    return changed_arr,y_arr\n",
    "\n",
    "def append_row(old,new):\n",
    "    ans = np.vstack([old,new])\n",
    "    return ans\n",
    "\n",
    "\n",
    "def enlarge_dataset(train,y,dic,arr_of_arr_str):\n",
    "    list_of_new_x = []\n",
    "    list_of_new_y = []\n",
    "    new_x = np.copy(train)\n",
    "    new_y = np.copy(y)\n",
    "    for i in range(len(arr_of_arr_str)):\n",
    "        cur_str = arr_of_arr_str[i]\n",
    "        changed_arr,y_arr = trainingset_generator(train,dic,cur_str)\n",
    "        #append\n",
    "        new_x = append_row(new_x,changed_arr)\n",
    "        new_y = append_row(new_y,y_arr)\n",
    "        list_of_new_x.append(changed_arr)\n",
    "        list_of_new_y.append(y_arr)\n",
    "#     return new_x,new_y\n",
    "    return list_of_new_x,list_of_new_y\n",
    "\n",
    "def createlist_list_string(dic):\n",
    "    ans = []\n",
    "    key_arr = list(dic.keys())\n",
    "    for i in range(len(key_arr)):\n",
    "        tmp = []\n",
    "        tmp.append(key_arr[i])\n",
    "        ans.append(tmp)\n",
    "        \n",
    "    #degree 2\n",
    "#     for i in range(len(key_arr)):\n",
    "#         for j in range(i,len(key_arr)):\n",
    "#             tmp = []\n",
    "#             tmp.append(key_arr[i])\n",
    "#             tmp.append(key_arr[j])\n",
    "#             ans.append(tmp)\n",
    "    return ans\n",
    "\n",
    "def train_list_of_dataset(nn,x,y):\n",
    "    nn.fit(y,y)\n",
    "    for i in range(0,len(x)):\n",
    "        print('current iter', i)\n",
    "        nn.fit_continue(x[i],y)\n",
    "    \n",
    "    return nn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MULTI_NeuralNetwork:\n",
    "    def __init__(self, n_h,n_h2, n_iterate=10, learning_rate=1, dic = []):\n",
    "        self.n_x = None  # size of the input layer\n",
    "        self.n_h = n_h  # size of the hidden layer\n",
    "        self.n_h2= n_h2\n",
    "        self.n_y = None # size of the output layer\n",
    "        self.W1 = None\n",
    "        self.W2 = None\n",
    "        self.W3 = None\n",
    "        self.b1 = None\n",
    "        self.b2 = None\n",
    "        self.b3 = None\n",
    "        self.A1 = None\n",
    "        self.A2 = None  # sigmoid output of the second activation\n",
    "        self.A3 = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterate = n_iterate\n",
    "#         self.dic = dic\n",
    "        self.dic = dic\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        self.W1 = np.random.randn(self.n_h, self.n_x) * 0.01\n",
    "        self.b1 = np.zeros((self.n_h, 1))\n",
    "        \n",
    "        self.W2 = np.random.randn(self.n_h2, self.n_h) * 0.01\n",
    "        self.b2 = np.zeros((self.n_h2, 1))\n",
    "        \n",
    "        self.W3 = np.random.randn(self.n_y, self.n_h2) * 0.01\n",
    "        self.b3 = np.zeros((self.n_y, 1))\n",
    "    \n",
    "    \n",
    "    def MSE(self,Y):\n",
    "        cost = 0\n",
    "        for i in range(len(Y)):\n",
    "            for j in range(len(Y[0])):\n",
    "                cost+=(Y[i][j]-self.A2[i][j])**2\n",
    "        cost = cost/len(Y)\n",
    "        return cost\n",
    "        \n",
    "    def relu(self, z):\n",
    "        return z * (z > 0)\n",
    "    \n",
    "#     def softmax(self,z):\n",
    "#         exps = np.exp(z-np.max(z,axis= 1,keepdims = True))\n",
    "#         return exps/np.sum(exps,axis = 1, keepdims = True)\n",
    "    \n",
    "    \n",
    "\n",
    "#     def softmax(self,A):  \n",
    "#         expA = np.exp(A)\n",
    "#         return expA / expA.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derv(self,x):\n",
    "        return self.sigmoid(x)*(1-self.sigmoid(x))\n",
    "\n",
    "    \n",
    "    \n",
    "    def cro_entro(self,Y):\n",
    "        cost = - (Y * np.log(self.A2) + (1-Y) * np.log(1-self.A2)).mean()\n",
    "        return cost\n",
    "    \n",
    "    def compute_cost(self, Y):# cross entrophy\n",
    "        #cost = np.linalg.norm(self.A2 - Y)\n",
    "        cost = - (Y * np.log(self.A3) + (1-Y) * np.log(1-self.A3)).mean()\n",
    "#         print('shape cost is: ' ,cost.shape)\n",
    "        return np.squeeze(cost) \n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.A1 = self.sigmoid(self.W1 @ X + self.b1)\n",
    "        #dropout function for output in hidden layer\n",
    "        u1 = np.random.binomial(1,0.3,size = self.A1.shape)\n",
    "        self.A1 *= u1\n",
    "\n",
    "        self.A2 = self.sigmoid(self.W2 @ self.A1 + self.b2)\n",
    "        #dropout function for hidden layer two\n",
    "        u2 = np.random.binomial(1,0.3,size = self.A2.shape)\n",
    "        self.A2 *= u2\n",
    "        \n",
    "        self.A3 = self.sigmoid(self.W3 @self.A2 + self.b3)\n",
    "        \n",
    "\n",
    "        \n",
    "#         u2 = np.random.binomial(1,0.5,size = self.A2.shape)\n",
    "#         self.A2 *= u2\n",
    "#         self.A2 = self.soft_max_For_multi_attribute(self.A2,self.dic)\n",
    "\n",
    "        \n",
    "    \n",
    "    def backward_propagation(self, X, Y):\n",
    "        m = X.shape[1]  #num of input candidate\n",
    "\n",
    "#         dZ2 = self.A2 - Y\n",
    "# #         print('cur shape of dz2 :',dZ2.shape)\n",
    "# #         print('cur cost of cros entro', c)\n",
    "#         dW2 = dZ2 @ self.A1.T / m\n",
    "#         db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "#         dZ1 = self.W2.T @ dZ2 * (self.A1 > 0)\n",
    "#         dW1 = dZ1 @ X.T / m\n",
    "#         db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "#         self.W1 -= self.learning_rate * dW1\n",
    "#         self.b1 -= self.learning_rate * db1\n",
    "#         self.W2 -= self.learning_rate * dW2\n",
    "#         self.b2 -= self.learning_rate * db2\n",
    "        \n",
    "        dZ3 = self.A3 - Y\n",
    "        dW3 = np.dot(dZ3 , self.A2.T) / m\n",
    "        db3 = np.sum(dZ3,axis = 1, keepdims = True)/ m\n",
    "        dZ2 = np.dot(self.W3.T,  dZ3) * self.sigmoid_derv(self.A2)\n",
    "        dW2 = dZ2 @self.A1.T/m\n",
    "        db2 = np.sum(dZ2,axis = 1, keepdims = True)/ m\n",
    "        dZ1 = np.dot(self.W2.T , dZ2) * self.sigmoid_derv(self.A1)\n",
    "        dW1 = dZ1@ X.T/m\n",
    "        db1 = np.sum(dZ1,axis = 1, keepdims = True)/ m\n",
    "        \n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.b2 -= self.learning_rate * db2\n",
    "        self.W3 -= self.learning_rate * dW3\n",
    "        self.b3 -= self.learning_rate * db3\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        X, Y = X.T, Y.T\n",
    "    \n",
    "        self.n_x = X.shape[0]\n",
    "        self.n_y = Y.shape[0]\n",
    "        self.initialize_parameters()\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(0, self.n_iterate):\n",
    "            self.forward_propagation(X)\n",
    "            self.backward_propagation(X, Y)\n",
    "            if i % 10 == 0:\n",
    "                cost = self.compute_cost(Y)\n",
    "#                  self.learning_rate = 5 * cost\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    def fit_continue(self,X,Y):\n",
    "        X, Y = X.T, Y.T\n",
    "        self.n_x = X.shape[0]\n",
    "        self.n_y = Y.shape[0]\n",
    "#         if self.W1 is None:\n",
    "#             self.initialize_parameters()\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(0, self.n_iterate):\n",
    "            self.forward_propagation(X)\n",
    "            self.backward_propagation(X, Y)\n",
    "            if i % 10 == 0:\n",
    "                cost = self.compute_cost(Y)\n",
    "#                 self.learning_rate = 5 * cost\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "                \n",
    "    def predict(self, X):\n",
    "        X = X.T\n",
    "        A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        A2 = self.soft_max_For_multi_attribute(self.W2 @ self.A1 + self.b2,self.dic)\n",
    "        return A2.T\n",
    "    \n",
    "    def predict_one_layer(self,X):\n",
    "        X = X.T\n",
    "        A1 = self.relu(self.W1 @ X + self.b1)\n",
    "        return A1.T\n",
    "    \n",
    "    def soft_max_For_multi_attribute(self,arr,dic):\n",
    "        #create array with all dict keys based on insertion order\n",
    "        tmp_arr = np.copy(arr)\n",
    "        dicarr = list(dic.keys())\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        for j in range(len(dicarr)):\n",
    "            cur_dict_key = dicarr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_dict_length = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_length    \n",
    "            #find the targeted array need to perform softmax\n",
    "            #create a dic to contain the location of those onehotencoding attribute\n",
    "            loc_dic = []\n",
    "            for i in range(cur_dict_length-1):\n",
    "                loc_dic.append(cur_loc+i)\n",
    "            #find targed array\n",
    "            target = tmp_arr[:,loc_dic]\n",
    "            #get softmaxed arr\n",
    "            arr_soft_max = self.api_softmax(target)\n",
    "            #reassig value back to orignal arr\n",
    "            tmp_arr[:,loc_dic] = arr_soft_max\n",
    "            #update cur_loc\n",
    "            cur_loc +=cur_dict_length\n",
    "            \n",
    "        return tmp_arr\n",
    "\n",
    "    def api_softmax(self,z):\n",
    "        exps = np.exp(z)\n",
    "        return exps/np.sum(exps,axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create testing attribut arra\n",
    "arr = []\n",
    "og_arr = []\n",
    "og_train = {} #init global dictionary\n",
    "og_pred = {}\n",
    "arr.append(['age',1])\n",
    "arr.append(['mood_01',2]) #Today I generally feel\n",
    "arr.append(['mood_02',2])\n",
    "# arr.append(['major',0])\n",
    "arr.append(['big5_01',2]) #I see myself as: Extraverted, enthusiastic.\n",
    "arr.append(['big5_02',2]) #I see myself as: Critical, quarrelsome.\n",
    "arr.append(['big5_03',2]) #I see myself as: Dependable, self-disciplined.\n",
    "arr.append(['big5_04',2]) #I see myself as: Anxious, easily upset.\n",
    "arr.append(['big5_05',2]) #I see myself as: Open to new experiences, complex.\n",
    "arr.append(['big5_06',2]) #I see myself as: Reserved, quiet.\n",
    "arr.append(['big5_07',2]) #I see myself as: Sympathetic, warm.\n",
    "arr.append(['big5_08',2]) #I see myself as: Disorganized, careless.\n",
    "arr.append(['big5_09',2]) #I see myself as: Calm, emotionally stable.\n",
    "arr.append(['big5_10',2]) #I see myself as: Conventional, uncreative.\n",
    "arr.append(['intrinsic_01',2])\n",
    "arr.append(['intrinsic_02',2])\n",
    "arr.append(['intrinsic_03',2])\n",
    "arr.append(['intrinsic_04',2])\n",
    "arr.append(['intrinsic_05',2])\n",
    "arr.append(['intrinsic_06',2])\n",
    "arr.append(['intrinsic_07',2])\n",
    "arr.append(['intrinsic_08',2])\n",
    "arr.append(['intrinsic_09',2])\n",
    "arr.append(['intrinsic_10',2])\n",
    "arr.append(['intrinsic_11',2])\n",
    "arr.append(['intrinsic_12',2])\n",
    "arr.append(['intrinsic_13',2])\n",
    "arr.append(['intrinsic_14',2])\n",
    "arr.append(['intrinsic_15',2])\n",
    "arr.append(['mcfiller1',2])\n",
    "arr.append(['mcfiller2',2])\n",
    "arr.append(['mcfiller3',2])\n",
    "arr.append(['mcmost1',2])\n",
    "arr.append(['mcmost2',2])\n",
    "arr.append(['mcmost3',2])\n",
    "arr.append(['mcmost4',2])\n",
    "arr.append(['mcmost5',2])\n",
    "arr.append(['mcsome1',2])\n",
    "arr.append(['mcsome2',2])\n",
    "arr.append(['mcsome3',2])\n",
    "arr.append(['mcsome4',2])\n",
    "arr.append(['mcsome5',2])\n",
    "arr.append(['mcdv1',2])\n",
    "arr.append(['mcdv2',2])\n",
    "arr.append(['pate_01',2])\n",
    "arr.append(['pate_02',2])\n",
    "arr.append(['pate_03',2])\n",
    "arr.append(['pate_04',2])\n",
    "arr.append(['pate_05',2])\n",
    "arr.append(['stress_01',2])\n",
    "arr.append(['stress_02',2])\n",
    "arr.append(['stress_03',2])\n",
    "arr.append(['stress_04',2])\n",
    "arr.append(['nfc_01',2])\n",
    "arr.append(['nfc_02',2])\n",
    "arr.append(['nfc_03',2])\n",
    "arr.append(['nfc_04',2])\n",
    "arr.append(['nfc_05',2])\n",
    "arr.append(['nfc_06',2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_df = df3.iloc[:100]\n",
    "trainning_df = df3\n",
    "# trainning_df = trainning_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur key is age\n",
      "cur key is mood_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 12)\n",
      "cur key is mood_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 20)\n",
      "cur key is big5_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 28)\n",
      "cur key is big5_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 36)\n",
      "cur key is big5_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 44)\n",
      "cur key is big5_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 52)\n",
      "cur key is big5_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 60)\n",
      "cur key is big5_06\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 68)\n",
      "cur key is big5_07\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 76)\n",
      "cur key is big5_08\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 84)\n",
      "cur key is big5_09\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 92)\n",
      "cur key is big5_10\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "(2998, 100)\n",
      "cur key is intrinsic_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 105)\n",
      "cur key is intrinsic_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 110)\n",
      "cur key is intrinsic_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 54.0]\n",
      "(2998, 116)\n",
      "cur key is intrinsic_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 77.0]\n",
      "(2998, 122)\n",
      "cur key is intrinsic_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 128)\n",
      "cur key is intrinsic_06\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 133)\n",
      "cur key is intrinsic_07\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 139)\n",
      "cur key is intrinsic_08\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 145)\n",
      "cur key is intrinsic_09\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 150)\n",
      "cur key is intrinsic_10\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 155)\n",
      "cur key is intrinsic_11\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 161)\n",
      "cur key is intrinsic_12\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 166)\n",
      "cur key is intrinsic_13\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 171)\n",
      "cur key is intrinsic_14\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 7.0]\n",
      "(2998, 177)\n",
      "cur key is intrinsic_15\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 182)\n",
      "cur key is mcfiller1\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 187)\n",
      "cur key is mcfiller2\n",
      "class_arr [0.0, 1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 193)\n",
      "cur key is mcfiller3\n",
      "class_arr [1.0, 2.0, 3.0, 4.0]\n",
      "(2998, 198)\n",
      "cur key is mcmost1\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 201)\n",
      "cur key is mcmost2\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 204)\n",
      "cur key is mcmost3\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 207)\n",
      "cur key is mcmost4\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 210)\n",
      "cur key is mcmost5\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 213)\n",
      "cur key is mcsome1\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 216)\n",
      "cur key is mcsome2\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 219)\n",
      "cur key is mcsome3\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 222)\n",
      "cur key is mcsome4\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 225)\n",
      "cur key is mcsome5\n",
      "class_arr [1.0, 2.0]\n",
      "(2998, 228)\n",
      "cur key is mcdv1\n",
      "class_arr [-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0]\n",
      "(2998, 236)\n",
      "cur key is mcdv2\n",
      "class_arr [-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0]\n",
      "(2998, 244)\n",
      "cur key is pate_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 250)\n",
      "cur key is pate_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 256)\n",
      "cur key is pate_03\n",
      "class_arr [1.0, 2.0, 3.0]\n",
      "(2998, 260)\n",
      "cur key is pate_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "(2998, 267)\n",
      "cur key is pate_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "(2998, 274)\n",
      "cur key is stress_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 280)\n",
      "cur key is stress_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 286)\n",
      "cur key is stress_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 292)\n",
      "cur key is stress_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 298)\n",
      "cur key is nfc_01\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 304)\n",
      "cur key is nfc_02\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 310)\n",
      "cur key is nfc_03\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 316)\n",
      "cur key is nfc_04\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 322)\n",
      "cur key is nfc_05\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 328)\n",
      "cur key is nfc_06\n",
      "class_arr [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "(2998, 334)\n"
     ]
    }
   ],
   "source": [
    "# X_train = semi_auto_append_attri(arr,predict_df,og)\n",
    "X_train = new_semi_auto_append_attri(arr,trainning_df,og_pred)\n",
    "# print('start training dataset')\n",
    "# X_tra = semi_auto_append_attri(arr,trainning_df,og_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = X_train[:2900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice nparr\n",
    "x_pred = X_train[2900:]\n",
    "x_real_train = X_train[:2900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = createlist_list_string(og_pred)\n",
    "x,y = enlarge_dataset(X_train_new,X_train_new,og_pred,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MULTI_NeuralNetwork(200,4000,200,0.1,og_pred) #size of input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.696630\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train_new,X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.694211\n",
      "Cost after iteration 10: 0.392252\n",
      "Cost after iteration 20: 0.391285\n",
      "Cost after iteration 30: 0.391039\n",
      "Cost after iteration 40: 0.390819\n",
      "Cost after iteration 50: 0.390767\n",
      "Cost after iteration 60: 0.390597\n",
      "Cost after iteration 70: 0.390200\n",
      "current iter 0\n",
      "Cost after iteration 0: 0.389670\n",
      "Cost after iteration 10: 0.388771\n",
      "Cost after iteration 20: 0.386923\n",
      "Cost after iteration 30: 0.384326\n",
      "Cost after iteration 40: 0.382311\n",
      "Cost after iteration 50: 0.382244\n",
      "Cost after iteration 60: 0.383324\n",
      "Cost after iteration 70: 0.384738\n",
      "current iter 1\n",
      "Cost after iteration 0: 0.386019\n",
      "Cost after iteration 10: 0.387071\n",
      "Cost after iteration 20: 0.388261\n",
      "Cost after iteration 30: 0.389106\n",
      "Cost after iteration 40: 0.389559\n",
      "Cost after iteration 50: 0.389673\n",
      "Cost after iteration 60: 0.389550\n",
      "Cost after iteration 70: 0.389299\n",
      "current iter 2\n",
      "Cost after iteration 0: 0.388726\n",
      "Cost after iteration 10: 0.388349\n",
      "Cost after iteration 20: 0.387119\n",
      "Cost after iteration 30: 0.379648\n",
      "Cost after iteration 40: 0.346179\n",
      "Cost after iteration 50: 0.319874\n",
      "Cost after iteration 60: 0.311730\n",
      "Cost after iteration 70: 0.307705\n",
      "current iter 3\n",
      "Cost after iteration 0: 0.305866\n",
      "Cost after iteration 10: 0.304463\n",
      "Cost after iteration 20: 0.303755\n",
      "Cost after iteration 30: 0.303372\n",
      "Cost after iteration 40: 0.302781\n",
      "Cost after iteration 50: 0.302381\n",
      "Cost after iteration 60: 0.302323\n",
      "Cost after iteration 70: 0.302105\n",
      "current iter 4\n",
      "Cost after iteration 0: 0.301926\n",
      "Cost after iteration 10: 0.301769\n",
      "Cost after iteration 20: 0.301763\n",
      "Cost after iteration 30: 0.301658\n",
      "Cost after iteration 40: 0.301481\n",
      "Cost after iteration 50: 0.301367\n",
      "Cost after iteration 60: 0.301344\n",
      "Cost after iteration 70: 0.301258\n",
      "current iter 5\n",
      "Cost after iteration 0: 0.301203\n",
      "Cost after iteration 10: 0.301113\n",
      "Cost after iteration 20: 0.301053\n",
      "Cost after iteration 30: 0.301144\n",
      "Cost after iteration 40: 0.301087\n",
      "Cost after iteration 50: 0.301056\n",
      "Cost after iteration 60: 0.301046\n",
      "Cost after iteration 70: 0.301048\n",
      "current iter 6\n",
      "Cost after iteration 0: 0.301017\n",
      "Cost after iteration 10: 0.300994\n",
      "Cost after iteration 20: 0.300930\n",
      "Cost after iteration 30: 0.300760\n",
      "Cost after iteration 40: 0.300893\n",
      "Cost after iteration 50: 0.300845\n",
      "Cost after iteration 60: 0.300891\n",
      "Cost after iteration 70: 0.300908\n",
      "current iter 7\n",
      "Cost after iteration 0: 0.300702\n",
      "Cost after iteration 10: 0.300831\n",
      "Cost after iteration 20: 0.300858\n",
      "Cost after iteration 30: 0.300740\n",
      "Cost after iteration 40: 0.300740\n",
      "Cost after iteration 50: 0.300772\n",
      "Cost after iteration 60: 0.300766\n",
      "Cost after iteration 70: 0.300625\n",
      "current iter 8\n",
      "Cost after iteration 0: 0.300726\n",
      "Cost after iteration 10: 0.300702\n",
      "Cost after iteration 20: 0.300650\n",
      "Cost after iteration 30: 0.300774\n",
      "Cost after iteration 40: 0.300676\n",
      "Cost after iteration 50: 0.300641\n",
      "Cost after iteration 60: 0.300845\n",
      "Cost after iteration 70: 0.300664\n",
      "current iter 9\n",
      "Cost after iteration 0: 0.300570\n",
      "Cost after iteration 10: 0.300668\n",
      "Cost after iteration 20: 0.300708\n",
      "Cost after iteration 30: 0.300656\n",
      "Cost after iteration 40: 0.300600\n",
      "Cost after iteration 50: 0.300662\n",
      "Cost after iteration 60: 0.300621\n",
      "Cost after iteration 70: 0.300614\n",
      "current iter 10\n",
      "Cost after iteration 0: 0.300629\n",
      "Cost after iteration 10: 0.300656\n",
      "Cost after iteration 20: 0.300603\n",
      "Cost after iteration 30: 0.300687\n",
      "Cost after iteration 40: 0.300610\n",
      "Cost after iteration 50: 0.300552\n",
      "Cost after iteration 60: 0.300479\n",
      "Cost after iteration 70: 0.300637\n",
      "current iter 11\n",
      "Cost after iteration 0: 0.300421\n",
      "Cost after iteration 10: 0.300526\n",
      "Cost after iteration 20: 0.300660\n",
      "Cost after iteration 30: 0.300666\n",
      "Cost after iteration 40: 0.300731\n",
      "Cost after iteration 50: 0.300601\n",
      "Cost after iteration 60: 0.300313\n",
      "Cost after iteration 70: 0.300540\n",
      "current iter 12\n",
      "Cost after iteration 0: 0.300535\n",
      "Cost after iteration 10: 0.300618\n",
      "Cost after iteration 20: 0.300558\n",
      "Cost after iteration 30: 0.300694\n",
      "Cost after iteration 40: 0.300558\n",
      "Cost after iteration 50: 0.300493\n",
      "Cost after iteration 60: 0.300637\n",
      "Cost after iteration 70: 0.300678\n",
      "current iter 13\n",
      "Cost after iteration 0: 0.300568\n",
      "Cost after iteration 10: 0.300492\n",
      "Cost after iteration 20: 0.300613\n",
      "Cost after iteration 30: 0.300559\n",
      "Cost after iteration 40: 0.300594\n",
      "Cost after iteration 50: 0.300522\n",
      "Cost after iteration 60: 0.300519\n",
      "Cost after iteration 70: 0.300653\n",
      "current iter 14\n",
      "Cost after iteration 0: 0.300485\n",
      "Cost after iteration 10: 0.300540\n",
      "Cost after iteration 20: 0.300581\n",
      "Cost after iteration 30: 0.300466\n",
      "Cost after iteration 40: 0.300478\n",
      "Cost after iteration 50: 0.300449\n",
      "Cost after iteration 60: 0.300567\n",
      "Cost after iteration 70: 0.300532\n",
      "current iter 15\n",
      "Cost after iteration 0: 0.300464\n",
      "Cost after iteration 10: 0.300601\n",
      "Cost after iteration 20: 0.300563\n",
      "Cost after iteration 30: 0.300461\n",
      "Cost after iteration 40: 0.300522\n",
      "Cost after iteration 50: 0.300484\n",
      "Cost after iteration 60: 0.300599\n",
      "Cost after iteration 70: 0.300628\n",
      "current iter 16\n",
      "Cost after iteration 0: 0.300506\n",
      "Cost after iteration 10: 0.300263\n",
      "Cost after iteration 20: 0.300541\n",
      "Cost after iteration 30: 0.300432\n",
      "Cost after iteration 40: 0.300402\n",
      "Cost after iteration 50: 0.300466\n",
      "Cost after iteration 60: 0.300473\n",
      "Cost after iteration 70: 0.300360\n",
      "current iter 17\n",
      "Cost after iteration 0: 0.300522\n",
      "Cost after iteration 10: 0.300475\n",
      "Cost after iteration 20: 0.300505\n",
      "Cost after iteration 30: 0.300511\n",
      "Cost after iteration 40: 0.300317\n",
      "Cost after iteration 50: 0.300364\n",
      "Cost after iteration 60: 0.300420\n",
      "Cost after iteration 70: 0.300628\n",
      "current iter 18\n",
      "Cost after iteration 0: 0.300354\n",
      "Cost after iteration 10: 0.300460\n",
      "Cost after iteration 20: 0.300518\n",
      "Cost after iteration 30: 0.300663\n",
      "Cost after iteration 40: 0.300558\n",
      "Cost after iteration 50: 0.300493\n",
      "Cost after iteration 60: 0.300574\n",
      "Cost after iteration 70: 0.300341\n",
      "current iter 19\n",
      "Cost after iteration 0: 0.300342\n",
      "Cost after iteration 10: 0.300505\n",
      "Cost after iteration 20: 0.300381\n",
      "Cost after iteration 30: 0.300466\n",
      "Cost after iteration 40: 0.300393\n",
      "Cost after iteration 50: 0.300433\n",
      "Cost after iteration 60: 0.300539\n",
      "Cost after iteration 70: 0.300374\n",
      "current iter 20\n",
      "Cost after iteration 0: 0.300484\n",
      "Cost after iteration 10: 0.300440\n",
      "Cost after iteration 20: 0.300483\n",
      "Cost after iteration 30: 0.300358\n",
      "Cost after iteration 40: 0.300516\n",
      "Cost after iteration 50: 0.300480\n",
      "Cost after iteration 60: 0.300366\n",
      "Cost after iteration 70: 0.300397\n",
      "current iter 21\n",
      "Cost after iteration 0: 0.300423\n",
      "Cost after iteration 10: 0.300474\n",
      "Cost after iteration 20: 0.300416\n",
      "Cost after iteration 30: 0.300407\n",
      "Cost after iteration 40: 0.300339\n",
      "Cost after iteration 50: 0.300527\n",
      "Cost after iteration 60: 0.300329\n",
      "Cost after iteration 70: 0.300355\n",
      "current iter 22\n",
      "Cost after iteration 0: 0.300381\n",
      "Cost after iteration 10: 0.300394\n",
      "Cost after iteration 20: 0.300255\n",
      "Cost after iteration 30: 0.300426\n",
      "Cost after iteration 40: 0.300460\n",
      "Cost after iteration 50: 0.300294\n",
      "Cost after iteration 60: 0.300407\n",
      "Cost after iteration 70: 0.300338\n",
      "current iter 23\n",
      "Cost after iteration 0: 0.300239\n",
      "Cost after iteration 10: 0.300408\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-38469ffe2f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_list_of_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-90c9084ff894>\u001b[0m in \u001b[0;36mtrain_list_of_dataset\u001b[0;34m(nn, x, y)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'current iter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_continue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-f2b7d10634ec>\u001b[0m in \u001b[0;36mfit_continue\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iterate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-f2b7d10634ec>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;31m#dropout function for output in hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-f2b7d10634ec>\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msigmoid_derv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mm = train_list_of_dataset(nn,x,X_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ans = nn.predict(x_real_train)\n",
    "find_classification_based_on_prob(ans,og_pred)\n",
    "print('unique ans', len(np.unique(ans,axis = 0)))\n",
    "print('unique input',len(np.unique(x_real_train,axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.57379310344828\n",
      "total count of same is 0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "p = []\n",
    "for i in range(len(x_real_train)):\n",
    "    for j in range(len(x_real_train[0])):\n",
    "        if x_real_train[i][j]!=ans[i][j]:\n",
    "            count+=1\n",
    "           \n",
    "        \n",
    "print(count/len(x_real_train))\n",
    "count = 0\n",
    "\n",
    "for i in range(len(x_real_train)):\n",
    "    issame = True\n",
    "    for j in range(len(x_real_train[0])):\n",
    "        if x_real_train[i][j]!=ans[i][j]:\n",
    "            issame = False\n",
    "            continue\n",
    "    if issame == True:    \n",
    "        count+=1\n",
    "print('total count of same is',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique ans 2900\n",
      "unique input 2396\n"
     ]
    }
   ],
   "source": [
    "find_classification_based_on_prob(ans,og_pred)\n",
    "print('unique ans', len(np.unique(ans,axis = 0)))\n",
    "print('unique input',len(np.unique(x_real_train,axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-399-e9ce81a5d2d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# find_classification_based_on_distance(ans,og_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfind_classification_based_on_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mog_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unique ans'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unique input'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-383-4cf5ef133f36>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoft_max_For_multi_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "ans = nn.predict(x)\n",
    "# find_classification_based_on_distance(ans,og_pred)\n",
    "find_classification_based_on_distance(ans,og_pred)\n",
    "print('unique ans', len(np.unique(ans,axis = 0)))\n",
    "print('unique input',len(np.unique(x,axis = 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.692850\n",
      "Cost after iteration 10: 0.563198\n",
      "Cost after iteration 20: 0.377452\n",
      "Cost after iteration 30: 0.334216\n",
      "Cost after iteration 40: 0.298466\n",
      "Cost after iteration 50: 0.280331\n",
      "Cost after iteration 60: 0.265899\n",
      "Cost after iteration 70: 0.256006\n",
      "Cost after iteration 80: 0.250119\n",
      "Cost after iteration 90: 0.242134\n",
      "Cost after iteration 100: 0.239535\n",
      "Cost after iteration 110: 0.236576\n",
      "Cost after iteration 120: 0.232729\n",
      "Cost after iteration 130: 0.233012\n",
      "Cost after iteration 140: 0.219288\n",
      "Cost after iteration 150: 0.212750\n",
      "Cost after iteration 160: 0.211935\n",
      "Cost after iteration 170: 0.205825\n",
      "Cost after iteration 180: 0.202364\n",
      "Cost after iteration 190: 0.202130\n",
      "current iter 0\n",
      "Cost after iteration 0: 0.199373\n",
      "Cost after iteration 10: 0.186431\n",
      "Cost after iteration 20: 0.182670\n",
      "Cost after iteration 30: 0.185291\n",
      "Cost after iteration 40: 0.172350\n",
      "Cost after iteration 50: 0.174810\n",
      "Cost after iteration 60: 0.168058\n",
      "Cost after iteration 70: 0.159936\n",
      "Cost after iteration 80: 0.155574\n",
      "Cost after iteration 90: 0.154369\n",
      "Cost after iteration 100: 0.150118\n",
      "Cost after iteration 110: 0.147205\n",
      "Cost after iteration 120: 0.142856\n",
      "Cost after iteration 130: 0.141013\n",
      "Cost after iteration 140: 0.134966\n",
      "Cost after iteration 150: 0.133015\n",
      "Cost after iteration 160: 0.129474\n",
      "Cost after iteration 170: 0.123468\n",
      "Cost after iteration 180: 0.123735\n",
      "Cost after iteration 190: 0.120309\n",
      "current iter 1\n",
      "Cost after iteration 0: 0.117578\n",
      "Cost after iteration 10: 0.115870\n",
      "Cost after iteration 20: 0.115462\n",
      "Cost after iteration 30: 0.113587\n",
      "Cost after iteration 40: 0.108992\n",
      "Cost after iteration 50: 0.104809\n",
      "Cost after iteration 60: 0.100438\n",
      "Cost after iteration 70: 0.099709\n",
      "Cost after iteration 80: 0.098546\n",
      "Cost after iteration 90: 0.096337\n",
      "Cost after iteration 100: 0.092954\n",
      "Cost after iteration 110: 0.091654\n",
      "Cost after iteration 120: 0.088464\n",
      "Cost after iteration 130: 0.086759\n",
      "Cost after iteration 140: 0.085389\n",
      "Cost after iteration 150: 0.084977\n",
      "Cost after iteration 160: 0.081835\n",
      "Cost after iteration 170: 0.080318\n",
      "Cost after iteration 180: 0.078791\n",
      "Cost after iteration 190: 0.077853\n",
      "current iter 2\n",
      "Cost after iteration 0: 0.080190\n",
      "Cost after iteration 10: 0.082534\n",
      "Cost after iteration 20: 0.080483\n",
      "Cost after iteration 30: 0.078591\n",
      "Cost after iteration 40: 0.076187\n",
      "Cost after iteration 50: 0.074632\n",
      "Cost after iteration 60: 0.072958\n",
      "Cost after iteration 70: 0.071633\n",
      "Cost after iteration 80: 0.071049\n",
      "Cost after iteration 90: 0.069453\n",
      "Cost after iteration 100: 0.068280\n",
      "Cost after iteration 110: 0.067561\n",
      "Cost after iteration 120: 0.066231\n",
      "Cost after iteration 130: 0.065707\n",
      "Cost after iteration 140: 0.064580\n",
      "Cost after iteration 150: 0.064061\n",
      "Cost after iteration 160: 0.064446\n",
      "Cost after iteration 170: 0.062986\n",
      "Cost after iteration 180: 0.061921\n",
      "Cost after iteration 190: 0.060911\n",
      "current iter 3\n",
      "Cost after iteration 0: 0.062449\n",
      "Cost after iteration 10: 0.063551\n",
      "Cost after iteration 20: 0.061498\n",
      "Cost after iteration 30: 0.060731\n",
      "Cost after iteration 40: 0.061455\n",
      "Cost after iteration 50: 0.058878\n",
      "Cost after iteration 60: 0.059333\n",
      "Cost after iteration 70: 0.057097\n",
      "Cost after iteration 80: 0.057470\n",
      "Cost after iteration 90: 0.055674\n",
      "Cost after iteration 100: 0.055715\n",
      "Cost after iteration 110: 0.054919\n",
      "Cost after iteration 120: 0.054664\n",
      "Cost after iteration 130: 0.054002\n",
      "Cost after iteration 140: 0.052935\n",
      "Cost after iteration 150: 0.052316\n",
      "Cost after iteration 160: 0.051815\n",
      "Cost after iteration 170: 0.051797\n",
      "Cost after iteration 180: 0.050801\n",
      "Cost after iteration 190: 0.050540\n",
      "current iter 4\n",
      "Cost after iteration 0: 0.051338\n",
      "Cost after iteration 10: 0.060778\n",
      "Cost after iteration 20: 0.051495\n",
      "Cost after iteration 30: 0.052622\n",
      "Cost after iteration 40: 0.051723\n",
      "Cost after iteration 50: 0.050686\n",
      "Cost after iteration 60: 0.049989\n",
      "Cost after iteration 70: 0.049108\n",
      "Cost after iteration 80: 0.048653\n",
      "Cost after iteration 90: 0.048795\n",
      "Cost after iteration 100: 0.047477\n",
      "Cost after iteration 110: 0.046733\n",
      "Cost after iteration 120: 0.045744\n",
      "Cost after iteration 130: 0.046227\n",
      "Cost after iteration 140: 0.045660\n",
      "Cost after iteration 150: 0.044985\n",
      "Cost after iteration 160: 0.044211\n",
      "Cost after iteration 170: 0.044044\n",
      "Cost after iteration 180: 0.043523\n",
      "Cost after iteration 190: 0.043000\n",
      "current iter 5\n",
      "Cost after iteration 0: 0.045792\n",
      "Cost after iteration 10: 0.045687\n",
      "Cost after iteration 20: 0.044706\n",
      "Cost after iteration 30: 0.044958\n",
      "Cost after iteration 40: 0.043586\n",
      "Cost after iteration 50: 0.043576\n",
      "Cost after iteration 60: 0.042390\n",
      "Cost after iteration 70: 0.042279\n",
      "Cost after iteration 80: 0.041877\n",
      "Cost after iteration 90: 0.041052\n",
      "Cost after iteration 100: 0.041003\n",
      "Cost after iteration 110: 0.040375\n",
      "Cost after iteration 120: 0.040151\n",
      "Cost after iteration 130: 0.039474\n",
      "Cost after iteration 140: 0.039316\n",
      "Cost after iteration 150: 0.038997\n",
      "Cost after iteration 160: 0.038925\n",
      "Cost after iteration 170: 0.038130\n",
      "Cost after iteration 180: 0.037971\n",
      "Cost after iteration 190: 0.037437\n",
      "current iter 6\n",
      "Cost after iteration 0: 0.039149\n",
      "Cost after iteration 10: 0.044739\n",
      "Cost after iteration 20: 0.040762\n",
      "Cost after iteration 30: 0.039603\n",
      "Cost after iteration 40: 0.039306\n",
      "Cost after iteration 50: 0.039157\n",
      "Cost after iteration 60: 0.038378\n",
      "Cost after iteration 70: 0.038136\n",
      "Cost after iteration 80: 0.037436\n",
      "Cost after iteration 90: 0.037347\n",
      "Cost after iteration 100: 0.037620\n",
      "Cost after iteration 110: 0.037269\n",
      "Cost after iteration 120: 0.036688\n",
      "Cost after iteration 130: 0.035882\n",
      "Cost after iteration 140: 0.035175\n",
      "Cost after iteration 150: 0.034915\n",
      "Cost after iteration 160: 0.034262\n",
      "Cost after iteration 170: 0.035214\n",
      "Cost after iteration 180: 0.034534\n",
      "Cost after iteration 190: 0.033978\n",
      "current iter 7\n",
      "Cost after iteration 0: 0.037281\n",
      "Cost after iteration 10: 0.037323\n",
      "Cost after iteration 20: 0.036996\n",
      "Cost after iteration 30: 0.036301\n",
      "Cost after iteration 40: 0.035337\n",
      "Cost after iteration 50: 0.035632\n",
      "Cost after iteration 60: 0.033575\n",
      "Cost after iteration 70: 0.034927\n",
      "Cost after iteration 80: 0.033625\n",
      "Cost after iteration 90: 0.033509\n",
      "Cost after iteration 100: 0.033348\n",
      "Cost after iteration 110: 0.033012\n",
      "Cost after iteration 120: 0.032590\n",
      "Cost after iteration 130: 0.032923\n",
      "Cost after iteration 140: 0.032399\n",
      "Cost after iteration 150: 0.031658\n",
      "Cost after iteration 160: 0.031574\n",
      "Cost after iteration 170: 0.031489\n",
      "Cost after iteration 180: 0.030768\n",
      "Cost after iteration 190: 0.030791\n",
      "current iter 8\n",
      "Cost after iteration 0: 0.032927\n",
      "Cost after iteration 10: 0.039276\n",
      "Cost after iteration 20: 0.036931\n",
      "Cost after iteration 30: 0.034657\n",
      "Cost after iteration 40: 0.033467\n",
      "Cost after iteration 50: 0.033507\n",
      "Cost after iteration 60: 0.032733\n",
      "Cost after iteration 70: 0.031867\n",
      "Cost after iteration 80: 0.031650\n",
      "Cost after iteration 90: 0.031115\n",
      "Cost after iteration 100: 0.030437\n",
      "Cost after iteration 110: 0.030021\n",
      "Cost after iteration 120: 0.029867\n",
      "Cost after iteration 130: 0.031693\n",
      "Cost after iteration 140: 0.030620\n",
      "Cost after iteration 150: 0.029846\n",
      "Cost after iteration 160: 0.029644\n",
      "Cost after iteration 170: 0.028930\n",
      "Cost after iteration 180: 0.028771\n",
      "Cost after iteration 190: 0.028592\n",
      "current iter 9\n",
      "Cost after iteration 0: 0.031310\n",
      "Cost after iteration 10: 0.030217\n",
      "Cost after iteration 20: 0.031813\n",
      "Cost after iteration 30: 0.030728\n",
      "Cost after iteration 40: 0.029850\n",
      "Cost after iteration 50: 0.030094\n",
      "Cost after iteration 60: 0.029433\n",
      "Cost after iteration 70: 0.028866\n",
      "Cost after iteration 80: 0.028485\n",
      "Cost after iteration 90: 0.027549\n",
      "Cost after iteration 100: 0.027821\n",
      "Cost after iteration 110: 0.027335\n",
      "Cost after iteration 120: 0.027443\n",
      "Cost after iteration 130: 0.026992\n",
      "Cost after iteration 140: 0.027134\n",
      "Cost after iteration 150: 0.026847\n",
      "Cost after iteration 160: 0.026453\n",
      "Cost after iteration 170: 0.026347\n",
      "Cost after iteration 180: 0.026463\n",
      "Cost after iteration 190: 0.027243\n",
      "current iter 10\n",
      "Cost after iteration 0: 0.028941\n",
      "Cost after iteration 10: 0.031775\n",
      "Cost after iteration 20: 0.028803\n",
      "Cost after iteration 30: 0.027429\n",
      "Cost after iteration 40: 0.028309\n",
      "Cost after iteration 50: 0.028349\n",
      "Cost after iteration 60: 0.027669\n",
      "Cost after iteration 70: 0.026926\n",
      "Cost after iteration 80: 0.026809\n",
      "Cost after iteration 90: 0.026686\n",
      "Cost after iteration 100: 0.026193\n",
      "Cost after iteration 110: 0.025982\n",
      "Cost after iteration 120: 0.026153\n",
      "Cost after iteration 130: 0.025304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 140: 0.025372\n",
      "Cost after iteration 150: 0.024834\n",
      "Cost after iteration 160: 0.024913\n",
      "Cost after iteration 170: 0.024384\n",
      "Cost after iteration 180: 0.024739\n",
      "Cost after iteration 190: 0.024728\n",
      "current iter 11\n",
      "Cost after iteration 0: 0.028415\n",
      "Cost after iteration 10: 0.028383\n",
      "Cost after iteration 20: 0.027013\n",
      "Cost after iteration 30: 0.026648\n",
      "Cost after iteration 40: 0.026425\n",
      "Cost after iteration 50: 0.026516\n",
      "Cost after iteration 60: 0.025870\n",
      "Cost after iteration 70: 0.025402\n",
      "Cost after iteration 80: 0.025335\n",
      "Cost after iteration 90: 0.025164\n",
      "Cost after iteration 100: 0.024770\n",
      "Cost after iteration 110: 0.025351\n",
      "Cost after iteration 120: 0.024829\n",
      "Cost after iteration 130: 0.024505\n",
      "Cost after iteration 140: 0.024324\n",
      "Cost after iteration 150: 0.024314\n",
      "Cost after iteration 160: 0.024146\n",
      "Cost after iteration 170: 0.023846\n",
      "Cost after iteration 180: 0.023947\n",
      "Cost after iteration 190: 0.023503\n",
      "current iter 12\n",
      "Cost after iteration 0: 0.023533\n",
      "Cost after iteration 10: 0.027522\n",
      "Cost after iteration 20: 0.024180\n",
      "Cost after iteration 30: 0.023777\n",
      "Cost after iteration 40: 0.023833\n",
      "Cost after iteration 50: 0.023158\n",
      "Cost after iteration 60: 0.022487\n",
      "Cost after iteration 70: 0.022187\n",
      "Cost after iteration 80: 0.022134\n",
      "Cost after iteration 90: 0.021708\n",
      "Cost after iteration 100: 0.021656\n",
      "Cost after iteration 110: 0.021265\n",
      "Cost after iteration 120: 0.021372\n",
      "Cost after iteration 130: 0.020979\n",
      "Cost after iteration 140: 0.020731\n",
      "Cost after iteration 150: 0.020520\n",
      "Cost after iteration 160: 0.020126\n",
      "Cost after iteration 170: 0.020342\n",
      "Cost after iteration 180: 0.020107\n",
      "Cost after iteration 190: 0.020073\n",
      "current iter 13\n",
      "Cost after iteration 0: 0.021803\n",
      "Cost after iteration 10: 0.026902\n",
      "Cost after iteration 20: 0.023815\n",
      "Cost after iteration 30: 0.022731\n",
      "Cost after iteration 40: 0.022495\n",
      "Cost after iteration 50: 0.021880\n",
      "Cost after iteration 60: 0.021353\n",
      "Cost after iteration 70: 0.020604\n",
      "Cost after iteration 80: 0.020763\n",
      "Cost after iteration 90: 0.020291\n",
      "Cost after iteration 100: 0.020170\n",
      "Cost after iteration 110: 0.019868\n",
      "Cost after iteration 120: 0.019651\n",
      "Cost after iteration 130: 0.019830\n",
      "Cost after iteration 140: 0.019537\n",
      "Cost after iteration 150: 0.019164\n",
      "Cost after iteration 160: 0.019100\n",
      "Cost after iteration 170: 0.018785\n",
      "Cost after iteration 180: 0.019026\n",
      "Cost after iteration 190: 0.018895\n",
      "current iter 14\n",
      "Cost after iteration 0: 0.020549\n",
      "Cost after iteration 10: 0.026441\n",
      "Cost after iteration 20: 0.023958\n",
      "Cost after iteration 30: 0.022613\n",
      "Cost after iteration 40: 0.022051\n",
      "Cost after iteration 50: 0.021795\n",
      "Cost after iteration 60: 0.021226\n",
      "Cost after iteration 70: 0.020681\n",
      "Cost after iteration 80: 0.019912\n",
      "Cost after iteration 90: 0.019970\n",
      "Cost after iteration 100: 0.020093\n",
      "Cost after iteration 110: 0.019756\n",
      "Cost after iteration 120: 0.019630\n",
      "Cost after iteration 130: 0.018978\n",
      "Cost after iteration 140: 0.019297\n",
      "Cost after iteration 150: 0.019150\n",
      "Cost after iteration 160: 0.018648\n",
      "Cost after iteration 170: 0.018452\n",
      "Cost after iteration 180: 0.018543\n",
      "Cost after iteration 190: 0.018363\n",
      "current iter 15\n",
      "Cost after iteration 0: 0.020399\n",
      "Cost after iteration 10: 0.026828\n",
      "Cost after iteration 20: 0.024235\n",
      "Cost after iteration 30: 0.023009\n",
      "Cost after iteration 40: 0.021855\n",
      "Cost after iteration 50: 0.021091\n",
      "Cost after iteration 60: 0.020596\n",
      "Cost after iteration 70: 0.020677\n",
      "Cost after iteration 80: 0.020406\n",
      "Cost after iteration 90: 0.019635\n",
      "Cost after iteration 100: 0.019607\n",
      "Cost after iteration 110: 0.019563\n",
      "Cost after iteration 120: 0.018928\n",
      "Cost after iteration 130: 0.018762\n",
      "Cost after iteration 140: 0.018628\n",
      "Cost after iteration 150: 0.018809\n",
      "Cost after iteration 160: 0.018427\n",
      "Cost after iteration 170: 0.018038\n",
      "Cost after iteration 180: 0.018397\n",
      "Cost after iteration 190: 0.018125\n",
      "current iter 16\n",
      "Cost after iteration 0: 0.020599\n",
      "Cost after iteration 10: 0.028090\n",
      "Cost after iteration 20: 0.023849\n",
      "Cost after iteration 30: 0.023124\n",
      "Cost after iteration 40: 0.022034\n",
      "Cost after iteration 50: 0.021416\n",
      "Cost after iteration 60: 0.021036\n",
      "Cost after iteration 70: 0.020694\n",
      "Cost after iteration 80: 0.020233\n",
      "Cost after iteration 90: 0.019962\n",
      "Cost after iteration 100: 0.019517\n",
      "Cost after iteration 110: 0.019290\n",
      "Cost after iteration 120: 0.018859\n",
      "Cost after iteration 130: 0.019114\n",
      "Cost after iteration 140: 0.018907\n",
      "Cost after iteration 150: 0.018355\n",
      "Cost after iteration 160: 0.018488\n",
      "Cost after iteration 170: 0.017968\n",
      "Cost after iteration 180: 0.018455\n",
      "Cost after iteration 190: 0.017817\n",
      "current iter 17\n",
      "Cost after iteration 0: 0.020359\n",
      "Cost after iteration 10: 0.024931\n",
      "Cost after iteration 20: 0.023488\n",
      "Cost after iteration 30: 0.022713\n",
      "Cost after iteration 40: 0.021984\n",
      "Cost after iteration 50: 0.020744\n",
      "Cost after iteration 60: 0.020669\n",
      "Cost after iteration 70: 0.019643\n",
      "Cost after iteration 80: 0.019434\n",
      "Cost after iteration 90: 0.019323\n",
      "Cost after iteration 100: 0.018917\n",
      "Cost after iteration 110: 0.018607\n",
      "Cost after iteration 120: 0.018845\n",
      "Cost after iteration 130: 0.018211\n",
      "Cost after iteration 140: 0.017992\n",
      "Cost after iteration 150: 0.017906\n",
      "Cost after iteration 160: 0.017689\n",
      "Cost after iteration 170: 0.017605\n",
      "Cost after iteration 180: 0.017603\n",
      "Cost after iteration 190: 0.017197\n",
      "current iter 18\n",
      "Cost after iteration 0: 0.019463\n",
      "Cost after iteration 10: 0.023073\n",
      "Cost after iteration 20: 0.021710\n",
      "Cost after iteration 30: 0.020916\n",
      "Cost after iteration 40: 0.020098\n",
      "Cost after iteration 50: 0.019063\n",
      "Cost after iteration 60: 0.018621\n",
      "Cost after iteration 70: 0.018354\n",
      "Cost after iteration 80: 0.017918\n",
      "Cost after iteration 90: 0.017899\n",
      "Cost after iteration 100: 0.017842\n",
      "Cost after iteration 110: 0.017673\n",
      "Cost after iteration 120: 0.017180\n",
      "Cost after iteration 130: 0.016968\n",
      "Cost after iteration 140: 0.016967\n",
      "Cost after iteration 150: 0.016816\n",
      "Cost after iteration 160: 0.016796\n",
      "Cost after iteration 170: 0.016813\n",
      "Cost after iteration 180: 0.016252\n",
      "Cost after iteration 190: 0.016019\n",
      "current iter 19\n",
      "Cost after iteration 0: 0.018228\n",
      "Cost after iteration 10: 0.022828\n",
      "Cost after iteration 20: 0.020348\n",
      "Cost after iteration 30: 0.019604\n",
      "Cost after iteration 40: 0.018812\n",
      "Cost after iteration 50: 0.018447\n",
      "Cost after iteration 60: 0.017918\n",
      "Cost after iteration 70: 0.018064\n",
      "Cost after iteration 80: 0.017585\n",
      "Cost after iteration 90: 0.016744\n",
      "Cost after iteration 100: 0.017078\n",
      "Cost after iteration 110: 0.016598\n",
      "Cost after iteration 120: 0.016604\n",
      "Cost after iteration 130: 0.016516\n",
      "Cost after iteration 140: 0.016006\n",
      "Cost after iteration 150: 0.016362\n",
      "Cost after iteration 160: 0.016054\n",
      "Cost after iteration 170: 0.016255\n",
      "Cost after iteration 180: 0.015600\n",
      "Cost after iteration 190: 0.015657\n",
      "current iter 20\n",
      "Cost after iteration 0: 0.018435\n",
      "Cost after iteration 10: 0.023413\n",
      "Cost after iteration 20: 0.021504\n",
      "Cost after iteration 30: 0.020754\n",
      "Cost after iteration 40: 0.019675\n",
      "Cost after iteration 50: 0.019278\n",
      "Cost after iteration 60: 0.019068\n",
      "Cost after iteration 70: 0.018484\n",
      "Cost after iteration 80: 0.017569\n",
      "Cost after iteration 90: 0.017322\n",
      "Cost after iteration 100: 0.016966\n",
      "Cost after iteration 110: 0.017688\n",
      "Cost after iteration 120: 0.017616\n",
      "Cost after iteration 130: 0.017261\n",
      "Cost after iteration 140: 0.016559\n",
      "Cost after iteration 150: 0.016320\n",
      "Cost after iteration 160: 0.016932\n",
      "Cost after iteration 170: 0.016375\n",
      "Cost after iteration 180: 0.016063\n",
      "Cost after iteration 190: 0.015915\n",
      "current iter 21\n",
      "Cost after iteration 0: 0.016108\n",
      "Cost after iteration 10: 0.018296\n",
      "Cost after iteration 20: 0.018208\n",
      "Cost after iteration 30: 0.016873\n",
      "Cost after iteration 40: 0.016424\n",
      "Cost after iteration 50: 0.015889\n",
      "Cost after iteration 60: 0.015283\n",
      "Cost after iteration 70: 0.015213\n",
      "Cost after iteration 80: 0.014945\n",
      "Cost after iteration 90: 0.015093\n",
      "Cost after iteration 100: 0.014410\n",
      "Cost after iteration 110: 0.014491\n",
      "Cost after iteration 120: 0.014082\n",
      "Cost after iteration 130: 0.014061\n",
      "Cost after iteration 140: 0.013863\n",
      "Cost after iteration 150: 0.013755\n",
      "Cost after iteration 160: 0.013928\n",
      "Cost after iteration 170: 0.013621\n",
      "Cost after iteration 180: 0.013499\n",
      "Cost after iteration 190: 0.013767\n",
      "current iter 22\n",
      "Cost after iteration 0: 0.016839\n",
      "Cost after iteration 10: 0.022297\n",
      "Cost after iteration 20: 0.020116\n",
      "Cost after iteration 30: 0.018875\n",
      "Cost after iteration 40: 0.018538\n",
      "Cost after iteration 50: 0.017821\n",
      "Cost after iteration 60: 0.016854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 70: 0.016781\n",
      "Cost after iteration 80: 0.016470\n",
      "Cost after iteration 90: 0.016471\n",
      "Cost after iteration 100: 0.016068\n",
      "Cost after iteration 110: 0.015748\n",
      "Cost after iteration 120: 0.015288\n",
      "Cost after iteration 130: 0.015557\n",
      "Cost after iteration 140: 0.015692\n",
      "Cost after iteration 150: 0.015108\n",
      "Cost after iteration 160: 0.014972\n",
      "Cost after iteration 170: 0.014635\n",
      "Cost after iteration 180: 0.014871\n",
      "Cost after iteration 190: 0.014284\n",
      "current iter 23\n",
      "Cost after iteration 0: 0.017961\n",
      "Cost after iteration 10: 0.023241\n",
      "Cost after iteration 20: 0.020769\n",
      "Cost after iteration 30: 0.019625\n",
      "Cost after iteration 40: 0.018503\n",
      "Cost after iteration 50: 0.018495\n",
      "Cost after iteration 60: 0.017625\n",
      "Cost after iteration 70: 0.017364\n",
      "Cost after iteration 80: 0.017272\n",
      "Cost after iteration 90: 0.016875\n",
      "Cost after iteration 100: 0.016754\n",
      "Cost after iteration 110: 0.016731\n",
      "Cost after iteration 120: 0.016544\n",
      "Cost after iteration 130: 0.016324\n",
      "Cost after iteration 140: 0.015948\n",
      "Cost after iteration 150: 0.015808\n",
      "Cost after iteration 160: 0.015746\n",
      "Cost after iteration 170: 0.015492\n",
      "Cost after iteration 180: 0.015480\n",
      "Cost after iteration 190: 0.015504\n",
      "current iter 24\n",
      "Cost after iteration 0: 0.016222\n",
      "Cost after iteration 10: 0.020288\n",
      "Cost after iteration 20: 0.019187\n",
      "Cost after iteration 30: 0.017700\n",
      "Cost after iteration 40: 0.017555\n",
      "Cost after iteration 50: 0.016891\n",
      "Cost after iteration 60: 0.016370\n",
      "Cost after iteration 70: 0.016275\n",
      "Cost after iteration 80: 0.015646\n",
      "Cost after iteration 90: 0.015688\n",
      "Cost after iteration 100: 0.015433\n",
      "Cost after iteration 110: 0.015434\n",
      "Cost after iteration 120: 0.014918\n",
      "Cost after iteration 130: 0.014971\n",
      "Cost after iteration 140: 0.014722\n",
      "Cost after iteration 150: 0.014385\n",
      "Cost after iteration 160: 0.014554\n",
      "Cost after iteration 170: 0.014442\n",
      "Cost after iteration 180: 0.014302\n",
      "Cost after iteration 190: 0.014200\n",
      "current iter 25\n",
      "Cost after iteration 0: 0.017357\n",
      "Cost after iteration 10: 0.021824\n",
      "Cost after iteration 20: 0.019205\n",
      "Cost after iteration 30: 0.018470\n",
      "Cost after iteration 40: 0.017566\n",
      "Cost after iteration 50: 0.017143\n",
      "Cost after iteration 60: 0.016419\n",
      "Cost after iteration 70: 0.016380\n",
      "Cost after iteration 80: 0.017552\n",
      "Cost after iteration 90: 0.016672\n",
      "Cost after iteration 100: 0.016074\n",
      "Cost after iteration 110: 0.015656\n",
      "Cost after iteration 120: 0.015959\n",
      "Cost after iteration 130: 0.015519\n",
      "Cost after iteration 140: 0.015315\n",
      "Cost after iteration 150: 0.015195\n",
      "Cost after iteration 160: 0.015043\n",
      "Cost after iteration 170: 0.014884\n",
      "Cost after iteration 180: 0.014935\n",
      "Cost after iteration 190: 0.014926\n",
      "current iter 26\n",
      "Cost after iteration 0: 0.013718\n",
      "Cost after iteration 10: 0.017066\n",
      "Cost after iteration 20: 0.019500\n",
      "Cost after iteration 30: 0.016226\n",
      "Cost after iteration 40: 0.014902\n",
      "Cost after iteration 50: 0.014472\n",
      "Cost after iteration 60: 0.013666\n",
      "Cost after iteration 70: 0.013659\n",
      "Cost after iteration 80: 0.013600\n",
      "Cost after iteration 90: 0.012752\n",
      "Cost after iteration 100: 0.013676\n",
      "Cost after iteration 110: 0.012746\n",
      "Cost after iteration 120: 0.013079\n",
      "Cost after iteration 130: 0.012391\n",
      "Cost after iteration 140: 0.012423\n",
      "Cost after iteration 150: 0.012328\n",
      "Cost after iteration 160: 0.012056\n",
      "Cost after iteration 170: 0.012053\n",
      "Cost after iteration 180: 0.011989\n",
      "Cost after iteration 190: 0.011789\n",
      "current iter 27\n",
      "Cost after iteration 0: 0.015516\n",
      "Cost after iteration 10: 0.019702\n",
      "Cost after iteration 20: 0.015456\n",
      "Cost after iteration 30: 0.016737\n",
      "Cost after iteration 40: 0.016231\n",
      "Cost after iteration 50: 0.015765\n",
      "Cost after iteration 60: 0.015434\n",
      "Cost after iteration 70: 0.015202\n",
      "Cost after iteration 80: 0.014783\n",
      "Cost after iteration 90: 0.014701\n",
      "Cost after iteration 100: 0.014326\n",
      "Cost after iteration 110: 0.014132\n",
      "Cost after iteration 120: 0.014102\n",
      "Cost after iteration 130: 0.013772\n",
      "Cost after iteration 140: 0.013789\n",
      "Cost after iteration 150: 0.013623\n",
      "Cost after iteration 160: 0.013376\n",
      "Cost after iteration 170: 0.013308\n",
      "Cost after iteration 180: 0.013266\n",
      "Cost after iteration 190: 0.013040\n",
      "current iter 28\n",
      "Cost after iteration 0: 0.015723\n",
      "Cost after iteration 10: 0.022137\n",
      "Cost after iteration 20: 0.018276\n",
      "Cost after iteration 30: 0.018549\n",
      "Cost after iteration 40: 0.016946\n",
      "Cost after iteration 50: 0.016816\n",
      "Cost after iteration 60: 0.015911\n",
      "Cost after iteration 70: 0.015879\n",
      "Cost after iteration 80: 0.015278\n",
      "Cost after iteration 90: 0.014355\n",
      "Cost after iteration 100: 0.014908\n",
      "Cost after iteration 110: 0.014477\n",
      "Cost after iteration 120: 0.014489\n",
      "Cost after iteration 130: 0.013719\n",
      "Cost after iteration 140: 0.013871\n",
      "Cost after iteration 150: 0.013657\n",
      "Cost after iteration 160: 0.013546\n",
      "Cost after iteration 170: 0.013556\n",
      "Cost after iteration 180: 0.013627\n",
      "Cost after iteration 190: 0.013179\n",
      "current iter 29\n",
      "Cost after iteration 0: 0.017751\n",
      "Cost after iteration 10: 0.014728\n",
      "Cost after iteration 20: 0.013560\n",
      "Cost after iteration 30: 0.013820\n",
      "Cost after iteration 40: 0.013200\n",
      "Cost after iteration 50: 0.013292\n",
      "Cost after iteration 60: 0.013442\n",
      "Cost after iteration 70: 0.013213\n",
      "Cost after iteration 80: 0.012985\n",
      "Cost after iteration 90: 0.012911\n",
      "Cost after iteration 100: 0.012704\n",
      "Cost after iteration 110: 0.012752\n",
      "Cost after iteration 120: 0.012382\n",
      "Cost after iteration 130: 0.012547\n",
      "Cost after iteration 140: 0.012354\n",
      "Cost after iteration 150: 0.012167\n",
      "Cost after iteration 160: 0.011948\n",
      "Cost after iteration 170: 0.012061\n",
      "Cost after iteration 180: 0.011746\n",
      "Cost after iteration 190: 0.011815\n",
      "current iter 30\n",
      "Cost after iteration 0: 0.010500\n",
      "Cost after iteration 10: 0.008810\n",
      "Cost after iteration 20: 0.008867\n",
      "Cost after iteration 30: 0.008578\n",
      "Cost after iteration 40: 0.008580\n",
      "Cost after iteration 50: 0.008552\n",
      "Cost after iteration 60: 0.008396\n",
      "Cost after iteration 70: 0.008396\n",
      "Cost after iteration 80: 0.008549\n",
      "Cost after iteration 90: 0.008546\n",
      "Cost after iteration 100: 0.008538\n",
      "Cost after iteration 110: 0.008282\n",
      "Cost after iteration 120: 0.008397\n",
      "Cost after iteration 130: 0.008551\n",
      "Cost after iteration 140: 0.008182\n",
      "Cost after iteration 150: 0.008258\n",
      "Cost after iteration 160: 0.008447\n",
      "Cost after iteration 170: 0.008248\n",
      "Cost after iteration 180: 0.008110\n",
      "Cost after iteration 190: 0.008413\n",
      "current iter 31\n",
      "Cost after iteration 0: 0.009080\n",
      "Cost after iteration 10: 0.008934\n",
      "Cost after iteration 20: 0.008762\n",
      "Cost after iteration 30: 0.008798\n",
      "Cost after iteration 40: 0.008826\n",
      "Cost after iteration 50: 0.008594\n",
      "Cost after iteration 60: 0.008746\n",
      "Cost after iteration 70: 0.008563\n",
      "Cost after iteration 80: 0.008791\n",
      "Cost after iteration 90: 0.008539\n",
      "Cost after iteration 100: 0.008415\n",
      "Cost after iteration 110: 0.008627\n",
      "Cost after iteration 120: 0.008586\n",
      "Cost after iteration 130: 0.008524\n",
      "Cost after iteration 140: 0.008576\n",
      "Cost after iteration 150: 0.008405\n",
      "Cost after iteration 160: 0.008376\n",
      "Cost after iteration 170: 0.008283\n",
      "Cost after iteration 180: 0.008194\n",
      "Cost after iteration 190: 0.008319\n",
      "current iter 32\n",
      "Cost after iteration 0: 0.008165\n",
      "Cost after iteration 10: 0.007905\n",
      "Cost after iteration 20: 0.008130\n",
      "Cost after iteration 30: 0.007966\n",
      "Cost after iteration 40: 0.007902\n",
      "Cost after iteration 50: 0.007828\n",
      "Cost after iteration 60: 0.007941\n",
      "Cost after iteration 70: 0.007812\n",
      "Cost after iteration 80: 0.007744\n",
      "Cost after iteration 90: 0.007732\n",
      "Cost after iteration 100: 0.007900\n",
      "Cost after iteration 110: 0.007614\n",
      "Cost after iteration 120: 0.007714\n",
      "Cost after iteration 130: 0.007747\n",
      "Cost after iteration 140: 0.007737\n",
      "Cost after iteration 150: 0.007672\n",
      "Cost after iteration 160: 0.007505\n",
      "Cost after iteration 170: 0.007727\n",
      "Cost after iteration 180: 0.007758\n",
      "Cost after iteration 190: 0.007570\n",
      "current iter 33\n",
      "Cost after iteration 0: 0.008776\n",
      "Cost after iteration 10: 0.008495\n",
      "Cost after iteration 20: 0.008528\n",
      "Cost after iteration 30: 0.008389\n",
      "Cost after iteration 40: 0.008370\n",
      "Cost after iteration 50: 0.008441\n",
      "Cost after iteration 60: 0.008155\n",
      "Cost after iteration 70: 0.007999\n",
      "Cost after iteration 80: 0.008290\n",
      "Cost after iteration 90: 0.007995\n",
      "Cost after iteration 100: 0.008164\n",
      "Cost after iteration 110: 0.008349\n",
      "Cost after iteration 120: 0.008060\n",
      "Cost after iteration 130: 0.008155\n",
      "Cost after iteration 140: 0.008245\n",
      "Cost after iteration 150: 0.008195\n",
      "Cost after iteration 160: 0.008017\n",
      "Cost after iteration 170: 0.008232\n",
      "Cost after iteration 180: 0.008047\n",
      "Cost after iteration 190: 0.008094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current iter 34\n",
      "Cost after iteration 0: 0.007040\n",
      "Cost after iteration 10: 0.006977\n",
      "Cost after iteration 20: 0.006889\n",
      "Cost after iteration 30: 0.006963\n",
      "Cost after iteration 40: 0.006818\n",
      "Cost after iteration 50: 0.006899\n",
      "Cost after iteration 60: 0.006930\n",
      "Cost after iteration 70: 0.006940\n",
      "Cost after iteration 80: 0.006950\n",
      "Cost after iteration 90: 0.006826\n",
      "Cost after iteration 100: 0.006954\n",
      "Cost after iteration 110: 0.006952\n",
      "Cost after iteration 120: 0.006756\n",
      "Cost after iteration 130: 0.006947\n",
      "Cost after iteration 140: 0.006652\n",
      "Cost after iteration 150: 0.006861\n",
      "Cost after iteration 160: 0.006876\n",
      "Cost after iteration 170: 0.006811\n",
      "Cost after iteration 180: 0.006913\n",
      "Cost after iteration 190: 0.006816\n",
      "current iter 35\n",
      "Cost after iteration 0: 0.008933\n",
      "Cost after iteration 10: 0.009926\n",
      "Cost after iteration 20: 0.008680\n",
      "Cost after iteration 30: 0.008374\n",
      "Cost after iteration 40: 0.008288\n",
      "Cost after iteration 50: 0.008114\n",
      "Cost after iteration 60: 0.007912\n",
      "Cost after iteration 70: 0.008211\n",
      "Cost after iteration 80: 0.008059\n",
      "Cost after iteration 90: 0.008014\n",
      "Cost after iteration 100: 0.008038\n",
      "Cost after iteration 110: 0.008056\n",
      "Cost after iteration 120: 0.008038\n",
      "Cost after iteration 130: 0.008074\n",
      "Cost after iteration 140: 0.008078\n",
      "Cost after iteration 150: 0.008085\n",
      "Cost after iteration 160: 0.008012\n",
      "Cost after iteration 170: 0.007712\n",
      "Cost after iteration 180: 0.007817\n",
      "Cost after iteration 190: 0.007910\n",
      "current iter 36\n",
      "Cost after iteration 0: 0.008420\n",
      "Cost after iteration 10: 0.008088\n",
      "Cost after iteration 20: 0.008113\n",
      "Cost after iteration 30: 0.008067\n",
      "Cost after iteration 40: 0.007979\n",
      "Cost after iteration 50: 0.007882\n",
      "Cost after iteration 60: 0.007919\n",
      "Cost after iteration 70: 0.007930\n",
      "Cost after iteration 80: 0.007812\n",
      "Cost after iteration 90: 0.007763\n",
      "Cost after iteration 100: 0.007881\n",
      "Cost after iteration 110: 0.007739\n",
      "Cost after iteration 120: 0.007776\n",
      "Cost after iteration 130: 0.007652\n",
      "Cost after iteration 140: 0.007596\n",
      "Cost after iteration 150: 0.007670\n",
      "Cost after iteration 160: 0.007701\n",
      "Cost after iteration 170: 0.007637\n",
      "Cost after iteration 180: 0.007432\n",
      "Cost after iteration 190: 0.007540\n",
      "current iter 37\n",
      "Cost after iteration 0: 0.008119\n",
      "Cost after iteration 10: 0.008301\n",
      "Cost after iteration 20: 0.008163\n",
      "Cost after iteration 30: 0.007943\n",
      "Cost after iteration 40: 0.007889\n",
      "Cost after iteration 50: 0.007763\n",
      "Cost after iteration 60: 0.007704\n",
      "Cost after iteration 70: 0.007746\n",
      "Cost after iteration 80: 0.007721\n",
      "Cost after iteration 90: 0.007601\n",
      "Cost after iteration 100: 0.007709\n",
      "Cost after iteration 110: 0.007527\n",
      "Cost after iteration 120: 0.007525\n",
      "Cost after iteration 130: 0.007762\n",
      "Cost after iteration 140: 0.007633\n",
      "Cost after iteration 150: 0.007675\n",
      "Cost after iteration 160: 0.007512\n",
      "Cost after iteration 170: 0.007497\n",
      "Cost after iteration 180: 0.007469\n",
      "Cost after iteration 190: 0.007408\n",
      "current iter 38\n",
      "Cost after iteration 0: 0.008229\n",
      "Cost after iteration 10: 0.008443\n",
      "Cost after iteration 20: 0.008193\n",
      "Cost after iteration 30: 0.007995\n",
      "Cost after iteration 40: 0.007847\n",
      "Cost after iteration 50: 0.007939\n",
      "Cost after iteration 60: 0.007702\n",
      "Cost after iteration 70: 0.007576\n",
      "Cost after iteration 80: 0.007622\n",
      "Cost after iteration 90: 0.007477\n",
      "Cost after iteration 100: 0.007368\n",
      "Cost after iteration 110: 0.007693\n",
      "Cost after iteration 120: 0.007543\n",
      "Cost after iteration 130: 0.007661\n",
      "Cost after iteration 140: 0.007493\n",
      "Cost after iteration 150: 0.007389\n",
      "Cost after iteration 160: 0.007657\n",
      "Cost after iteration 170: 0.007590\n",
      "Cost after iteration 180: 0.007450\n",
      "Cost after iteration 190: 0.007523\n",
      "current iter 39\n",
      "Cost after iteration 0: 0.007894\n",
      "Cost after iteration 10: 0.008264\n",
      "Cost after iteration 20: 0.007937\n",
      "Cost after iteration 30: 0.007901\n",
      "Cost after iteration 40: 0.007715\n",
      "Cost after iteration 50: 0.007514\n",
      "Cost after iteration 60: 0.007737\n",
      "Cost after iteration 70: 0.007722\n",
      "Cost after iteration 80: 0.007604\n",
      "Cost after iteration 90: 0.007507\n",
      "Cost after iteration 100: 0.007449\n",
      "Cost after iteration 110: 0.007639\n",
      "Cost after iteration 120: 0.007371\n",
      "Cost after iteration 130: 0.007425\n",
      "Cost after iteration 140: 0.007289\n",
      "Cost after iteration 150: 0.007442\n",
      "Cost after iteration 160: 0.007392\n",
      "Cost after iteration 170: 0.007276\n",
      "Cost after iteration 180: 0.007369\n",
      "Cost after iteration 190: 0.007327\n",
      "current iter 40\n",
      "Cost after iteration 0: 0.019882\n",
      "Cost after iteration 10: 0.012223\n",
      "Cost after iteration 20: 0.010974\n",
      "Cost after iteration 30: 0.011914\n",
      "Cost after iteration 40: 0.011064\n",
      "Cost after iteration 50: 0.010842\n",
      "Cost after iteration 60: 0.010722\n",
      "Cost after iteration 70: 0.010790\n",
      "Cost after iteration 80: 0.010842\n",
      "Cost after iteration 90: 0.010821\n",
      "Cost after iteration 100: 0.010396\n",
      "Cost after iteration 110: 0.010254\n",
      "Cost after iteration 120: 0.010515\n",
      "Cost after iteration 130: 0.010221\n",
      "Cost after iteration 140: 0.010125\n",
      "Cost after iteration 150: 0.010065\n",
      "Cost after iteration 160: 0.009939\n",
      "Cost after iteration 170: 0.009753\n",
      "Cost after iteration 180: 0.009802\n",
      "Cost after iteration 190: 0.009813\n",
      "current iter 41\n",
      "Cost after iteration 0: 0.027999\n",
      "Cost after iteration 10: 0.018160\n",
      "Cost after iteration 20: 0.017193\n",
      "Cost after iteration 30: 0.016737\n",
      "Cost after iteration 40: 0.015740\n",
      "Cost after iteration 50: 0.015166\n",
      "Cost after iteration 60: 0.015145\n",
      "Cost after iteration 70: 0.014633\n",
      "Cost after iteration 80: 0.014132\n",
      "Cost after iteration 90: 0.013718\n",
      "Cost after iteration 100: 0.014286\n",
      "Cost after iteration 110: 0.013403\n",
      "Cost after iteration 120: 0.013658\n",
      "Cost after iteration 130: 0.013185\n",
      "Cost after iteration 140: 0.012968\n",
      "Cost after iteration 150: 0.013045\n",
      "Cost after iteration 160: 0.012661\n",
      "Cost after iteration 170: 0.012510\n",
      "Cost after iteration 180: 0.012433\n",
      "Cost after iteration 190: 0.012269\n",
      "current iter 42\n",
      "Cost after iteration 0: 0.012363\n",
      "Cost after iteration 10: 0.008133\n",
      "Cost after iteration 20: 0.007724\n",
      "Cost after iteration 30: 0.007504\n",
      "Cost after iteration 40: 0.007439\n",
      "Cost after iteration 50: 0.007391\n",
      "Cost after iteration 60: 0.007111\n",
      "Cost after iteration 70: 0.007039\n",
      "Cost after iteration 80: 0.006962\n",
      "Cost after iteration 90: 0.007151\n",
      "Cost after iteration 100: 0.006854\n",
      "Cost after iteration 110: 0.006615\n",
      "Cost after iteration 120: 0.006703\n",
      "Cost after iteration 130: 0.006733\n",
      "Cost after iteration 140: 0.006609\n",
      "Cost after iteration 150: 0.006695\n",
      "Cost after iteration 160: 0.006546\n",
      "Cost after iteration 170: 0.006437\n",
      "Cost after iteration 180: 0.006563\n",
      "Cost after iteration 190: 0.006390\n",
      "current iter 43\n",
      "Cost after iteration 0: 0.011970\n",
      "Cost after iteration 10: 0.013092\n",
      "Cost after iteration 20: 0.011123\n",
      "Cost after iteration 30: 0.010753\n",
      "Cost after iteration 40: 0.010441\n",
      "Cost after iteration 50: 0.010192\n",
      "Cost after iteration 60: 0.009861\n",
      "Cost after iteration 70: 0.009777\n",
      "Cost after iteration 80: 0.009696\n",
      "Cost after iteration 90: 0.010043\n",
      "Cost after iteration 100: 0.009839\n",
      "Cost after iteration 110: 0.009729\n",
      "Cost after iteration 120: 0.009564\n",
      "Cost after iteration 130: 0.009491\n",
      "Cost after iteration 140: 0.009344\n",
      "Cost after iteration 150: 0.009361\n",
      "Cost after iteration 160: 0.009165\n",
      "Cost after iteration 170: 0.009223\n",
      "Cost after iteration 180: 0.009138\n",
      "Cost after iteration 190: 0.009236\n",
      "current iter 44\n",
      "Cost after iteration 0: 0.013099\n",
      "Cost after iteration 10: 0.018114\n",
      "Cost after iteration 20: 0.015771\n",
      "Cost after iteration 30: 0.014257\n",
      "Cost after iteration 40: 0.013597\n",
      "Cost after iteration 50: 0.013252\n",
      "Cost after iteration 60: 0.012801\n",
      "Cost after iteration 70: 0.012331\n",
      "Cost after iteration 80: 0.011743\n",
      "Cost after iteration 90: 0.011987\n",
      "Cost after iteration 100: 0.011598\n",
      "Cost after iteration 110: 0.011786\n",
      "Cost after iteration 120: 0.011399\n",
      "Cost after iteration 130: 0.011098\n",
      "Cost after iteration 140: 0.010876\n",
      "Cost after iteration 150: 0.010688\n",
      "Cost after iteration 160: 0.010667\n",
      "Cost after iteration 170: 0.010297\n",
      "Cost after iteration 180: 0.010452\n",
      "Cost after iteration 190: 0.010330\n",
      "current iter 45\n",
      "Cost after iteration 0: 0.008734\n",
      "Cost after iteration 10: 0.006373\n",
      "Cost after iteration 20: 0.006296\n",
      "Cost after iteration 30: 0.006148\n",
      "Cost after iteration 40: 0.006162\n",
      "Cost after iteration 50: 0.006137\n",
      "Cost after iteration 60: 0.005945\n",
      "Cost after iteration 70: 0.005978\n",
      "Cost after iteration 80: 0.005972\n",
      "Cost after iteration 90: 0.006033\n",
      "Cost after iteration 100: 0.006014\n",
      "Cost after iteration 110: 0.006014\n",
      "Cost after iteration 120: 0.005920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 130: 0.005808\n",
      "Cost after iteration 140: 0.005909\n",
      "Cost after iteration 150: 0.005863\n",
      "Cost after iteration 160: 0.006037\n",
      "Cost after iteration 170: 0.006124\n",
      "Cost after iteration 180: 0.005831\n",
      "Cost after iteration 190: 0.005859\n",
      "current iter 46\n",
      "Cost after iteration 0: 0.017002\n",
      "Cost after iteration 10: 0.019597\n",
      "Cost after iteration 20: 0.017735\n",
      "Cost after iteration 30: 0.016517\n",
      "Cost after iteration 40: 0.015780\n",
      "Cost after iteration 50: 0.015350\n",
      "Cost after iteration 60: 0.015261\n",
      "Cost after iteration 70: 0.014830\n",
      "Cost after iteration 80: 0.014420\n",
      "Cost after iteration 90: 0.013911\n",
      "Cost after iteration 100: 0.014091\n",
      "Cost after iteration 110: 0.013553\n",
      "Cost after iteration 120: 0.013681\n",
      "Cost after iteration 130: 0.013074\n",
      "Cost after iteration 140: 0.013347\n",
      "Cost after iteration 150: 0.013087\n",
      "Cost after iteration 160: 0.012847\n",
      "Cost after iteration 170: 0.012402\n",
      "Cost after iteration 180: 0.012659\n",
      "Cost after iteration 190: 0.012534\n",
      "current iter 47\n",
      "Cost after iteration 0: 0.010966\n",
      "Cost after iteration 10: 0.008117\n",
      "Cost after iteration 20: 0.007924\n",
      "Cost after iteration 30: 0.007905\n",
      "Cost after iteration 40: 0.007519\n",
      "Cost after iteration 50: 0.007507\n",
      "Cost after iteration 60: 0.007462\n",
      "Cost after iteration 70: 0.007569\n",
      "Cost after iteration 80: 0.007312\n",
      "Cost after iteration 90: 0.007470\n",
      "Cost after iteration 100: 0.007277\n",
      "Cost after iteration 110: 0.007449\n",
      "Cost after iteration 120: 0.007400\n",
      "Cost after iteration 130: 0.007366\n",
      "Cost after iteration 140: 0.007083\n",
      "Cost after iteration 150: 0.007253\n",
      "Cost after iteration 160: 0.007155\n",
      "Cost after iteration 170: 0.007273\n",
      "Cost after iteration 180: 0.007356\n",
      "Cost after iteration 190: 0.006995\n",
      "current iter 48\n",
      "Cost after iteration 0: 0.016801\n",
      "Cost after iteration 10: 0.016145\n",
      "Cost after iteration 20: 0.014704\n",
      "Cost after iteration 30: 0.014449\n",
      "Cost after iteration 40: 0.013951\n",
      "Cost after iteration 50: 0.013405\n",
      "Cost after iteration 60: 0.013273\n",
      "Cost after iteration 70: 0.013367\n",
      "Cost after iteration 80: 0.013004\n",
      "Cost after iteration 90: 0.012766\n",
      "Cost after iteration 100: 0.012590\n",
      "Cost after iteration 110: 0.012794\n",
      "Cost after iteration 120: 0.012285\n",
      "Cost after iteration 130: 0.012433\n",
      "Cost after iteration 140: 0.011998\n",
      "Cost after iteration 150: 0.012224\n",
      "Cost after iteration 160: 0.012123\n",
      "Cost after iteration 170: 0.012046\n",
      "Cost after iteration 180: 0.011626\n",
      "Cost after iteration 190: 0.011546\n",
      "current iter 49\n",
      "Cost after iteration 0: 0.014738\n",
      "Cost after iteration 10: 0.016958\n",
      "Cost after iteration 20: 0.014456\n",
      "Cost after iteration 30: 0.013835\n",
      "Cost after iteration 40: 0.013481\n",
      "Cost after iteration 50: 0.013145\n",
      "Cost after iteration 60: 0.013045\n",
      "Cost after iteration 70: 0.012379\n",
      "Cost after iteration 80: 0.012487\n",
      "Cost after iteration 90: 0.011944\n",
      "Cost after iteration 100: 0.011769\n",
      "Cost after iteration 110: 0.011741\n",
      "Cost after iteration 120: 0.011760\n",
      "Cost after iteration 130: 0.011483\n",
      "Cost after iteration 140: 0.010970\n",
      "Cost after iteration 150: 0.011140\n",
      "Cost after iteration 160: 0.011027\n",
      "Cost after iteration 170: 0.010912\n",
      "Cost after iteration 180: 0.010518\n",
      "Cost after iteration 190: 0.010614\n",
      "current iter 50\n",
      "Cost after iteration 0: 0.013116\n",
      "Cost after iteration 10: 0.015569\n",
      "Cost after iteration 20: 0.013572\n",
      "Cost after iteration 30: 0.013007\n",
      "Cost after iteration 40: 0.012103\n",
      "Cost after iteration 50: 0.012101\n",
      "Cost after iteration 60: 0.011304\n",
      "Cost after iteration 70: 0.011237\n",
      "Cost after iteration 80: 0.011130\n",
      "Cost after iteration 90: 0.010796\n",
      "Cost after iteration 100: 0.010490\n",
      "Cost after iteration 110: 0.010642\n",
      "Cost after iteration 120: 0.009888\n",
      "Cost after iteration 130: 0.010019\n",
      "Cost after iteration 140: 0.010092\n",
      "Cost after iteration 150: 0.009920\n",
      "Cost after iteration 160: 0.009573\n",
      "Cost after iteration 170: 0.009647\n",
      "Cost after iteration 180: 0.009494\n",
      "Cost after iteration 190: 0.009301\n",
      "current iter 51\n",
      "Cost after iteration 0: 0.015581\n",
      "Cost after iteration 10: 0.016591\n",
      "Cost after iteration 20: 0.015215\n",
      "Cost after iteration 30: 0.014153\n",
      "Cost after iteration 40: 0.013723\n",
      "Cost after iteration 50: 0.013156\n",
      "Cost after iteration 60: 0.012367\n",
      "Cost after iteration 70: 0.012436\n",
      "Cost after iteration 80: 0.012120\n",
      "Cost after iteration 90: 0.011958\n",
      "Cost after iteration 100: 0.011583\n",
      "Cost after iteration 110: 0.011689\n",
      "Cost after iteration 120: 0.011254\n",
      "Cost after iteration 130: 0.011422\n",
      "Cost after iteration 140: 0.011247\n",
      "Cost after iteration 150: 0.010940\n",
      "Cost after iteration 160: 0.010634\n",
      "Cost after iteration 170: 0.010873\n",
      "Cost after iteration 180: 0.010592\n",
      "Cost after iteration 190: 0.010511\n",
      "current iter 52\n",
      "Cost after iteration 0: 0.015639\n",
      "Cost after iteration 10: 0.015731\n",
      "Cost after iteration 20: 0.014172\n",
      "Cost after iteration 30: 0.013713\n",
      "Cost after iteration 40: 0.012866\n",
      "Cost after iteration 50: 0.012605\n",
      "Cost after iteration 60: 0.012477\n",
      "Cost after iteration 70: 0.012439\n",
      "Cost after iteration 80: 0.012235\n",
      "Cost after iteration 90: 0.012187\n",
      "Cost after iteration 100: 0.011880\n",
      "Cost after iteration 110: 0.011695\n",
      "Cost after iteration 120: 0.011669\n",
      "Cost after iteration 130: 0.011324\n",
      "Cost after iteration 140: 0.010985\n",
      "Cost after iteration 150: 0.011296\n",
      "Cost after iteration 160: 0.010932\n",
      "Cost after iteration 170: 0.011268\n",
      "Cost after iteration 180: 0.010799\n",
      "Cost after iteration 190: 0.010820\n",
      "current iter 53\n",
      "Cost after iteration 0: 0.015973\n",
      "Cost after iteration 10: 0.016721\n",
      "Cost after iteration 20: 0.015680\n",
      "Cost after iteration 30: 0.014327\n",
      "Cost after iteration 40: 0.014187\n",
      "Cost after iteration 50: 0.013263\n",
      "Cost after iteration 60: 0.012964\n",
      "Cost after iteration 70: 0.013784\n",
      "Cost after iteration 80: 0.012957\n",
      "Cost after iteration 90: 0.012611\n",
      "Cost after iteration 100: 0.012003\n",
      "Cost after iteration 110: 0.012250\n",
      "Cost after iteration 120: 0.012625\n",
      "Cost after iteration 130: 0.012630\n",
      "Cost after iteration 140: 0.011948\n",
      "Cost after iteration 150: 0.011674\n",
      "Cost after iteration 160: 0.011637\n",
      "Cost after iteration 170: 0.011694\n",
      "Cost after iteration 180: 0.011401\n",
      "Cost after iteration 190: 0.011503\n",
      "current iter 54\n",
      "Cost after iteration 0: 0.014796\n",
      "Cost after iteration 10: 0.015663\n",
      "Cost after iteration 20: 0.013833\n",
      "Cost after iteration 30: 0.013576\n",
      "Cost after iteration 40: 0.012477\n",
      "Cost after iteration 50: 0.012647\n",
      "Cost after iteration 60: 0.012052\n",
      "Cost after iteration 70: 0.012152\n",
      "Cost after iteration 80: 0.011830\n",
      "Cost after iteration 90: 0.011746\n",
      "Cost after iteration 100: 0.011430\n",
      "Cost after iteration 110: 0.011265\n",
      "Cost after iteration 120: 0.011325\n",
      "Cost after iteration 130: 0.010883\n",
      "Cost after iteration 140: 0.011013\n",
      "Cost after iteration 150: 0.010729\n",
      "Cost after iteration 160: 0.010773\n",
      "Cost after iteration 170: 0.010370\n",
      "Cost after iteration 180: 0.010399\n",
      "Cost after iteration 190: 0.010572\n",
      "current iter 55\n",
      "Cost after iteration 0: 0.013620\n",
      "Cost after iteration 10: 0.015231\n",
      "Cost after iteration 20: 0.013790\n",
      "Cost after iteration 30: 0.013263\n",
      "Cost after iteration 40: 0.012717\n",
      "Cost after iteration 50: 0.012289\n",
      "Cost after iteration 60: 0.012025\n",
      "Cost after iteration 70: 0.011735\n",
      "Cost after iteration 80: 0.011727\n",
      "Cost after iteration 90: 0.011271\n",
      "Cost after iteration 100: 0.010981\n",
      "Cost after iteration 110: 0.010803\n",
      "Cost after iteration 120: 0.011164\n",
      "Cost after iteration 130: 0.010642\n",
      "Cost after iteration 140: 0.010566\n",
      "Cost after iteration 150: 0.010569\n",
      "Cost after iteration 160: 0.010316\n",
      "Cost after iteration 170: 0.010163\n",
      "Cost after iteration 180: 0.010402\n",
      "Cost after iteration 190: 0.009954\n",
      "current iter 56\n",
      "Cost after iteration 0: 0.013617\n",
      "Cost after iteration 10: 0.014165\n",
      "Cost after iteration 20: 0.015100\n",
      "Cost after iteration 30: 0.013453\n",
      "Cost after iteration 40: 0.012906\n",
      "Cost after iteration 50: 0.012324\n",
      "Cost after iteration 60: 0.011976\n",
      "Cost after iteration 70: 0.011854\n",
      "Cost after iteration 80: 0.011825\n",
      "Cost after iteration 90: 0.011104\n",
      "Cost after iteration 100: 0.011520\n",
      "Cost after iteration 110: 0.010915\n",
      "Cost after iteration 120: 0.010872\n",
      "Cost after iteration 130: 0.010686\n",
      "Cost after iteration 140: 0.010587\n",
      "Cost after iteration 150: 0.011044\n",
      "Cost after iteration 160: 0.010069\n",
      "Cost after iteration 170: 0.010055\n",
      "Cost after iteration 180: 0.010131\n",
      "Cost after iteration 190: 0.009995\n",
      "current iter 57\n",
      "Cost after iteration 0: 0.013088\n",
      "Cost after iteration 10: 0.014023\n",
      "Cost after iteration 20: 0.012452\n",
      "Cost after iteration 30: 0.011897\n",
      "Cost after iteration 40: 0.011725\n",
      "Cost after iteration 50: 0.010854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 60: 0.010815\n",
      "Cost after iteration 70: 0.010671\n",
      "Cost after iteration 80: 0.010473\n",
      "Cost after iteration 90: 0.010715\n",
      "Cost after iteration 100: 0.010070\n",
      "Cost after iteration 110: 0.010367\n",
      "Cost after iteration 120: 0.009818\n",
      "Cost after iteration 130: 0.009604\n",
      "Cost after iteration 140: 0.009503\n",
      "Cost after iteration 150: 0.009454\n",
      "Cost after iteration 160: 0.009596\n",
      "Cost after iteration 170: 0.009316\n",
      "Cost after iteration 180: 0.009255\n",
      "Cost after iteration 190: 0.008908\n"
     ]
    }
   ],
   "source": [
    "mm = train_list_of_dataset(nn,x,X_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO : CREATE FUNCTION TO RECOVER CLASSIFICATION DATASET INTO HUMAN-Fridendly DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_answer(predict_arr,dic):\n",
    "    reverse_dic = create_reverse_dictionary(dic)\n",
    "    #init an dic as final anser array\n",
    "    ans_arr = []\n",
    "    #create array with all dict keys based on insertion order\n",
    "    dic_arr = list(dic.keys())\n",
    "    #recover predicted candidate sequentilaay\n",
    "    for i in range(len(predict_arr)):\n",
    "        tmp_ans_array = {} # cur dict to store anser for this candidate\n",
    "        cur_loc = 0\n",
    "        cur_des = 0\n",
    "        #loop through dictionary array to recover each attribute column\n",
    "        for j in range(len(dic_arr)):\n",
    "            cur_dict_key = dic_arr[j]\n",
    "            cur_dict_arr = dic.get(cur_dict_key)\n",
    "            cur_reverse_dic_arr = reverse_dic.get(cur_dict_key)\n",
    "            cur_dict_length = len(cur_dict_arr)+1\n",
    "            cur_des += cur_dict_length #update current cutting position\n",
    "            #restore\n",
    "            tmp_cutting_arr = predict_arr[i][cur_loc:cur_des]\n",
    "            tag = tmp_cutting_arr[-1]\n",
    "            if tag == 0:\n",
    "                tmp_ans_array[cur_dict_key] = 'NA'\n",
    "            else:\n",
    "                #find the position where tag is 1\n",
    "                loc = 0\n",
    "                for pos in range(len(tmp_cutting_arr)):\n",
    "                    if tmp_cutting_arr[pos] == 1:\n",
    "                        loc = pos\n",
    "                        break\n",
    "                #now based on location, create cover anser\n",
    "                #case one loc is 0\n",
    "                if loc == 0:\n",
    "                    ans =\"<\" + str(cur_reverse_dic_arr.get(loc))\n",
    "                    tmp_ans_array[cur_dict_key] = ans\n",
    "                else:\n",
    "                    last_loc = loc-1\n",
    "                    ans=\"\"\n",
    "                    ans+=str(cur_reverse_dic_arr.get(last_loc))+\" < \"\n",
    "                    ans+=str(cur_reverse_dic_arr.get(loc))\n",
    "                    tmp_ans_array[cur_dict_key] = ans\n",
    "#                 ans =cur_reverse_dic_arr.get(loc)\n",
    "#                 tmp_ans_array[cur_dict_key] = ans\n",
    "        \n",
    "            cur_loc+=cur_dict_length\n",
    "        #append current tmp candidate array to finaly answer\n",
    "        ans_arr.append(tmp_ans_array)\n",
    "    \n",
    "    return ans_arr\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE FUNCTION FOR CROSS VALIDATION\n",
    "Assess the accuracy by count how many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(arr_all_candidate,num_fold,og_pred,size_hidden_layer,iter_num,learning_rate):\n",
    "    cost = 0\n",
    "    #create num_fold equal size fold from input nparray\n",
    "    list_of_fold = np.array_split(arr_all_candidate,num_fold)\n",
    "    \n",
    "    #need to append training dataset together, while leave testing set alone\n",
    "    for i in range(num_fold):\n",
    "        x_pred = list_of_fold[i]\n",
    "        \n",
    "        tmp = 0\n",
    "        for j in range(0,num_fold):\n",
    "            if j == i:\n",
    "                continue\n",
    "            else:\n",
    "                \n",
    "                if tmp == 0:\n",
    "                    ans = np.asarray(list_of_fold[j])\n",
    "                    tmp+=1\n",
    "                \n",
    "                elif tmp > 0:\n",
    "                    \n",
    "                    ans = cro_new_append(ans,list_of_fold[j])\n",
    "\n",
    "            \n",
    "        #finished rebuilding training dataset\n",
    "        #start training neural network\n",
    "        nn = NeuralNetwork(size_hidden_layer,iter_num,learning_rate) #size of input layer\n",
    "        \n",
    "        nn.fit(ans,ans)\n",
    "        predict_ans = nn.predict(x_pred)\n",
    "        find_classification_based_on_distance(predict_ans,og_pred)\n",
    "        #coun\n",
    "        tmpcost = cost_function(predict_ans,x_pred)\n",
    "        cost+=tmpcost\n",
    "        print('#### current cutting position is : ', i, 'current cost is: ',tmpcost)\n",
    "    print('final avg cost is : ', cost/num_fold)\n",
    "    return cost/num_fold\n",
    "\n",
    "\n",
    "\n",
    "def cost_function(x,y):\n",
    "    #compute the average num of wrongly predicted column as cost function\n",
    "    total_num = len(x)\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        tmpcount = 0\n",
    "        for j in range(len(x[i])):\n",
    "            if x[i][j]!=y[i][j]:\n",
    "                tmpcount+=1\n",
    "        count+=tmpcount\n",
    "    avg_count = count/total_num\n",
    "    return avg_count\n",
    "def cross_valid_append_arr(old,new):\n",
    "    ans = np.append([old],[new],axis = 0)\n",
    "    return ans\n",
    "\n",
    "def cro_new_append(old,new):\n",
    "    #num_col\n",
    "    num_col = len(old[0])\n",
    "    new_ans = []\n",
    "    for i in range(len(old)):\n",
    "        new_ans.append(old[i])\n",
    "    for i in range(len(new)):\n",
    "        new_ans.append(new[i])\n",
    "    return np.asarray(new_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
