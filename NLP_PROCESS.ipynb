{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO MACHIING LEARNING LIBRARY USED FOR TRAINING/PREDICTION/CLASSIFCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why NLP?\n",
    "combine manylabs with training('train.csv') daset\n",
    "apply count vectorizer to both of them together\n",
    "only train a model based on logistic regression using training dataset\n",
    "use trained model to analyze sentiment of manylabs highpower and low power column\n",
    "create a CSV FILE TO store sentiment for [highpower] [lowpoer] for each candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic:\n",
    "    def __init__(self,rate,iteration):\n",
    "        self.rate = rate\n",
    "        self.weights = None\n",
    "        self.iteration = iteration\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def loss(self,h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def gradient_descent(self,X, h, y):\n",
    "        return np.dot(X.T, (h - y)) / y.shape[0]\n",
    "\n",
    "    def update_weight_loss(self,weight, learning_rate, gradient):\n",
    "        return weight - learning_rate * gradient\n",
    "    \n",
    "    def initialize_parameters(self,X):\n",
    "        self.weights = np.random.randn(len(X[0])) * 0.01\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.initialize_parameters(X)\n",
    "        #gradient descent\n",
    "        for i in range(self.iteration):\n",
    "            h = self.sigmoid(np.dot(X,self.weights))\n",
    "            gradient = self.gradient_descent(X,h,y)\n",
    "            self.weights = self.update_weight_loss(self.weights,self.rate,gradient)\n",
    "            loss_val = self.loss(h,y)\n",
    "            if i % 10 == 0:\n",
    "                print('current iter is: ', i,'loss' ,loss_val)\n",
    "    def predict(self,X):\n",
    "        ans = self.sigmoid(np.dot(X,self.weights))\n",
    "        for i in range(len(ans)):\n",
    "            if ans[i] < 0.5:\n",
    "                ans[i] = 0\n",
    "            else:\n",
    "                ans[i] = 1\n",
    "        return ans\n",
    "    \n",
    "    def predict_no_threshold(self,X):\n",
    "        ans = self.sigmoid(np.dot(X,self.weights))\n",
    "        ans_with_threshold = []\n",
    "        for i in range(len(ans)):\n",
    "            if(ans[i]) < 0.5 :\n",
    "                ans_with_threshold.append(0)\n",
    "            elif ans[i] == 0.5:\n",
    "                ans_with_threshold.append(0.5)\n",
    "            elif ans[i] > 0.5:\n",
    "                ans_with_threshold.append(1)\n",
    "        return ans,ans_with_threshold\n",
    "    \n",
    "    def test_accuracy(self,X,y):\n",
    "        cor = 0\n",
    "        total = len(y)\n",
    "        for i in range(total):\n",
    "            if X[i] == y[i]:\n",
    "                cor+=1\n",
    "        \n",
    "        print(\"predict accuracy is: \", cor/total)\n",
    "        return cor/total\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array_for_training_testing_sent_data():\n",
    "    train = pd.read_csv('data/train.csv',encoding='latin-1')\n",
    "    test = pd.read_csv('data/test.csv',encoding='latin-1')\n",
    "    train = train[0:10000]\n",
    "#     test = test[0:8000]\n",
    "    #create list to hold all data\n",
    "    train_tags = train['Sentiment'].values\n",
    "    train_text = train['SentimentText'].values\n",
    "\n",
    "    return train_text,train_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE df to store manylab text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def many_lab_array(feature):\n",
    "    df3 = pd.read_csv('./ML3AllSites.csv', sep=',',encoding='ISO-8859-1')\n",
    "    df = pd.read_csv('./Tab.delimited.Cleaned.dataset.WITH.variable.labels.csv', sep='\\t',encoding='ISO-8859-1')\n",
    "    many_lab_text = df3[feature].values\n",
    "    return many_lab_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processnull(arr):\n",
    "    arrnew = []\n",
    "    for i in range(len(arr)):\n",
    "        if type(arr[i]) == float and math.isnan(arr[i]) :\n",
    "            arrnew.append('nan')\n",
    "        else:\n",
    "            arrnew.append(arr[i])\n",
    "    return arrnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean dataset into regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reviews(reviews):\n",
    "\n",
    "    REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    NO_SPACE = \"\"\n",
    "    SPACE = \" \"\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "# reviews_test_clean = preprocess_reviews(reviews_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams size = 1,2\n",
    "# STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(reviews_train_clean):\n",
    "    stop_words = stopwords.words('english')\n",
    "    ngram_vectorizer = CountVectorizer(binary=True,ngram_range=(1,1),stop_words= stop_words)\n",
    "    ngram_vectorizer.fit(reviews_train_clean)\n",
    "    X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append dataset into og\n",
    "def append(og,lab_arr):\n",
    "    ans = list(og)\n",
    "    for i in range(len(lab_arr)):\n",
    "        ans.append(lab_arr[i])\n",
    "    return np.asarray(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_training_data(x,manylab):\n",
    "    x_train = []\n",
    "    x_pred = []\n",
    "    for i in range(0,len(x)-len(manylab)):\n",
    "        x_train.append(x[i])\n",
    "    for i in range(len(x)-len(manylab),len(x)):\n",
    "        x_pred.append(x[i])\n",
    "    return np.asarray(x_train),np.asarray(x_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE TRAINING AND PREDICTION DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (1,11,12,19,20,129,132,169,230) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (17,55,59,61,65,68,69,70,83,90,91,92,93,120,121,122,123,126,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "train_text,train_tags = create_array_for_training_testing_sent_data()\n",
    "reviews_train_clean = preprocess_reviews(train_text)\n",
    "man_arr = many_lab_array('lowpower')\n",
    "man_arr_no_null = processnull(man_arr)\n",
    "clean_man_arr = preprocess_reviews(man_arr_no_null)\n",
    "new_x = append(reviews_train_clean,clean_man_arr)\n",
    "X = data_preprocessing(new_x)\n",
    "X_spase_many_lab = csr_matrix(X).toarray()\n",
    "x_train,x_pred = recover_training_data(X_spase_many_lab,clean_man_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_with_many_lab = Logistic(6,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current iter is:  0 loss 0.6928807826629382\n",
      "current iter is:  10 loss 0.604737906370955\n",
      "current iter is:  20 loss 0.5655888876998287\n",
      "current iter is:  30 loss 0.541029114868506\n",
      "current iter is:  40 loss 0.5229798372843328\n",
      "current iter is:  50 loss 0.5086012982491588\n",
      "current iter is:  60 loss 0.4965928742209115\n",
      "current iter is:  70 loss 0.4862491156399814\n",
      "current iter is:  80 loss 0.4771423069391077\n",
      "current iter is:  90 loss 0.46899241790480944\n"
     ]
    }
   ],
   "source": [
    "log_with_many_lab.fit(x_train,train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_for_many_labs,threshold = log_with_many_lab.predict_no_threshold(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85, 145, 265, 389, 424, 515, 847, 854, 1253, 1921, 1940, 1958, 2105, 2333, 2337, 2394, 2403, 2430, 2533, 2620, 2897, 2905]\n"
     ]
    }
   ],
   "source": [
    "dic = []\n",
    "for i in range(len(sentiment_analysis_for_many_labs)):\n",
    "    if sentiment_analysis_for_many_labs[i] >0.500:\n",
    "        dic.append(i)\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A situation in which someone had power over me was at volleyball tryouts. They had the ability to cut me from the team based off of my performance. At first I was very nervous and was not assured of my abilities, but after I found out that I made the team I was very happy.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['lowpower'][424]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1,11,12,19,20,129,132,169,230) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I was being evaluated at a sports tryout. I felt as if I had no power for the outcome of the tryout and felt very nervous not knowing how the personal in control felt about me making the team.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv('./ML3AllSites.csv', sep=',',encoding='ISO-8859-1')\n",
    "df3['lowpower'][2905]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sentiment_analysis_for_many_labs).to_csv(\"./lower.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for 'highpower'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (1,11,12,19,20,129,132,169,230) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "/usr/lib/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (17,55,59,61,65,68,69,70,83,90,91,92,93,120,121,122,123,126,140,141) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "train_text,train_tags = create_array_for_training_testing_sent_data()\n",
    "reviews_train_clean = preprocess_reviews(train_text)\n",
    "man_arr = many_lab_array('highpower')\n",
    "man_arr_no_null = processnull(man_arr)\n",
    "clean_man_arr = preprocess_reviews(man_arr_no_null)\n",
    "new_x = append(reviews_train_clean,clean_man_arr)\n",
    "X = data_preprocessing(new_x)\n",
    "X_spase_many_lab = csr_matrix(X).toarray()\n",
    "x_train,x_pred = recover_training_data(X_spase_many_lab,clean_man_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_with_many_lab = Logistic(6,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current iter is:  0 loss 0.6927661384062759\n",
      "current iter is:  10 loss 0.6048229903055362\n",
      "current iter is:  20 loss 0.5657161373948147\n",
      "current iter is:  30 loss 0.5411713927774857\n",
      "current iter is:  40 loss 0.5231253083816345\n",
      "current iter is:  50 loss 0.5087450114884917\n",
      "current iter is:  60 loss 0.49673278978154395\n",
      "current iter is:  70 loss 0.486384440084307\n",
      "current iter is:  80 loss 0.4772728098243504\n",
      "current iter is:  90 loss 0.46911813753036286\n"
     ]
    }
   ],
   "source": [
    "log_with_many_lab.fit(x_train,train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_for_many_labs,threshold = log_with_many_lab.predict_no_threshold(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 80, 114, 167, 251, 253, 341, 356, 363, 375, 387, 409, 421, 440, 486, 537, 811, 859, 876, 914, 918, 1063, 1079, 1266, 1304, 1312, 1323, 1594, 1620, 1680, 1698, 1724, 1728, 1747, 1794, 1841, 1853, 1865, 2009, 2036, 2072, 2131, 2161, 2306, 2379, 2412, 2415, 2423, 2458, 2463, 2544, 2597, 2636, 2708, 2820, 2925, 2936, 2939]\n"
     ]
    }
   ],
   "source": [
    "dic = []\n",
    "for i in range(len(sentiment_analysis_for_many_labs)):\n",
    "    if sentiment_analysis_for_many_labs[i] >0.500:\n",
    "        dic.append(i)\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"An incident when a friend needed to borrow a textbook from someone but she knew it won't be given to her. She pleaded that I ask for the textbook on her behalf as the person was more likely to give it to me because of the relationship I had with that person. I could see the desperation in my friend and enjoyed having the power to determine if she gets the textbook or not. I however was ready to help as helping people makes me happy, so I readily obliged her request.\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['highpower'][251]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
